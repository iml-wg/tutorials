{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMVA::Tools::Instance();\n",
    "\n",
    "auto outputFile = TFile::Open(\"CNN_ClassificationOutput.root\", \"RECREATE\");\n",
    "\n",
    "TMVA::Factory factory(\"TMVA_CNN_Classification\", outputFile,\n",
    "                      \"!V:ROC:!Silent:Color:!DrawProgressBar:AnalysisType=Classification\" ); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Features\n",
    "\n",
    "Input data is an image of 16x16 pixels from an EM shower (photon or electron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMVA::DataLoader * loader = new TMVA::DataLoader(\"dataset\");\n",
    "\n",
    "int imgSize = 8 * 8; \n",
    "\n",
    "for(auto i = 0; i < imgSize; i++)\n",
    "     loader->AddVariable(Form(\"var%d\",i),'F');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree sig_tree of type Signal with 10000 events\n",
      "DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree bkg_tree of type Background with 10000 events\n"
     ]
    }
   ],
   "source": [
    "TString inputFileName = \"data/images_data.root\";\n",
    "\n",
    "auto inputFile = TFile::Open( inputFileName );\n",
    "\n",
    "TTree *signalTree     = (TTree*)inputFile->Get(\"sig_tree\");\n",
    "TTree *backgroundTree = (TTree*)inputFile->Get(\"bkg_tree\");\n",
    "\n",
    "Double_t signalWeight     = 1.0;\n",
    "Double_t backgroundWeight = 1.0;\n",
    "   \n",
    "loader->AddSignalTree    ( signalTree,     signalWeight     );\n",
    "loader->AddBackgroundTree( backgroundTree, backgroundWeight );\n",
    "\n",
    "TCut mycuts = \"\"; \n",
    "TCut mycutb = \"\";\n",
    "\n",
    "loader->PrepareTrainingAndTestTree( mycuts, mycutb,\n",
    "                                    \"nTrain_Signal=0:nTrain_Background=0:SplitMode=Random:NormMode=NumEvents:!V\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Decision Trees (BDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mBDT\u001b[0m\n",
      "                         : \n",
      "DataSetFactory           : [dataset] : Number of events in input trees\n",
      "                         : \n",
      "                         : \n",
      "                         : Number of training and testing events\n",
      "                         : ---------------------------------------------------------------------------\n",
      "                         : Signal     -- training events            : 5000\n",
      "                         : Signal     -- testing events             : 5000\n",
      "                         : Signal     -- training and testing events: 10000\n",
      "                         : Background -- training events            : 5000\n",
      "                         : Background -- testing events             : 5000\n",
      "                         : Background -- training and testing events: 10000\n",
      "                         : \n",
      "DataSetInfo              : Correlation matrix (Signal):\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                         :             var0    var1    var2    var3    var4    var5    var6    var7    var8    var9   var10   var11   var12   var13   var14   var15   var16   var17   var18   var19   var20   var21   var22   var23   var24   var25   var26   var27   var28   var29   var30   var31   var32   var33   var34   var35   var36   var37   var38   var39   var40   var41   var42   var43   var44   var45   var46   var47   var48   var49   var50   var51   var52   var53   var54   var55   var56   var57   var58   var59   var60   var61   var62   var63\n",
      "                         :    var0:  +1.000  +0.270  +0.261  +0.210  +0.091  -0.039  -0.087  -0.103  +0.279  +0.321  +0.296  +0.202  +0.085  -0.098  -0.151  -0.145  +0.308  +0.301  +0.313  +0.190  +0.010  -0.095  -0.191  -0.184  +0.287  +0.276  +0.252  +0.113  -0.070  -0.217  -0.250  -0.221  +0.227  +0.233  +0.188  +0.026  -0.167  -0.252  -0.301  -0.233  +0.177  +0.155  +0.078  -0.048  -0.203  -0.273  -0.293  -0.241  +0.110  +0.075  +0.018  -0.103  -0.219  -0.261  -0.255  -0.226  +0.059  +0.014  -0.029  -0.113  -0.197  -0.236  -0.220  -0.184\n",
      "                         :    var1:  +0.270  +1.000  +0.360  +0.290  +0.176  +0.029  -0.055  -0.073  +0.345  +0.395  +0.380  +0.288  +0.164  -0.016  -0.082  -0.147  +0.337  +0.370  +0.370  +0.274  +0.074  -0.097  -0.178  -0.174  +0.302  +0.325  +0.294  +0.182  -0.076  -0.204  -0.269  -0.239  +0.241  +0.229  +0.188  +0.023  -0.196  -0.292  -0.316  -0.270  +0.154  +0.141  +0.050  -0.135  -0.282  -0.348  -0.323  -0.294  +0.065  +0.037  -0.056  -0.181  -0.303  -0.325  -0.327  -0.266  +0.036  +0.000  -0.084  -0.205  -0.269  -0.302  -0.290  -0.233\n",
      "                         :    var2:  +0.261  +0.360  +1.000  +0.348  +0.251  +0.147  +0.027  +0.004  +0.303  +0.379  +0.395  +0.352  +0.248  +0.072  -0.010  -0.069  +0.279  +0.335  +0.375  +0.293  +0.154  +0.006  -0.074  -0.109  +0.252  +0.262  +0.228  +0.184  +0.006  -0.129  -0.193  -0.186  +0.152  +0.158  +0.122  -0.026  -0.199  -0.271  -0.259  -0.237  +0.070  +0.037  -0.033  -0.190  -0.284  -0.335  -0.297  -0.267  -0.002  -0.043  -0.136  -0.232  -0.344  -0.364  -0.308  -0.262  -0.054  -0.080  -0.163  -0.270  -0.307  -0.353  -0.303  -0.206\n",
      "                         :    var3:  +0.210  +0.290  +0.348  +1.000  +0.336  +0.248  +0.160  +0.080  +0.217  +0.299  +0.342  +0.364  +0.332  +0.211  +0.114  +0.058  +0.167  +0.216  +0.269  +0.298  +0.228  +0.147  +0.057  +0.003  +0.117  +0.136  +0.148  +0.122  +0.071  -0.001  -0.046  -0.053  +0.025  +0.014  -0.035  -0.076  -0.134  -0.153  -0.145  -0.124  -0.030  -0.103  -0.160  -0.238  -0.293  -0.259  -0.203  -0.187  -0.108  -0.170  -0.253  -0.329  -0.361  -0.323  -0.263  -0.224  -0.129  -0.213  -0.274  -0.333  -0.342  -0.308  -0.282  -0.186\n",
      "                         :    var4:  +0.091  +0.176  +0.251  +0.336  +1.000  +0.333  +0.279  +0.189  +0.088  +0.151  +0.215  +0.319  +0.362  +0.333  +0.283  +0.187  +0.033  +0.060  +0.131  +0.258  +0.270  +0.278  +0.207  +0.173  -0.025  -0.038  -0.004  +0.074  +0.123  +0.146  +0.124  +0.090  -0.114  -0.148  -0.161  -0.142  -0.084  -0.039  +0.003  +0.014  -0.165  -0.217  -0.256  -0.283  -0.247  -0.149  -0.093  -0.061  -0.222  -0.253  -0.329  -0.365  -0.315  -0.247  -0.155  -0.111  -0.194  -0.263  -0.321  -0.346  -0.331  -0.275  -0.179  -0.113\n",
      "                         :    var5:  -0.039  +0.029  +0.147  +0.248  +0.333  +1.000  +0.326  +0.283  -0.036  -0.006  +0.087  +0.242  +0.355  +0.386  +0.372  +0.277  -0.102  -0.085  +0.007  +0.162  +0.278  +0.356  +0.311  +0.284  -0.159  -0.172  -0.143  +0.001  +0.161  +0.258  +0.257  +0.218  -0.225  -0.268  -0.261  -0.180  +0.011  +0.109  +0.155  +0.153  -0.260  -0.309  -0.331  -0.273  -0.177  -0.027  +0.062  +0.079  -0.247  -0.316  -0.356  -0.323  -0.248  -0.122  -0.032  -0.011  -0.213  -0.295  -0.321  -0.317  -0.261  -0.162  -0.101  -0.042\n",
      "                         :    var6:  -0.087  -0.055  +0.027  +0.160  +0.279  +0.326  +1.000  +0.280  -0.126  -0.101  -0.015  +0.145  +0.285  +0.384  +0.377  +0.333  -0.170  -0.164  -0.098  +0.073  +0.255  +0.344  +0.372  +0.342  -0.233  -0.253  -0.204  -0.063  +0.151  +0.293  +0.315  +0.284  -0.281  -0.289  -0.304  -0.190  +0.026  +0.145  +0.230  +0.222  -0.268  -0.331  -0.342  -0.263  -0.102  +0.056  +0.152  +0.131  -0.274  -0.318  -0.331  -0.275  -0.140  -0.033  +0.065  +0.082  -0.211  -0.268  -0.308  -0.274  -0.174  -0.090  -0.001  +0.009\n",
      "                         :    var7:  -0.103  -0.073  +0.004  +0.080  +0.189  +0.283  +0.280  +1.000  -0.126  -0.118  -0.062  +0.085  +0.198  +0.295  +0.331  +0.264  -0.181  -0.158  -0.103  +0.021  +0.175  +0.298  +0.311  +0.277  -0.203  -0.222  -0.203  -0.092  +0.124  +0.256  +0.291  +0.252  -0.224  -0.284  -0.261  -0.158  +0.046  +0.166  +0.216  +0.219  -0.244  -0.269  -0.278  -0.208  -0.062  +0.081  +0.158  +0.153  -0.211  -0.265  -0.250  -0.207  -0.094  +0.003  +0.067  +0.095  -0.194  -0.202  -0.214  -0.198  -0.127  -0.034  +0.033  +0.036\n",
      "                         :    var8:  +0.279  +0.345  +0.303  +0.217  +0.088  -0.036  -0.126  -0.126  +1.000  +0.369  +0.340  +0.222  +0.051  -0.128  -0.185  -0.183  +0.364  +0.377  +0.357  +0.213  -0.020  -0.172  -0.240  -0.252  +0.342  +0.371  +0.321  +0.158  -0.105  -0.271  -0.314  -0.280  +0.283  +0.305  +0.227  +0.064  -0.190  -0.323  -0.355  -0.297  +0.227  +0.215  +0.139  -0.058  -0.217  -0.345  -0.351  -0.296  +0.175  +0.132  +0.044  -0.099  -0.258  -0.296  -0.320  -0.257  +0.099  +0.062  -0.001  -0.088  -0.221  -0.277  -0.278  -0.185\n",
      "                         :    var9:  +0.321  +0.395  +0.379  +0.299  +0.151  -0.006  -0.101  -0.118  +0.369  +1.000  +0.428  +0.311  +0.119  -0.077  -0.167  -0.181  +0.385  +0.432  +0.418  +0.271  +0.035  -0.159  -0.238  -0.245  +0.370  +0.385  +0.346  +0.197  -0.096  -0.261  -0.337  -0.306  +0.297  +0.329  +0.248  +0.055  -0.210  -0.354  -0.384  -0.347  +0.226  +0.221  +0.113  -0.079  -0.304  -0.397  -0.396  -0.354  +0.140  +0.092  +0.014  -0.149  -0.332  -0.379  -0.385  -0.313  +0.074  +0.041  -0.051  -0.183  -0.290  -0.345  -0.354  -0.264\n",
      "                         :   var10:  +0.296  +0.380  +0.395  +0.342  +0.215  +0.087  -0.015  -0.062  +0.340  +0.428  +1.000  +0.360  +0.212  +0.037  -0.061  -0.124  +0.325  +0.399  +0.406  +0.309  +0.112  -0.064  -0.172  -0.185  +0.311  +0.340  +0.313  +0.201  -0.027  -0.195  -0.261  -0.260  +0.221  +0.241  +0.169  +0.019  -0.199  -0.324  -0.340  -0.310  +0.158  +0.128  +0.016  -0.141  -0.327  -0.397  -0.376  -0.340  +0.069  +0.025  -0.065  -0.212  -0.344  -0.396  -0.388  -0.317  +0.002  -0.055  -0.125  -0.253  -0.326  -0.365  -0.345  -0.288\n",
      "                         :   var11:  +0.202  +0.288  +0.352  +0.364  +0.319  +0.242  +0.145  +0.085  +0.222  +0.311  +0.360  +1.000  +0.329  +0.186  +0.115  +0.036  +0.166  +0.234  +0.295  +0.299  +0.209  +0.117  +0.034  -0.003  +0.129  +0.148  +0.156  +0.150  +0.041  -0.017  -0.085  -0.095  +0.043  +0.052  +0.010  -0.050  -0.140  -0.193  -0.181  -0.167  +0.003  -0.050  -0.124  -0.222  -0.308  -0.282  -0.235  -0.223  -0.077  -0.140  -0.214  -0.312  -0.348  -0.340  -0.290  -0.230  -0.105  -0.172  -0.229  -0.315  -0.349  -0.343  -0.293  -0.217\n",
      "                         :   var12:  +0.085  +0.164  +0.248  +0.332  +0.362  +0.355  +0.285  +0.198  +0.051  +0.119  +0.212  +0.329  +1.000  +0.347  +0.295  +0.195  -0.009  +0.041  +0.133  +0.257  +0.311  +0.281  +0.237  +0.171  -0.057  -0.048  -0.023  +0.087  +0.142  +0.158  +0.141  +0.122  -0.147  -0.178  -0.180  -0.169  -0.056  -0.006  +0.034  +0.043  -0.190  -0.246  -0.293  -0.298  -0.254  -0.128  -0.054  -0.029  -0.237  -0.292  -0.331  -0.342  -0.295  -0.218  -0.131  -0.096  -0.239  -0.289  -0.338  -0.371  -0.313  -0.272  -0.188  -0.106\n",
      "                         :   var13:  -0.098  -0.016  +0.072  +0.211  +0.333  +0.386  +0.384  +0.295  -0.128  -0.077  +0.037  +0.186  +0.347  +1.000  +0.414  +0.324  -0.195  -0.167  -0.070  +0.135  +0.285  +0.402  +0.377  +0.347  -0.246  -0.239  -0.212  -0.042  +0.184  +0.320  +0.338  +0.282  -0.294  -0.340  -0.337  -0.202  +0.013  +0.175  +0.224  +0.219  -0.305  -0.367  -0.372  -0.304  -0.116  +0.042  +0.142  +0.154  -0.336  -0.362  -0.384  -0.325  -0.188  -0.065  +0.026  +0.051  -0.269  -0.326  -0.347  -0.326  -0.236  -0.124  -0.028  -0.019\n",
      "                         :   var14:  -0.151  -0.082  -0.010  +0.114  +0.283  +0.372  +0.377  +0.331  -0.185  -0.167  -0.061  +0.115  +0.295  +0.414  +1.000  +0.353  -0.241  -0.249  -0.143  +0.051  +0.268  +0.404  +0.418  +0.371  -0.299  -0.307  -0.276  -0.077  +0.185  +0.344  +0.390  +0.342  -0.332  -0.369  -0.353  -0.201  +0.073  +0.236  +0.290  +0.283  -0.324  -0.392  -0.377  -0.270  -0.062  +0.107  +0.212  +0.206  -0.308  -0.378  -0.361  -0.284  -0.115  +0.005  +0.126  +0.120  -0.253  -0.304  -0.320  -0.277  -0.164  -0.044  +0.044  +0.047\n",
      "                         :   var15:  -0.145  -0.147  -0.069  +0.058  +0.187  +0.277  +0.333  +0.264  -0.183  -0.181  -0.124  +0.036  +0.195  +0.324  +0.353  +1.000  -0.246  -0.256  -0.196  -0.029  +0.191  +0.349  +0.378  +0.367  -0.274  -0.311  -0.280  -0.128  +0.149  +0.319  +0.378  +0.345  -0.301  -0.349  -0.318  -0.185  +0.060  +0.256  +0.309  +0.297  -0.303  -0.343  -0.330  -0.221  -0.037  +0.163  +0.240  +0.248  -0.260  -0.312  -0.319  -0.217  -0.056  +0.053  +0.153  +0.164  -0.218  -0.249  -0.252  -0.182  -0.103  +0.026  +0.101  +0.103\n",
      "                         :   var16:  +0.308  +0.337  +0.279  +0.167  +0.033  -0.102  -0.170  -0.181  +0.364  +0.385  +0.325  +0.166  -0.009  -0.195  -0.241  -0.246  +1.000  +0.407  +0.381  +0.197  -0.064  -0.258  -0.323  -0.294  +0.389  +0.399  +0.350  +0.170  -0.131  -0.319  -0.374  -0.326  +0.364  +0.379  +0.292  +0.105  -0.193  -0.365  -0.408  -0.362  +0.306  +0.303  +0.207  +0.012  -0.211  -0.360  -0.377  -0.339  +0.233  +0.221  +0.111  -0.027  -0.222  -0.322  -0.340  -0.316  +0.165  +0.136  +0.048  -0.055  -0.188  -0.263  -0.289  -0.224\n",
      "                         :   var17:  +0.301  +0.370  +0.335  +0.216  +0.060  -0.085  -0.164  -0.158  +0.377  +0.432  +0.399  +0.234  +0.041  -0.167  -0.249  -0.256  +0.407  +1.000  +0.416  +0.217  -0.041  -0.254  -0.335  -0.325  +0.426  +0.440  +0.384  +0.200  -0.141  -0.327  -0.412  -0.361  +0.372  +0.413  +0.304  +0.090  -0.236  -0.391  -0.430  -0.402  +0.306  +0.301  +0.201  -0.012  -0.252  -0.401  -0.417  -0.405  +0.234  +0.222  +0.130  -0.064  -0.265  -0.361  -0.406  -0.337  +0.162  +0.104  +0.045  -0.077  -0.213  -0.333  -0.331  -0.281\n",
      "                         :   var18:  +0.313  +0.370  +0.375  +0.269  +0.131  +0.007  -0.098  -0.103  +0.357  +0.418  +0.406  +0.295  +0.133  -0.070  -0.143  -0.196  +0.381  +0.416  +1.000  +0.264  +0.060  -0.152  -0.242  -0.251  +0.378  +0.368  +0.358  +0.198  -0.058  -0.255  -0.337  -0.300  +0.297  +0.310  +0.258  +0.051  -0.201  -0.345  -0.384  -0.336  +0.227  +0.217  +0.113  -0.050  -0.293  -0.411  -0.393  -0.365  +0.135  +0.117  +0.013  -0.133  -0.323  -0.380  -0.386  -0.336  +0.074  +0.021  -0.033  -0.155  -0.286  -0.364  -0.347  -0.255\n",
      "                         :   var19:  +0.190  +0.274  +0.293  +0.298  +0.258  +0.162  +0.073  +0.021  +0.213  +0.271  +0.309  +0.299  +0.257  +0.135  +0.051  -0.029  +0.197  +0.217  +0.264  +1.000  +0.170  +0.062  -0.019  -0.078  +0.153  +0.173  +0.175  +0.132  +0.038  -0.054  -0.123  -0.121  +0.093  +0.083  +0.071  -0.020  -0.124  -0.191  -0.211  -0.170  +0.027  -0.007  -0.043  -0.168  -0.252  -0.286  -0.249  -0.228  -0.032  -0.064  -0.138  -0.226  -0.289  -0.304  -0.276  -0.240  -0.073  -0.106  -0.163  -0.261  -0.313  -0.308  -0.299  -0.209\n",
      "                         :   var20:  +0.010  +0.074  +0.154  +0.228  +0.270  +0.278  +0.255  +0.175  -0.020  +0.035  +0.112  +0.209  +0.311  +0.285  +0.268  +0.191  -0.064  -0.041  +0.060  +0.170  +1.000  +0.270  +0.220  +0.198  -0.130  -0.116  -0.071  +0.012  +0.125  +0.193  +0.168  +0.136  -0.172  -0.188  -0.191  -0.114  -0.016  +0.063  +0.079  +0.090  -0.215  -0.253  -0.255  -0.222  -0.138  -0.069  +0.018  +0.032  -0.225  -0.272  -0.309  -0.282  -0.222  -0.145  -0.067  -0.025  -0.205  -0.262  -0.268  -0.284  -0.227  -0.156  -0.108  -0.072\n",
      "                         :   var21:  -0.095  -0.097  +0.006  +0.147  +0.278  +0.356  +0.344  +0.298  -0.172  -0.159  -0.064  +0.117  +0.281  +0.402  +0.404  +0.349  -0.258  -0.254  -0.152  +0.062  +0.270  +1.000  +0.418  +0.372  -0.309  -0.321  -0.277  -0.090  +0.212  +0.362  +0.383  +0.329  -0.345  -0.388  -0.359  -0.197  +0.065  +0.246  +0.293  +0.296  -0.365  -0.411  -0.379  -0.263  -0.057  +0.136  +0.212  +0.220  -0.336  -0.386  -0.394  -0.287  -0.113  +0.022  +0.115  +0.160  -0.283  -0.338  -0.324  -0.260  -0.154  -0.015  +0.061  +0.069\n",
      "                         :   var22:  -0.191  -0.178  -0.074  +0.057  +0.207  +0.311  +0.372  +0.311  -0.240  -0.238  -0.172  +0.034  +0.237  +0.377  +0.418  +0.378  -0.323  -0.335  -0.242  -0.019  +0.220  +0.418  +1.000  +0.410  -0.357  -0.386  -0.347  -0.141  +0.184  +0.396  +0.449  +0.407  -0.397  -0.439  -0.410  -0.239  +0.118  +0.298  +0.385  +0.379  -0.378  -0.427  -0.395  -0.238  +0.017  +0.228  +0.308  +0.301  -0.338  -0.377  -0.370  -0.243  -0.039  +0.122  +0.233  +0.210  -0.288  -0.316  -0.302  -0.239  -0.097  +0.042  +0.142  +0.141\n",
      "                         :   var23:  -0.184  -0.174  -0.109  +0.003  +0.173  +0.284  +0.342  +0.277  -0.252  -0.245  -0.185  -0.003  +0.171  +0.347  +0.371  +0.367  -0.294  -0.325  -0.251  -0.078  +0.198  +0.372  +0.410  +1.000  -0.352  -0.367  -0.337  -0.132  +0.170  +0.338  +0.435  +0.389  -0.371  -0.410  -0.376  -0.208  +0.095  +0.299  +0.371  +0.357  -0.330  -0.393  -0.355  -0.220  +0.030  +0.239  +0.307  +0.312  -0.322  -0.343  -0.310  -0.207  -0.018  +0.131  +0.247  +0.241  -0.249  -0.289  -0.269  -0.177  -0.050  +0.089  +0.139  +0.154\n",
      "                         :   var24:  +0.287  +0.302  +0.252  +0.117  -0.025  -0.159  -0.233  -0.203  +0.342  +0.370  +0.311  +0.129  -0.057  -0.246  -0.299  -0.274  +0.389  +0.426  +0.378  +0.153  -0.130  -0.309  -0.357  -0.352  +1.000  +0.432  +0.375  +0.155  -0.158  -0.389  -0.431  -0.351  +0.393  +0.436  +0.360  +0.106  -0.212  -0.388  -0.425  -0.392  +0.354  +0.358  +0.275  +0.068  -0.203  -0.361  -0.392  -0.377  +0.278  +0.285  +0.215  +0.024  -0.188  -0.298  -0.348  -0.312  +0.214  +0.210  +0.138  +0.001  -0.132  -0.254  -0.276  -0.253\n",
      "                         :   var25:  +0.276  +0.325  +0.262  +0.136  -0.038  -0.172  -0.253  -0.222  +0.371  +0.385  +0.340  +0.148  -0.048  -0.239  -0.307  -0.311  +0.399  +0.440  +0.368  +0.173  -0.116  -0.321  -0.386  -0.367  +0.432  +1.000  +0.401  +0.178  -0.174  -0.394  -0.460  -0.388  +0.416  +0.447  +0.371  +0.145  -0.207  -0.405  -0.456  -0.422  +0.381  +0.365  +0.281  +0.070  -0.195  -0.398  -0.432  -0.384  +0.308  +0.289  +0.214  +0.046  -0.175  -0.318  -0.385  -0.341  +0.206  +0.207  +0.152  +0.027  -0.163  -0.270  -0.313  -0.257\n",
      "                         :   var26:  +0.252  +0.294  +0.228  +0.148  -0.004  -0.143  -0.204  -0.203  +0.321  +0.346  +0.313  +0.156  -0.023  -0.212  -0.276  -0.280  +0.350  +0.384  +0.358  +0.175  -0.071  -0.277  -0.347  -0.337  +0.375  +0.401  +1.000  +0.164  -0.140  -0.336  -0.408  -0.364  +0.340  +0.388  +0.332  +0.139  -0.186  -0.376  -0.423  -0.386  +0.312  +0.333  +0.246  +0.062  -0.185  -0.363  -0.387  -0.362  +0.239  +0.245  +0.183  +0.026  -0.179  -0.312  -0.353  -0.331  +0.176  +0.168  +0.121  -0.005  -0.153  -0.239  -0.286  -0.260\n",
      "                         :   var27:  +0.113  +0.182  +0.184  +0.122  +0.074  +0.001  -0.063  -0.092  +0.158  +0.197  +0.201  +0.150  +0.087  -0.042  -0.077  -0.128  +0.170  +0.200  +0.198  +0.132  +0.012  -0.090  -0.141  -0.132  +0.155  +0.178  +0.164  +1.000  -0.025  -0.129  -0.192  -0.179  +0.138  +0.150  +0.134  +0.030  -0.123  -0.175  -0.211  -0.220  +0.113  +0.097  +0.055  -0.003  -0.152  -0.206  -0.223  -0.211  +0.064  +0.048  -0.007  -0.070  -0.150  -0.201  -0.205  -0.190  +0.027  +0.037  -0.009  -0.077  -0.138  -0.176  -0.184  -0.157\n",
      "                         :   var28:  -0.070  -0.076  +0.006  +0.071  +0.123  +0.161  +0.151  +0.124  -0.105  -0.096  -0.027  +0.041  +0.142  +0.184  +0.185  +0.149  -0.131  -0.141  -0.058  +0.038  +0.125  +0.212  +0.184  +0.170  -0.158  -0.174  -0.140  -0.025  +1.000  +0.153  +0.183  +0.139  -0.191  -0.219  -0.167  -0.092  +0.042  +0.122  +0.143  +0.138  -0.183  -0.234  -0.211  -0.136  -0.001  +0.082  +0.120  +0.090  -0.192  -0.222  -0.193  -0.147  -0.031  +0.011  +0.088  +0.075  -0.185  -0.184  -0.165  -0.154  -0.079  -0.023  +0.044  +0.061\n",
      "                         :   var29:  -0.217  -0.204  -0.129  -0.001  +0.146  +0.258  +0.293  +0.256  -0.271  -0.261  -0.195  -0.017  +0.158  +0.320  +0.344  +0.319  -0.319  -0.327  -0.255  -0.054  +0.193  +0.362  +0.396  +0.338  -0.389  -0.394  -0.336  -0.129  +0.153  +1.000  +0.426  +0.369  -0.385  -0.429  -0.386  -0.199  +0.138  +0.319  +0.369  +0.355  -0.374  -0.397  -0.353  -0.160  +0.059  +0.260  +0.333  +0.307  -0.340  -0.352  -0.333  -0.178  +0.019  +0.166  +0.254  +0.243  -0.270  -0.273  -0.275  -0.159  -0.016  +0.107  +0.175  +0.172\n",
      "                         :   var30:  -0.250  -0.269  -0.193  -0.046  +0.124  +0.257  +0.315  +0.291  -0.314  -0.337  -0.261  -0.085  +0.141  +0.338  +0.390  +0.378  -0.374  -0.412  -0.337  -0.123  +0.168  +0.383  +0.449  +0.435  -0.431  -0.460  -0.408  -0.192  +0.183  +0.426  +1.000  +0.455  -0.436  -0.485  -0.450  -0.210  +0.156  +0.385  +0.474  +0.450  -0.419  -0.465  -0.390  -0.209  +0.099  +0.316  +0.401  +0.404  -0.349  -0.393  -0.341  -0.195  +0.066  +0.232  +0.327  +0.317  -0.281  -0.293  -0.262  -0.157  +0.017  +0.164  +0.242  +0.244\n",
      "                         :   var31:  -0.221  -0.239  -0.186  -0.053  +0.090  +0.218  +0.284  +0.252  -0.280  -0.306  -0.260  -0.095  +0.122  +0.282  +0.342  +0.345  -0.326  -0.361  -0.300  -0.121  +0.136  +0.329  +0.407  +0.389  -0.351  -0.388  -0.364  -0.179  +0.139  +0.369  +0.455  +1.000  -0.381  -0.419  -0.398  -0.196  +0.126  +0.329  +0.417  +0.390  -0.347  -0.400  -0.336  -0.164  +0.083  +0.300  +0.382  +0.358  -0.314  -0.333  -0.288  -0.155  +0.051  +0.222  +0.296  +0.290  -0.231  -0.258  -0.215  -0.138  +0.034  +0.154  +0.236  +0.203\n",
      "                         :   var32:  +0.227  +0.241  +0.152  +0.025  -0.114  -0.225  -0.281  -0.224  +0.283  +0.297  +0.221  +0.043  -0.147  -0.294  -0.332  -0.301  +0.364  +0.372  +0.297  +0.093  -0.172  -0.345  -0.397  -0.371  +0.393  +0.416  +0.340  +0.138  -0.191  -0.385  -0.436  -0.381  +1.000  +0.434  +0.371  +0.158  -0.183  -0.368  -0.413  -0.372  +0.382  +0.418  +0.356  +0.123  -0.130  -0.323  -0.381  -0.349  +0.347  +0.342  +0.274  +0.118  -0.110  -0.238  -0.306  -0.291  +0.272  +0.284  +0.206  +0.102  -0.065  -0.180  -0.243  -0.233\n",
      "                         :   var33:  +0.233  +0.229  +0.158  +0.014  -0.148  -0.268  -0.289  -0.284  +0.305  +0.329  +0.241  +0.052  -0.178  -0.340  -0.369  -0.349  +0.379  +0.413  +0.310  +0.083  -0.188  -0.388  -0.439  -0.410  +0.436  +0.447  +0.388  +0.150  -0.219  -0.429  -0.485  -0.419  +0.434  +1.000  +0.410  +0.182  -0.178  -0.389  -0.457  -0.415  +0.418  +0.451  +0.373  +0.174  -0.109  -0.355  -0.426  -0.364  +0.366  +0.379  +0.331  +0.166  -0.092  -0.258  -0.341  -0.298  +0.287  +0.299  +0.260  +0.126  -0.037  -0.181  -0.259  -0.238\n",
      "                         :   var34:  +0.188  +0.188  +0.122  -0.035  -0.161  -0.261  -0.304  -0.261  +0.227  +0.248  +0.169  +0.010  -0.180  -0.337  -0.353  -0.318  +0.292  +0.304  +0.258  +0.071  -0.191  -0.359  -0.410  -0.376  +0.360  +0.371  +0.332  +0.134  -0.167  -0.386  -0.450  -0.398  +0.371  +0.410  +1.000  +0.181  -0.133  -0.339  -0.404  -0.360  +0.388  +0.409  +0.357  +0.177  -0.075  -0.290  -0.339  -0.322  +0.309  +0.338  +0.314  +0.183  -0.049  -0.179  -0.261  -0.267  +0.249  +0.289  +0.252  +0.152  -0.001  -0.123  -0.201  -0.194\n",
      "                         :   var35:  +0.026  +0.023  -0.026  -0.076  -0.142  -0.180  -0.190  -0.158  +0.064  +0.055  +0.019  -0.050  -0.169  -0.202  -0.201  -0.185  +0.105  +0.090  +0.051  -0.020  -0.114  -0.197  -0.239  -0.208  +0.106  +0.145  +0.139  +0.030  -0.092  -0.199  -0.210  -0.196  +0.158  +0.182  +0.181  +1.000  -0.014  -0.148  -0.180  -0.184  +0.181  +0.196  +0.198  +0.135  +0.033  -0.089  -0.134  -0.123  +0.167  +0.200  +0.190  +0.145  +0.055  -0.043  -0.093  -0.094  +0.135  +0.160  +0.168  +0.141  +0.065  -0.001  -0.049  -0.049\n",
      "                         :   var36:  -0.167  -0.196  -0.199  -0.134  -0.084  +0.011  +0.026  +0.046  -0.190  -0.210  -0.199  -0.140  -0.056  +0.013  +0.073  +0.060  -0.193  -0.236  -0.201  -0.124  -0.016  +0.065  +0.118  +0.095  -0.212  -0.207  -0.186  -0.123  +0.042  +0.138  +0.156  +0.126  -0.183  -0.178  -0.133  -0.014  +1.000  +0.180  +0.184  +0.158  -0.167  -0.133  -0.087  +0.025  +0.140  +0.211  +0.190  +0.174  -0.103  -0.083  -0.022  +0.056  +0.158  +0.184  +0.196  +0.163  -0.083  -0.045  +0.001  +0.081  +0.139  +0.166  +0.171  +0.144\n",
      "                         :   var37:  -0.252  -0.292  -0.271  -0.153  -0.039  +0.109  +0.145  +0.166  -0.323  -0.354  -0.324  -0.193  -0.006  +0.175  +0.236  +0.256  -0.365  -0.391  -0.345  -0.191  +0.063  +0.246  +0.298  +0.299  -0.388  -0.405  -0.376  -0.175  +0.122  +0.319  +0.385  +0.329  -0.368  -0.389  -0.339  -0.148  +0.180  +1.000  +0.392  +0.375  -0.322  -0.329  -0.246  -0.042  +0.173  +0.374  +0.388  +0.363  -0.247  -0.259  -0.176  -0.007  +0.206  +0.317  +0.352  +0.328  -0.189  -0.194  -0.136  -0.003  +0.154  +0.268  +0.303  +0.254\n",
      "                         :   var38:  -0.301  -0.316  -0.259  -0.145  +0.003  +0.155  +0.230  +0.216  -0.355  -0.384  -0.340  -0.181  +0.034  +0.224  +0.290  +0.309  -0.408  -0.430  -0.384  -0.211  +0.079  +0.293  +0.385  +0.371  -0.425  -0.456  -0.423  -0.211  +0.143  +0.369  +0.474  +0.417  -0.413  -0.457  -0.404  -0.180  +0.184  +0.392  +1.000  +0.444  -0.391  -0.404  -0.318  -0.114  +0.175  +0.393  +0.458  +0.415  -0.322  -0.332  -0.247  -0.079  +0.195  +0.346  +0.410  +0.369  -0.235  -0.230  -0.177  -0.039  +0.137  +0.269  +0.344  +0.291\n",
      "                         :   var39:  -0.233  -0.270  -0.237  -0.124  +0.014  +0.153  +0.222  +0.219  -0.297  -0.347  -0.310  -0.167  +0.043  +0.219  +0.283  +0.297  -0.362  -0.402  -0.336  -0.170  +0.090  +0.296  +0.379  +0.357  -0.392  -0.422  -0.386  -0.220  +0.138  +0.355  +0.450  +0.390  -0.372  -0.415  -0.360  -0.184  +0.158  +0.375  +0.444  +1.000  -0.357  -0.383  -0.302  -0.124  +0.175  +0.345  +0.425  +0.393  -0.285  -0.322  -0.230  -0.092  +0.150  +0.286  +0.372  +0.341  -0.214  -0.234  -0.168  -0.040  +0.114  +0.245  +0.307  +0.262\n",
      "                         :   var40:  +0.177  +0.154  +0.070  -0.030  -0.165  -0.260  -0.268  -0.244  +0.227  +0.226  +0.158  +0.003  -0.190  -0.305  -0.324  -0.303  +0.306  +0.306  +0.227  +0.027  -0.215  -0.365  -0.378  -0.330  +0.354  +0.381  +0.312  +0.113  -0.183  -0.374  -0.419  -0.347  +0.382  +0.418  +0.388  +0.181  -0.167  -0.322  -0.391  -0.357  +1.000  +0.427  +0.371  +0.190  -0.082  -0.265  -0.345  -0.330  +0.363  +0.381  +0.327  +0.185  -0.052  -0.189  -0.277  -0.269  +0.278  +0.300  +0.279  +0.157  -0.010  -0.116  -0.220  -0.197\n",
      "                         :   var41:  +0.155  +0.141  +0.037  -0.103  -0.217  -0.309  -0.331  -0.269  +0.215  +0.221  +0.128  -0.050  -0.246  -0.367  -0.392  -0.343  +0.303  +0.301  +0.217  -0.007  -0.253  -0.411  -0.427  -0.393  +0.358  +0.365  +0.333  +0.097  -0.234  -0.397  -0.465  -0.400  +0.418  +0.451  +0.409  +0.196  -0.133  -0.329  -0.404  -0.383  +0.427  +1.000  +0.418  +0.243  -0.034  -0.263  -0.348  -0.337  +0.397  +0.439  +0.398  +0.263  +0.001  -0.165  -0.276  -0.264  +0.314  +0.354  +0.331  +0.233  +0.045  -0.102  -0.198  -0.192\n",
      "                         :   var42:  +0.078  +0.050  -0.033  -0.160  -0.256  -0.331  -0.342  -0.278  +0.139  +0.113  +0.016  -0.124  -0.293  -0.372  -0.377  -0.330  +0.207  +0.201  +0.113  -0.043  -0.255  -0.379  -0.395  -0.355  +0.275  +0.281  +0.246  +0.055  -0.211  -0.353  -0.390  -0.336  +0.356  +0.373  +0.357  +0.198  -0.087  -0.246  -0.318  -0.302  +0.371  +0.418  +1.000  +0.260  +0.044  -0.135  -0.258  -0.252  +0.355  +0.414  +0.399  +0.290  +0.092  -0.048  -0.175  -0.197  +0.315  +0.363  +0.348  +0.269  +0.133  -0.006  -0.087  -0.112\n",
      "                         :   var43:  -0.048  -0.135  -0.190  -0.238  -0.283  -0.273  -0.263  -0.208  -0.058  -0.079  -0.141  -0.222  -0.298  -0.304  -0.270  -0.221  +0.012  -0.012  -0.050  -0.168  -0.222  -0.263  -0.238  -0.220  +0.068  +0.070  +0.062  -0.003  -0.136  -0.160  -0.209  -0.164  +0.123  +0.174  +0.177  +0.135  +0.025  -0.042  -0.114  -0.124  +0.190  +0.243  +0.260  +1.000  +0.157  +0.042  -0.054  -0.072  +0.195  +0.269  +0.308  +0.292  +0.212  +0.135  +0.023  -0.018  +0.196  +0.251  +0.279  +0.316  +0.224  +0.152  +0.074  +0.029\n",
      "                         :   var44:  -0.203  -0.282  -0.284  -0.293  -0.247  -0.177  -0.102  -0.062  -0.217  -0.304  -0.327  -0.308  -0.254  -0.116  -0.062  -0.037  -0.211  -0.252  -0.293  -0.252  -0.138  -0.057  +0.017  +0.030  -0.203  -0.195  -0.185  -0.152  -0.001  +0.059  +0.099  +0.083  -0.130  -0.109  -0.075  +0.033  +0.140  +0.173  +0.175  +0.175  -0.082  -0.034  +0.044  +0.157  +1.000  +0.274  +0.232  +0.203  -0.028  +0.032  +0.110  +0.221  +0.316  +0.316  +0.260  +0.227  +0.037  +0.088  +0.135  +0.236  +0.300  +0.314  +0.267  +0.199\n",
      "                         :   var45:  -0.273  -0.348  -0.335  -0.259  -0.149  -0.027  +0.056  +0.081  -0.345  -0.397  -0.397  -0.282  -0.128  +0.042  +0.107  +0.163  -0.360  -0.401  -0.411  -0.286  -0.069  +0.136  +0.228  +0.239  -0.361  -0.398  -0.363  -0.206  +0.082  +0.260  +0.316  +0.300  -0.323  -0.355  -0.290  -0.089  +0.211  +0.374  +0.393  +0.345  -0.265  -0.263  -0.135  +0.042  +0.274  +1.000  +0.426  +0.373  -0.194  -0.174  -0.075  +0.093  +0.309  +0.397  +0.414  +0.369  -0.127  -0.088  -0.009  +0.126  +0.274  +0.362  +0.374  +0.305\n",
      "                         :   var46:  -0.293  -0.323  -0.297  -0.203  -0.093  +0.062  +0.152  +0.158  -0.351  -0.396  -0.376  -0.235  -0.054  +0.142  +0.212  +0.240  -0.377  -0.417  -0.393  -0.249  +0.018  +0.212  +0.308  +0.307  -0.392  -0.432  -0.387  -0.223  +0.120  +0.333  +0.401  +0.382  -0.381  -0.426  -0.339  -0.134  +0.190  +0.388  +0.458  +0.425  -0.345  -0.348  -0.258  -0.054  +0.232  +0.426  +1.000  +0.415  -0.264  -0.265  -0.160  +0.009  +0.250  +0.371  +0.423  +0.384  -0.179  -0.164  -0.100  +0.039  +0.218  +0.326  +0.366  +0.320\n",
      "                         :   var47:  -0.241  -0.294  -0.267  -0.187  -0.061  +0.079  +0.131  +0.153  -0.296  -0.354  -0.340  -0.223  -0.029  +0.154  +0.206  +0.248  -0.339  -0.405  -0.365  -0.228  +0.032  +0.220  +0.301  +0.312  -0.377  -0.384  -0.362  -0.211  +0.090  +0.307  +0.404  +0.358  -0.349  -0.364  -0.322  -0.123  +0.174  +0.363  +0.415  +0.393  -0.330  -0.337  -0.252  -0.072  +0.203  +0.373  +0.415  +1.000  -0.249  -0.252  -0.175  -0.011  +0.209  +0.316  +0.395  +0.351  -0.187  -0.171  -0.112  +0.008  +0.167  +0.292  +0.338  +0.286\n",
      "                         :   var48:  +0.110  +0.065  -0.002  -0.108  -0.222  -0.247  -0.274  -0.211  +0.175  +0.140  +0.069  -0.077  -0.237  -0.336  -0.308  -0.260  +0.233  +0.234  +0.135  -0.032  -0.225  -0.336  -0.338  -0.322  +0.278  +0.308  +0.239  +0.064  -0.192  -0.340  -0.349  -0.314  +0.347  +0.366  +0.309  +0.167  -0.103  -0.247  -0.322  -0.285  +0.363  +0.397  +0.355  +0.195  -0.028  -0.194  -0.264  -0.249  +1.000  +0.377  +0.353  +0.216  +0.023  -0.114  -0.191  -0.207  +0.291  +0.326  +0.299  +0.222  +0.074  -0.043  -0.122  -0.127\n",
      "                         :   var49:  +0.075  +0.037  -0.043  -0.170  -0.253  -0.316  -0.318  -0.265  +0.132  +0.092  +0.025  -0.140  -0.292  -0.362  -0.378  -0.312  +0.221  +0.222  +0.117  -0.064  -0.272  -0.386  -0.377  -0.343  +0.285  +0.289  +0.245  +0.048  -0.222  -0.352  -0.393  -0.333  +0.342  +0.379  +0.338  +0.200  -0.083  -0.259  -0.332  -0.322  +0.381  +0.439  +0.414  +0.269  +0.032  -0.174  -0.265  -0.252  +0.377  +1.000  +0.419  +0.276  +0.083  -0.064  -0.178  -0.189  +0.333  +0.376  +0.373  +0.285  +0.125  -0.006  -0.106  -0.128\n",
      "                         :   var50:  +0.018  -0.056  -0.136  -0.253  -0.329  -0.356  -0.331  -0.250  +0.044  +0.014  -0.065  -0.214  -0.331  -0.384  -0.361  -0.319  +0.111  +0.130  +0.013  -0.138  -0.309  -0.394  -0.370  -0.310  +0.215  +0.214  +0.183  -0.007  -0.193  -0.333  -0.341  -0.288  +0.274  +0.331  +0.314  +0.190  -0.022  -0.176  -0.247  -0.230  +0.327  +0.398  +0.399  +0.308  +0.110  -0.075  -0.160  -0.175  +0.353  +0.419  +1.000  +0.355  +0.190  +0.040  -0.072  -0.128  +0.306  +0.352  +0.411  +0.361  +0.233  +0.071  -0.012  -0.066\n",
      "                         :   var51:  -0.103  -0.181  -0.232  -0.329  -0.365  -0.323  -0.275  -0.207  -0.099  -0.149  -0.212  -0.312  -0.342  -0.325  -0.284  -0.217  -0.027  -0.064  -0.133  -0.226  -0.282  -0.287  -0.243  -0.207  +0.024  +0.046  +0.026  -0.070  -0.147  -0.178  -0.195  -0.155  +0.118  +0.166  +0.183  +0.145  +0.056  -0.007  -0.079  -0.092  +0.185  +0.263  +0.290  +0.292  +0.221  +0.093  +0.009  -0.011  +0.216  +0.276  +0.355  +1.000  +0.293  +0.193  +0.099  +0.063  +0.229  +0.302  +0.340  +0.383  +0.329  +0.222  +0.140  +0.070\n",
      "                         :   var52:  -0.219  -0.303  -0.344  -0.361  -0.315  -0.248  -0.140  -0.094  -0.258  -0.332  -0.344  -0.348  -0.295  -0.188  -0.115  -0.056  -0.222  -0.265  -0.323  -0.289  -0.222  -0.113  -0.039  -0.018  -0.188  -0.175  -0.179  -0.150  -0.031  +0.019  +0.066  +0.051  -0.110  -0.092  -0.049  +0.055  +0.158  +0.206  +0.195  +0.150  -0.052  +0.001  +0.092  +0.212  +0.316  +0.309  +0.250  +0.209  +0.023  +0.083  +0.190  +0.293  +1.000  +0.352  +0.288  +0.229  +0.075  +0.127  +0.217  +0.317  +0.362  +0.351  +0.320  +0.223\n",
      "                         :   var53:  -0.261  -0.325  -0.364  -0.323  -0.247  -0.122  -0.033  +0.003  -0.296  -0.379  -0.396  -0.340  -0.218  -0.065  +0.005  +0.053  -0.322  -0.361  -0.380  -0.304  -0.145  +0.022  +0.122  +0.131  -0.298  -0.318  -0.312  -0.201  +0.011  +0.166  +0.232  +0.222  -0.238  -0.258  -0.179  -0.043  +0.184  +0.317  +0.346  +0.286  -0.189  -0.165  -0.048  +0.135  +0.316  +0.397  +0.371  +0.316  -0.114  -0.064  +0.040  +0.193  +0.352  +1.000  +0.407  +0.333  -0.035  +0.010  +0.076  +0.223  +0.337  +0.388  +0.377  +0.276\n",
      "                         :   var54:  -0.255  -0.327  -0.308  -0.263  -0.155  -0.032  +0.065  +0.067  -0.320  -0.385  -0.388  -0.290  -0.131  +0.026  +0.126  +0.153  -0.340  -0.406  -0.386  -0.276  -0.067  +0.115  +0.233  +0.247  -0.348  -0.385  -0.353  -0.205  +0.088  +0.254  +0.327  +0.296  -0.306  -0.341  -0.261  -0.093  +0.196  +0.352  +0.410  +0.372  -0.277  -0.276  -0.175  +0.023  +0.260  +0.414  +0.423  +0.395  -0.191  -0.178  -0.072  +0.099  +0.288  +0.407  +1.000  +0.375  -0.117  -0.092  -0.024  +0.112  +0.275  +0.353  +0.377  +0.320\n",
      "                         :   var55:  -0.226  -0.266  -0.262  -0.224  -0.111  -0.011  +0.082  +0.095  -0.257  -0.313  -0.317  -0.230  -0.096  +0.051  +0.120  +0.164  -0.316  -0.337  -0.336  -0.240  -0.025  +0.160  +0.210  +0.241  -0.312  -0.341  -0.331  -0.190  +0.075  +0.243  +0.317  +0.290  -0.291  -0.298  -0.267  -0.094  +0.163  +0.328  +0.369  +0.341  -0.269  -0.264  -0.197  -0.018  +0.227  +0.369  +0.384  +0.351  -0.207  -0.189  -0.128  +0.063  +0.229  +0.333  +0.375  +1.000  -0.121  -0.115  -0.057  +0.070  +0.217  +0.309  +0.321  +0.278\n",
      "                         :   var56:  +0.059  +0.036  -0.054  -0.129  -0.194  -0.213  -0.211  -0.194  +0.099  +0.074  +0.002  -0.105  -0.239  -0.269  -0.253  -0.218  +0.165  +0.162  +0.074  -0.073  -0.205  -0.283  -0.288  -0.249  +0.214  +0.206  +0.176  +0.027  -0.185  -0.270  -0.281  -0.231  +0.272  +0.287  +0.249  +0.135  -0.083  -0.189  -0.235  -0.214  +0.278  +0.314  +0.315  +0.196  +0.037  -0.127  -0.179  -0.187  +0.291  +0.333  +0.306  +0.229  +0.075  -0.035  -0.117  -0.121  +1.000  +0.299  +0.272  +0.215  +0.107  -0.015  -0.080  -0.087\n",
      "                         :   var57:  +0.014  +0.000  -0.080  -0.213  -0.263  -0.295  -0.268  -0.202  +0.062  +0.041  -0.055  -0.172  -0.289  -0.326  -0.304  -0.249  +0.136  +0.104  +0.021  -0.106  -0.262  -0.338  -0.316  -0.289  +0.210  +0.207  +0.168  +0.037  -0.184  -0.273  -0.293  -0.258  +0.284  +0.299  +0.289  +0.160  -0.045  -0.194  -0.230  -0.234  +0.300  +0.354  +0.363  +0.251  +0.088  -0.088  -0.164  -0.171  +0.326  +0.376  +0.352  +0.302  +0.127  +0.010  -0.092  -0.115  +0.299  +1.000  +0.340  +0.287  +0.174  +0.046  -0.039  -0.075\n",
      "                         :   var58:  -0.029  -0.084  -0.163  -0.274  -0.321  -0.321  -0.308  -0.214  -0.001  -0.051  -0.125  -0.229  -0.338  -0.347  -0.320  -0.252  +0.048  +0.045  -0.033  -0.163  -0.268  -0.324  -0.302  -0.269  +0.138  +0.152  +0.121  -0.009  -0.165  -0.275  -0.262  -0.215  +0.206  +0.260  +0.252  +0.168  +0.001  -0.136  -0.177  -0.168  +0.279  +0.331  +0.348  +0.279  +0.135  -0.009  -0.100  -0.112  +0.299  +0.373  +0.411  +0.340  +0.217  +0.076  -0.024  -0.057  +0.272  +0.340  +1.000  +0.342  +0.244  +0.135  +0.042  -0.012\n",
      "                         :   var59:  -0.113  -0.205  -0.270  -0.333  -0.346  -0.317  -0.274  -0.198  -0.088  -0.183  -0.253  -0.315  -0.371  -0.326  -0.277  -0.182  -0.055  -0.077  -0.155  -0.261  -0.284  -0.260  -0.239  -0.177  +0.001  +0.027  -0.005  -0.077  -0.154  -0.159  -0.157  -0.138  +0.102  +0.126  +0.152  +0.141  +0.081  -0.003  -0.039  -0.040  +0.157  +0.233  +0.269  +0.316  +0.236  +0.126  +0.039  +0.008  +0.222  +0.285  +0.361  +0.383  +0.317  +0.223  +0.112  +0.070  +0.215  +0.287  +0.342  +1.000  +0.347  +0.265  +0.167  +0.083\n",
      "                         :   var60:  -0.197  -0.269  -0.307  -0.342  -0.331  -0.261  -0.174  -0.127  -0.221  -0.290  -0.326  -0.349  -0.313  -0.236  -0.164  -0.103  -0.188  -0.213  -0.286  -0.313  -0.227  -0.154  -0.097  -0.050  -0.132  -0.163  -0.153  -0.138  -0.079  -0.016  +0.017  +0.034  -0.065  -0.037  -0.001  +0.065  +0.139  +0.154  +0.137  +0.114  -0.010  +0.045  +0.133  +0.224  +0.300  +0.274  +0.218  +0.167  +0.074  +0.125  +0.233  +0.329  +0.362  +0.337  +0.275  +0.217  +0.107  +0.174  +0.244  +0.347  +1.000  +0.354  +0.288  +0.187\n",
      "                         :   var61:  -0.236  -0.302  -0.353  -0.308  -0.275  -0.162  -0.090  -0.034  -0.277  -0.345  -0.365  -0.343  -0.272  -0.124  -0.044  +0.026  -0.263  -0.333  -0.364  -0.308  -0.156  -0.015  +0.042  +0.089  -0.254  -0.270  -0.239  -0.176  -0.023  +0.107  +0.164  +0.154  -0.180  -0.181  -0.123  -0.001  +0.166  +0.268  +0.269  +0.245  -0.116  -0.102  -0.006  +0.152  +0.314  +0.362  +0.326  +0.292  -0.043  -0.006  +0.071  +0.222  +0.351  +0.388  +0.353  +0.309  -0.015  +0.046  +0.135  +0.265  +0.354  +1.000  +0.349  +0.282\n",
      "                         :   var62:  -0.220  -0.290  -0.303  -0.282  -0.179  -0.101  -0.001  +0.033  -0.278  -0.354  -0.345  -0.293  -0.188  -0.028  +0.044  +0.101  -0.289  -0.331  -0.347  -0.299  -0.108  +0.061  +0.142  +0.139  -0.276  -0.313  -0.286  -0.184  +0.044  +0.175  +0.242  +0.236  -0.243  -0.259  -0.201  -0.049  +0.171  +0.303  +0.344  +0.307  -0.220  -0.198  -0.087  +0.074  +0.267  +0.374  +0.366  +0.338  -0.122  -0.106  -0.012  +0.140  +0.320  +0.377  +0.377  +0.321  -0.080  -0.039  +0.042  +0.167  +0.288  +0.349  +1.000  +0.287\n",
      "                         :   var63:  -0.184  -0.233  -0.206  -0.186  -0.113  -0.042  +0.009  +0.036  -0.185  -0.264  -0.288  -0.217  -0.106  -0.019  +0.047  +0.103  -0.224  -0.281  -0.255  -0.209  -0.072  +0.069  +0.141  +0.154  -0.253  -0.257  -0.260  -0.157  +0.061  +0.172  +0.244  +0.203  -0.233  -0.238  -0.194  -0.049  +0.144  +0.254  +0.291  +0.262  -0.197  -0.192  -0.112  +0.029  +0.199  +0.305  +0.320  +0.286  -0.127  -0.128  -0.066  +0.070  +0.223  +0.276  +0.320  +0.278  -0.087  -0.075  -0.012  +0.083  +0.187  +0.282  +0.287  +1.000\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "DataSetInfo              : Correlation matrix (Background):\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                         :             var0    var1    var2    var3    var4    var5    var6    var7    var8    var9   var10   var11   var12   var13   var14   var15   var16   var17   var18   var19   var20   var21   var22   var23   var24   var25   var26   var27   var28   var29   var30   var31   var32   var33   var34   var35   var36   var37   var38   var39   var40   var41   var42   var43   var44   var45   var46   var47   var48   var49   var50   var51   var52   var53   var54   var55   var56   var57   var58   var59   var60   var61   var62   var63\n",
      "                         :    var0:  +1.000  +0.279  +0.265  +0.252  +0.153  +0.067  +0.016  -0.014  +0.301  +0.315  +0.310  +0.259  +0.164  +0.044  -0.000  -0.080  +0.281  +0.302  +0.311  +0.223  +0.099  -0.038  -0.120  -0.112  +0.254  +0.256  +0.243  +0.126  -0.008  -0.129  -0.168  -0.184  +0.158  +0.138  +0.081  -0.024  -0.160  -0.222  -0.260  -0.223  +0.093  +0.040  -0.028  -0.120  -0.226  -0.277  -0.276  -0.231  +0.011  -0.016  -0.072  -0.181  -0.243  -0.265  -0.263  -0.220  -0.015  -0.087  -0.123  -0.168  -0.228  -0.231  -0.225  -0.170\n",
      "                         :    var1:  +0.279  +1.000  +0.343  +0.304  +0.232  +0.126  +0.052  +0.009  +0.336  +0.370  +0.367  +0.327  +0.229  +0.093  +0.008  -0.037  +0.303  +0.340  +0.325  +0.288  +0.158  +0.017  -0.080  -0.106  +0.258  +0.269  +0.238  +0.154  +0.015  -0.116  -0.166  -0.186  +0.154  +0.123  +0.066  -0.045  -0.189  -0.248  -0.273  -0.224  +0.070  +0.013  -0.066  -0.185  -0.281  -0.309  -0.340  -0.284  -0.013  -0.067  -0.145  -0.236  -0.307  -0.329  -0.322  -0.268  -0.053  -0.123  -0.166  -0.231  -0.295  -0.284  -0.262  -0.217\n",
      "                         :    var2:  +0.265  +0.343  +1.000  +0.390  +0.314  +0.252  +0.147  +0.069  +0.313  +0.387  +0.416  +0.401  +0.339  +0.211  +0.111  +0.042  +0.292  +0.330  +0.352  +0.323  +0.245  +0.115  +0.035  -0.028  +0.218  +0.213  +0.213  +0.136  +0.069  -0.053  -0.105  -0.112  +0.097  +0.075  +0.010  -0.082  -0.197  -0.258  -0.219  -0.189  -0.009  -0.067  -0.157  -0.258  -0.341  -0.343  -0.311  -0.269  -0.078  -0.164  -0.228  -0.327  -0.382  -0.360  -0.340  -0.267  -0.123  -0.200  -0.252  -0.330  -0.363  -0.333  -0.313  -0.257\n",
      "                         :    var3:  +0.252  +0.304  +0.390  +1.000  +0.360  +0.310  +0.225  +0.186  +0.229  +0.326  +0.383  +0.395  +0.382  +0.279  +0.216  +0.131  +0.198  +0.254  +0.313  +0.331  +0.300  +0.209  +0.120  +0.066  +0.123  +0.118  +0.144  +0.136  +0.106  +0.031  +0.001  -0.036  -0.016  -0.034  -0.075  -0.109  -0.146  -0.167  -0.148  -0.114  -0.095  -0.145  -0.243  -0.285  -0.341  -0.307  -0.268  -0.209  -0.157  -0.221  -0.309  -0.377  -0.388  -0.360  -0.310  -0.221  -0.179  -0.255  -0.317  -0.359  -0.378  -0.340  -0.298  -0.210\n",
      "                         :    var4:  +0.153  +0.232  +0.314  +0.360  +1.000  +0.360  +0.311  +0.235  +0.157  +0.216  +0.306  +0.362  +0.399  +0.363  +0.309  +0.250  +0.069  +0.128  +0.216  +0.287  +0.344  +0.302  +0.254  +0.184  -0.012  +0.016  +0.041  +0.099  +0.135  +0.141  +0.134  +0.096  -0.128  -0.156  -0.147  -0.153  -0.133  -0.097  -0.032  -0.007  -0.185  -0.233  -0.317  -0.320  -0.313  -0.223  -0.168  -0.111  -0.218  -0.289  -0.337  -0.386  -0.355  -0.306  -0.238  -0.165  -0.207  -0.281  -0.347  -0.357  -0.361  -0.318  -0.241  -0.177\n",
      "                         :    var5:  +0.067  +0.126  +0.252  +0.310  +0.360  +1.000  +0.344  +0.279  +0.042  +0.133  +0.203  +0.299  +0.385  +0.374  +0.356  +0.298  -0.024  +0.047  +0.120  +0.223  +0.323  +0.344  +0.300  +0.261  -0.098  -0.102  -0.047  +0.061  +0.171  +0.210  +0.207  +0.192  -0.202  -0.208  -0.222  -0.171  -0.090  -0.001  +0.055  +0.093  -0.249  -0.296  -0.328  -0.314  -0.253  -0.159  -0.090  -0.013  -0.263  -0.323  -0.360  -0.373  -0.299  -0.222  -0.158  -0.077  -0.226  -0.284  -0.349  -0.314  -0.320  -0.253  -0.173  -0.105\n",
      "                         :    var6:  +0.016  +0.052  +0.147  +0.225  +0.311  +0.344  +1.000  +0.292  -0.024  +0.031  +0.121  +0.221  +0.338  +0.375  +0.370  +0.338  -0.085  -0.051  +0.018  +0.173  +0.298  +0.344  +0.347  +0.309  -0.177  -0.165  -0.107  +0.001  +0.178  +0.236  +0.263  +0.244  -0.240  -0.260  -0.246  -0.166  -0.056  +0.063  +0.124  +0.152  -0.272  -0.313  -0.316  -0.295  -0.191  -0.081  +0.012  +0.045  -0.248  -0.308  -0.331  -0.306  -0.230  -0.147  -0.086  -0.012  -0.217  -0.270  -0.315  -0.292  -0.254  -0.190  -0.101  -0.061\n",
      "                         :    var7:  -0.014  +0.009  +0.069  +0.186  +0.235  +0.279  +0.292  +1.000  -0.060  -0.014  +0.049  +0.156  +0.264  +0.285  +0.313  +0.290  -0.098  -0.067  -0.004  +0.082  +0.224  +0.290  +0.315  +0.271  -0.165  -0.177  -0.107  +0.003  +0.131  +0.226  +0.254  +0.243  -0.225  -0.259  -0.233  -0.141  -0.046  +0.094  +0.142  +0.178  -0.240  -0.267  -0.277  -0.239  -0.128  -0.030  +0.076  +0.102  -0.228  -0.259  -0.282  -0.254  -0.169  -0.101  -0.038  +0.013  -0.192  -0.219  -0.257  -0.215  -0.182  -0.105  -0.062  -0.002\n",
      "                         :    var8:  +0.301  +0.336  +0.313  +0.229  +0.157  +0.042  -0.024  -0.060  +1.000  +0.387  +0.356  +0.271  +0.121  +0.019  -0.079  -0.130  +0.344  +0.363  +0.344  +0.230  +0.088  -0.073  -0.159  -0.181  +0.328  +0.336  +0.283  +0.162  -0.046  -0.180  -0.238  -0.247  +0.256  +0.207  +0.150  +0.001  -0.176  -0.294  -0.308  -0.285  +0.173  +0.128  +0.028  -0.104  -0.259  -0.330  -0.346  -0.309  +0.075  +0.006  -0.057  -0.177  -0.254  -0.320  -0.317  -0.259  +0.005  -0.036  -0.086  -0.187  -0.250  -0.271  -0.280  -0.227\n",
      "                         :    var9:  +0.315  +0.370  +0.387  +0.326  +0.216  +0.133  +0.031  -0.014  +0.387  +1.000  +0.433  +0.351  +0.217  +0.074  -0.014  -0.075  +0.365  +0.400  +0.399  +0.321  +0.142  -0.009  -0.117  -0.157  +0.331  +0.341  +0.303  +0.180  +0.006  -0.161  -0.218  -0.236  +0.207  +0.205  +0.118  -0.001  -0.236  -0.321  -0.333  -0.299  +0.114  +0.055  -0.024  -0.165  -0.315  -0.391  -0.385  -0.336  +0.033  -0.045  -0.122  -0.248  -0.336  -0.381  -0.399  -0.308  -0.032  -0.087  -0.159  -0.249  -0.319  -0.342  -0.334  -0.254\n",
      "                         :   var10:  +0.310  +0.367  +0.416  +0.383  +0.306  +0.203  +0.121  +0.049  +0.356  +0.433  +1.000  +0.417  +0.307  +0.177  +0.067  -0.010  +0.321  +0.378  +0.397  +0.367  +0.231  +0.075  -0.019  -0.072  +0.269  +0.295  +0.263  +0.178  +0.059  -0.088  -0.163  -0.167  +0.143  +0.115  +0.058  -0.084  -0.193  -0.286  -0.294  -0.263  +0.046  -0.012  -0.130  -0.233  -0.345  -0.392  -0.382  -0.317  -0.037  -0.124  -0.206  -0.324  -0.395  -0.398  -0.383  -0.308  -0.092  -0.183  -0.243  -0.315  -0.378  -0.379  -0.345  -0.261\n",
      "                         :   var11:  +0.259  +0.327  +0.401  +0.395  +0.362  +0.299  +0.221  +0.156  +0.271  +0.351  +0.417  +1.000  +0.374  +0.287  +0.191  +0.123  +0.214  +0.281  +0.334  +0.329  +0.293  +0.195  +0.122  +0.053  +0.152  +0.159  +0.179  +0.163  +0.098  +0.034  -0.002  -0.033  +0.005  -0.005  -0.057  -0.115  -0.162  -0.191  -0.177  -0.138  -0.087  -0.143  -0.230  -0.295  -0.362  -0.318  -0.295  -0.227  -0.137  -0.235  -0.292  -0.375  -0.403  -0.378  -0.335  -0.241  -0.171  -0.252  -0.319  -0.366  -0.387  -0.370  -0.323  -0.229\n",
      "                         :   var12:  +0.164  +0.229  +0.339  +0.382  +0.399  +0.385  +0.338  +0.264  +0.121  +0.217  +0.307  +0.374  +1.000  +0.378  +0.342  +0.262  +0.072  +0.126  +0.215  +0.294  +0.357  +0.339  +0.268  +0.204  -0.011  -0.007  +0.037  +0.091  +0.174  +0.171  +0.136  +0.112  -0.152  -0.170  -0.187  -0.168  -0.120  -0.076  -0.022  +0.034  -0.214  -0.273  -0.336  -0.336  -0.316  -0.219  -0.154  -0.116  -0.264  -0.329  -0.372  -0.414  -0.391  -0.315  -0.233  -0.163  -0.254  -0.323  -0.379  -0.378  -0.393  -0.323  -0.239  -0.167\n",
      "                         :   var13:  +0.044  +0.093  +0.211  +0.279  +0.363  +0.374  +0.375  +0.285  +0.019  +0.074  +0.177  +0.287  +0.378  +1.000  +0.411  +0.351  -0.073  -0.010  +0.078  +0.205  +0.356  +0.387  +0.395  +0.312  -0.154  -0.137  -0.092  +0.043  +0.183  +0.275  +0.279  +0.255  -0.261  -0.293  -0.285  -0.181  -0.066  +0.051  +0.127  +0.163  -0.319  -0.354  -0.378  -0.349  -0.254  -0.105  -0.011  +0.031  -0.304  -0.385  -0.404  -0.392  -0.299  -0.194  -0.126  -0.037  -0.290  -0.363  -0.367  -0.366  -0.319  -0.235  -0.139  -0.080\n",
      "                         :   var14:  -0.000  +0.008  +0.111  +0.216  +0.309  +0.356  +0.370  +0.313  -0.079  -0.014  +0.067  +0.191  +0.342  +0.411  +1.000  +0.367  -0.136  -0.107  -0.026  +0.131  +0.307  +0.382  +0.408  +0.373  -0.220  -0.214  -0.181  -0.011  +0.190  +0.289  +0.349  +0.308  -0.325  -0.331  -0.308  -0.200  -0.010  +0.134  +0.211  +0.222  -0.319  -0.366  -0.363  -0.301  -0.172  -0.017  +0.089  +0.130  -0.312  -0.354  -0.393  -0.332  -0.220  -0.109  -0.024  +0.039  -0.255  -0.306  -0.334  -0.298  -0.235  -0.151  -0.065  -0.009\n",
      "                         :   var15:  -0.080  -0.037  +0.042  +0.131  +0.250  +0.298  +0.338  +0.290  -0.130  -0.075  -0.010  +0.123  +0.262  +0.351  +0.367  +1.000  -0.179  -0.173  -0.090  +0.054  +0.225  +0.356  +0.394  +0.336  -0.249  -0.261  -0.188  -0.041  +0.160  +0.294  +0.324  +0.334  -0.296  -0.331  -0.291  -0.175  -0.003  +0.164  +0.218  +0.259  -0.304  -0.340  -0.329  -0.261  -0.105  +0.056  +0.139  +0.178  -0.258  -0.320  -0.320  -0.253  -0.161  -0.042  +0.032  +0.076  -0.192  -0.254  -0.277  -0.221  -0.166  -0.075  -0.014  +0.018\n",
      "                         :   var16:  +0.281  +0.303  +0.292  +0.198  +0.069  -0.024  -0.085  -0.098  +0.344  +0.365  +0.321  +0.214  +0.072  -0.073  -0.136  -0.179  +1.000  +0.398  +0.344  +0.212  +0.008  -0.139  -0.234  -0.253  +0.384  +0.370  +0.308  +0.133  -0.092  -0.244  -0.294  -0.307  +0.315  +0.319  +0.203  +0.038  -0.173  -0.326  -0.371  -0.338  +0.244  +0.211  +0.125  -0.053  -0.228  -0.345  -0.372  -0.339  +0.153  +0.095  +0.043  -0.115  -0.229  -0.310  -0.339  -0.288  +0.062  +0.041  -0.026  -0.114  -0.203  -0.253  -0.279  -0.235\n",
      "                         :   var17:  +0.302  +0.340  +0.330  +0.254  +0.128  +0.047  -0.051  -0.067  +0.363  +0.400  +0.378  +0.281  +0.126  -0.010  -0.107  -0.173  +0.398  +1.000  +0.386  +0.265  +0.077  -0.126  -0.233  -0.243  +0.374  +0.384  +0.336  +0.155  -0.048  -0.249  -0.290  -0.297  +0.288  +0.295  +0.197  +0.046  -0.188  -0.341  -0.375  -0.359  +0.201  +0.179  +0.067  -0.093  -0.266  -0.394  -0.398  -0.376  +0.121  +0.067  -0.015  -0.152  -0.280  -0.357  -0.370  -0.331  +0.052  -0.013  -0.071  -0.164  -0.275  -0.299  -0.334  -0.258\n",
      "                         :   var18:  +0.311  +0.325  +0.352  +0.313  +0.216  +0.120  +0.018  -0.004  +0.344  +0.399  +0.397  +0.334  +0.215  +0.078  -0.026  -0.090  +0.344  +0.386  +1.000  +0.314  +0.138  -0.033  -0.127  -0.151  +0.326  +0.334  +0.321  +0.161  -0.001  -0.160  -0.240  -0.242  +0.203  +0.205  +0.139  -0.006  -0.192  -0.306  -0.342  -0.312  +0.115  +0.092  -0.025  -0.147  -0.317  -0.379  -0.390  -0.337  +0.028  -0.021  -0.111  -0.229  -0.320  -0.370  -0.369  -0.311  -0.040  -0.085  -0.138  -0.246  -0.317  -0.344  -0.343  -0.277\n",
      "                         :   var19:  +0.223  +0.288  +0.323  +0.331  +0.287  +0.223  +0.173  +0.082  +0.230  +0.321  +0.367  +0.329  +0.294  +0.205  +0.131  +0.054  +0.212  +0.265  +0.314  +1.000  +0.248  +0.140  +0.046  +0.004  +0.134  +0.179  +0.176  +0.114  +0.087  -0.016  -0.048  -0.088  +0.038  +0.039  -0.011  -0.063  -0.149  -0.208  -0.185  -0.164  -0.028  -0.081  -0.171  -0.215  -0.292  -0.319  -0.282  -0.222  -0.096  -0.170  -0.220  -0.290  -0.348  -0.350  -0.318  -0.222  -0.118  -0.198  -0.258  -0.312  -0.348  -0.333  -0.298  -0.239\n",
      "                         :   var20:  +0.099  +0.158  +0.245  +0.300  +0.344  +0.323  +0.298  +0.224  +0.088  +0.142  +0.231  +0.293  +0.357  +0.356  +0.307  +0.225  +0.008  +0.077  +0.138  +0.248  +1.000  +0.303  +0.268  +0.215  -0.061  -0.057  +0.022  +0.090  +0.154  +0.172  +0.158  +0.150  -0.174  -0.176  -0.196  -0.145  -0.066  -0.014  +0.008  +0.051  -0.223  -0.265  -0.312  -0.292  -0.260  -0.162  -0.113  -0.048  -0.251  -0.317  -0.354  -0.351  -0.321  -0.237  -0.178  -0.094  -0.220  -0.298  -0.323  -0.335  -0.329  -0.266  -0.193  -0.143\n",
      "                         :   var21:  -0.038  +0.017  +0.115  +0.209  +0.302  +0.344  +0.344  +0.290  -0.073  -0.009  +0.075  +0.195  +0.339  +0.387  +0.382  +0.356  -0.139  -0.126  -0.033  +0.140  +0.303  +1.000  +0.399  +0.360  -0.238  -0.228  -0.168  -0.006  +0.154  +0.301  +0.321  +0.292  -0.305  -0.328  -0.316  -0.196  -0.020  +0.127  +0.192  +0.224  -0.321  -0.356  -0.367  -0.306  -0.156  +0.005  +0.075  +0.124  -0.317  -0.355  -0.369  -0.311  -0.224  -0.096  -0.028  +0.029  -0.268  -0.310  -0.340  -0.297  -0.236  -0.146  -0.057  -0.022\n",
      "                         :   var22:  -0.120  -0.080  +0.035  +0.120  +0.254  +0.300  +0.347  +0.315  -0.159  -0.117  -0.019  +0.122  +0.268  +0.395  +0.408  +0.394  -0.234  -0.233  -0.127  +0.046  +0.268  +0.399  +1.000  +0.396  -0.328  -0.305  -0.247  -0.037  +0.183  +0.340  +0.394  +0.389  -0.371  -0.392  -0.350  -0.198  +0.028  +0.225  +0.317  +0.311  -0.389  -0.397  -0.393  -0.280  -0.083  +0.093  +0.193  +0.231  -0.340  -0.367  -0.360  -0.272  -0.127  -0.015  +0.079  +0.124  -0.259  -0.309  -0.314  -0.239  -0.171  -0.080  +0.035  +0.060\n",
      "                         :   var23:  -0.112  -0.106  -0.028  +0.066  +0.184  +0.261  +0.309  +0.271  -0.181  -0.157  -0.072  +0.053  +0.204  +0.312  +0.373  +0.336  -0.253  -0.243  -0.151  +0.004  +0.215  +0.360  +0.396  +1.000  -0.325  -0.318  -0.277  -0.091  +0.163  +0.323  +0.375  +0.374  -0.377  -0.361  -0.322  -0.174  +0.046  +0.235  +0.301  +0.333  -0.352  -0.372  -0.335  -0.213  -0.042  +0.130  +0.239  +0.254  -0.298  -0.317  -0.302  -0.221  -0.077  +0.060  +0.125  +0.154  -0.224  -0.253  -0.264  -0.189  -0.081  -0.013  +0.072  +0.093\n",
      "                         :   var24:  +0.254  +0.258  +0.218  +0.123  -0.012  -0.098  -0.177  -0.165  +0.328  +0.331  +0.269  +0.152  -0.011  -0.154  -0.220  -0.249  +0.384  +0.374  +0.326  +0.134  -0.061  -0.238  -0.328  -0.325  +1.000  +0.417  +0.316  +0.143  -0.130  -0.308  -0.367  -0.355  +0.386  +0.365  +0.286  +0.091  -0.154  -0.319  -0.390  -0.374  +0.309  +0.302  +0.193  +0.031  -0.174  -0.317  -0.365  -0.358  +0.233  +0.214  +0.143  -0.013  -0.168  -0.265  -0.344  -0.274  +0.119  +0.126  +0.076  -0.035  -0.138  -0.208  -0.242  -0.209\n",
      "                         :   var25:  +0.256  +0.269  +0.213  +0.118  +0.016  -0.102  -0.165  -0.177  +0.336  +0.341  +0.295  +0.159  -0.007  -0.137  -0.214  -0.261  +0.370  +0.384  +0.334  +0.179  -0.057  -0.228  -0.305  -0.318  +0.417  +1.000  +0.350  +0.161  -0.103  -0.321  -0.396  -0.362  +0.378  +0.362  +0.309  +0.095  -0.171  -0.347  -0.403  -0.399  +0.331  +0.292  +0.196  +0.035  -0.180  -0.349  -0.401  -0.386  +0.231  +0.199  +0.134  -0.016  -0.175  -0.283  -0.342  -0.315  +0.123  +0.115  +0.063  -0.052  -0.163  -0.235  -0.280  -0.247\n",
      "                         :   var26:  +0.243  +0.238  +0.213  +0.144  +0.041  -0.047  -0.107  -0.107  +0.283  +0.303  +0.263  +0.179  +0.037  -0.092  -0.181  -0.188  +0.308  +0.336  +0.321  +0.176  +0.022  -0.168  -0.247  -0.277  +0.316  +0.350  +1.000  +0.140  -0.077  -0.241  -0.303  -0.301  +0.307  +0.294  +0.250  +0.085  -0.149  -0.307  -0.350  -0.359  +0.226  +0.222  +0.113  +0.018  -0.195  -0.304  -0.350  -0.344  +0.168  +0.117  +0.070  -0.047  -0.163  -0.288  -0.319  -0.302  +0.091  +0.068  -0.005  -0.078  -0.168  -0.224  -0.282  -0.212\n",
      "                         :   var27:  +0.126  +0.154  +0.136  +0.136  +0.099  +0.061  +0.001  +0.003  +0.162  +0.180  +0.178  +0.163  +0.091  +0.043  -0.011  -0.041  +0.133  +0.155  +0.161  +0.114  +0.090  -0.006  -0.037  -0.091  +0.143  +0.161  +0.140  +1.000  +0.026  -0.090  -0.098  -0.135  +0.081  +0.094  +0.045  -0.005  -0.088  -0.139  -0.148  -0.158  +0.035  +0.021  -0.003  -0.072  -0.154  -0.170  -0.192  -0.178  -0.014  -0.002  -0.061  -0.109  -0.175  -0.205  -0.220  -0.162  -0.024  -0.091  -0.082  -0.141  -0.173  -0.172  -0.186  -0.139\n",
      "                         :   var28:  -0.008  +0.015  +0.069  +0.106  +0.135  +0.171  +0.178  +0.131  -0.046  +0.006  +0.059  +0.098  +0.174  +0.183  +0.190  +0.160  -0.092  -0.048  -0.001  +0.087  +0.154  +0.154  +0.183  +0.163  -0.130  -0.103  -0.077  +0.026  +1.000  +0.150  +0.172  +0.134  -0.172  -0.177  -0.151  -0.084  +0.010  +0.055  +0.099  +0.102  -0.191  -0.198  -0.217  -0.136  -0.097  -0.010  +0.060  +0.062  -0.184  -0.197  -0.201  -0.159  -0.119  -0.064  -0.032  -0.008  -0.150  -0.195  -0.197  -0.172  -0.131  -0.089  -0.059  -0.034\n",
      "                         :   var29:  -0.129  -0.116  -0.053  +0.031  +0.141  +0.210  +0.236  +0.226  -0.180  -0.161  -0.088  +0.034  +0.171  +0.275  +0.289  +0.294  -0.244  -0.249  -0.160  -0.016  +0.172  +0.301  +0.340  +0.323  -0.308  -0.321  -0.241  -0.090  +0.150  +1.000  +0.335  +0.320  -0.339  -0.354  -0.316  -0.165  +0.074  +0.248  +0.287  +0.305  -0.338  -0.339  -0.306  -0.171  -0.029  +0.160  +0.205  +0.244  -0.285  -0.311  -0.286  -0.182  -0.034  +0.077  +0.144  +0.154  -0.223  -0.245  -0.233  -0.139  -0.070  +0.017  +0.087  +0.103\n",
      "                         :   var30:  -0.168  -0.166  -0.105  +0.001  +0.134  +0.207  +0.263  +0.254  -0.238  -0.218  -0.163  -0.002  +0.136  +0.279  +0.349  +0.324  -0.294  -0.290  -0.240  -0.048  +0.158  +0.321  +0.394  +0.375  -0.367  -0.396  -0.303  -0.098  +0.172  +0.335  +1.000  +0.419  -0.389  -0.407  -0.334  -0.156  +0.101  +0.296  +0.365  +0.358  -0.387  -0.372  -0.323  -0.184  +0.031  +0.218  +0.302  +0.317  -0.307  -0.306  -0.289  -0.183  -0.020  +0.123  +0.201  +0.212  -0.252  -0.262  -0.232  -0.140  -0.033  +0.054  +0.138  +0.131\n",
      "                         :   var31:  -0.184  -0.186  -0.112  -0.036  +0.096  +0.192  +0.244  +0.243  -0.247  -0.236  -0.167  -0.033  +0.112  +0.255  +0.308  +0.334  -0.307  -0.297  -0.242  -0.088  +0.150  +0.292  +0.389  +0.374  -0.355  -0.362  -0.301  -0.135  +0.134  +0.320  +0.419  +1.000  -0.399  -0.386  -0.344  -0.171  +0.087  +0.290  +0.367  +0.381  -0.352  -0.353  -0.304  -0.178  +0.025  +0.224  +0.322  +0.322  -0.299  -0.291  -0.272  -0.135  +0.004  +0.154  +0.235  +0.223  -0.202  -0.230  -0.207  -0.108  -0.009  +0.088  +0.151  +0.160\n",
      "                         :   var32:  +0.158  +0.154  +0.097  -0.016  -0.128  -0.202  -0.240  -0.225  +0.256  +0.207  +0.143  +0.005  -0.152  -0.261  -0.325  -0.296  +0.315  +0.288  +0.203  +0.038  -0.174  -0.305  -0.371  -0.377  +0.386  +0.378  +0.307  +0.081  -0.172  -0.339  -0.389  -0.399  +1.000  +0.410  +0.336  +0.152  -0.117  -0.315  -0.381  -0.373  +0.390  +0.384  +0.306  +0.133  -0.077  -0.251  -0.331  -0.324  +0.334  +0.321  +0.288  +0.134  -0.040  -0.176  -0.248  -0.257  +0.241  +0.261  +0.217  +0.100  +0.011  -0.105  -0.189  -0.182\n",
      "                         :   var33:  +0.138  +0.123  +0.075  -0.034  -0.156  -0.208  -0.260  -0.259  +0.207  +0.205  +0.115  -0.005  -0.170  -0.293  -0.331  -0.331  +0.319  +0.295  +0.205  +0.039  -0.176  -0.328  -0.392  -0.361  +0.365  +0.362  +0.294  +0.094  -0.177  -0.354  -0.407  -0.386  +0.410  +1.000  +0.345  +0.172  -0.100  -0.294  -0.362  -0.371  +0.385  +0.375  +0.312  +0.185  -0.059  -0.251  -0.326  -0.316  +0.321  +0.317  +0.283  +0.151  -0.023  -0.175  -0.233  -0.239  +0.229  +0.257  +0.209  +0.095  +0.001  -0.096  -0.192  -0.171\n",
      "                         :   var34:  +0.081  +0.066  +0.010  -0.075  -0.147  -0.222  -0.246  -0.233  +0.150  +0.118  +0.058  -0.057  -0.187  -0.285  -0.308  -0.291  +0.203  +0.197  +0.139  -0.011  -0.196  -0.316  -0.350  -0.322  +0.286  +0.309  +0.250  +0.045  -0.151  -0.316  -0.334  -0.344  +0.336  +0.345  +1.000  +0.170  -0.083  -0.242  -0.316  -0.304  +0.351  +0.343  +0.326  +0.187  -0.001  -0.175  -0.234  -0.257  +0.298  +0.305  +0.272  +0.175  +0.035  -0.107  -0.173  -0.200  +0.244  +0.242  +0.235  +0.134  +0.068  -0.052  -0.121  -0.114\n",
      "                         :   var35:  -0.024  -0.045  -0.082  -0.109  -0.153  -0.171  -0.166  -0.141  +0.001  -0.001  -0.084  -0.115  -0.168  -0.181  -0.200  -0.175  +0.038  +0.046  -0.006  -0.063  -0.145  -0.196  -0.198  -0.174  +0.091  +0.095  +0.085  -0.005  -0.084  -0.165  -0.156  -0.171  +0.152  +0.172  +0.170  +1.000  -0.008  -0.079  -0.111  -0.141  +0.156  +0.157  +0.187  +0.144  +0.086  -0.021  -0.045  -0.088  +0.147  +0.183  +0.175  +0.170  +0.093  +0.022  -0.007  -0.043  +0.116  +0.145  +0.145  +0.143  +0.077  +0.052  +0.020  -0.009\n",
      "                         :   var36:  -0.160  -0.189  -0.197  -0.146  -0.133  -0.090  -0.056  -0.046  -0.176  -0.236  -0.193  -0.162  -0.120  -0.066  -0.010  -0.003  -0.173  -0.188  -0.192  -0.149  -0.066  -0.020  +0.028  +0.046  -0.154  -0.171  -0.149  -0.088  +0.010  +0.074  +0.101  +0.087  -0.117  -0.100  -0.083  -0.008  +1.000  +0.162  +0.157  +0.140  -0.080  -0.058  +0.008  +0.093  +0.153  +0.186  +0.187  +0.155  -0.061  +0.011  +0.034  +0.106  +0.164  +0.177  +0.188  +0.140  -0.015  -0.001  +0.068  +0.109  +0.167  +0.161  +0.154  +0.125\n",
      "                         :   var37:  -0.222  -0.248  -0.258  -0.167  -0.097  -0.001  +0.063  +0.094  -0.294  -0.321  -0.286  -0.191  -0.076  +0.051  +0.134  +0.164  -0.326  -0.341  -0.306  -0.208  -0.014  +0.127  +0.225  +0.235  -0.319  -0.347  -0.307  -0.139  +0.055  +0.248  +0.296  +0.290  -0.315  -0.294  -0.242  -0.079  +0.162  +1.000  +0.330  +0.323  -0.236  -0.229  -0.148  +0.014  +0.181  +0.321  +0.356  +0.331  -0.193  -0.153  -0.081  +0.037  +0.163  +0.286  +0.303  +0.271  -0.122  -0.095  -0.047  +0.045  +0.155  +0.216  +0.251  +0.210\n",
      "                         :   var38:  -0.260  -0.273  -0.219  -0.148  -0.032  +0.055  +0.124  +0.142  -0.308  -0.333  -0.294  -0.177  -0.022  +0.127  +0.211  +0.218  -0.371  -0.375  -0.342  -0.185  +0.008  +0.192  +0.317  +0.301  -0.390  -0.403  -0.350  -0.148  +0.099  +0.287  +0.365  +0.367  -0.381  -0.362  -0.316  -0.111  +0.157  +0.330  +1.000  +0.409  -0.320  -0.305  -0.211  -0.046  +0.173  +0.353  +0.397  +0.389  -0.240  -0.206  -0.144  +0.007  +0.174  +0.280  +0.352  +0.326  -0.157  -0.148  -0.115  +0.017  +0.124  +0.229  +0.282  +0.238\n",
      "                         :   var39:  -0.223  -0.224  -0.189  -0.114  -0.007  +0.093  +0.152  +0.178  -0.285  -0.299  -0.263  -0.138  +0.034  +0.163  +0.222  +0.259  -0.338  -0.359  -0.312  -0.164  +0.051  +0.224  +0.311  +0.333  -0.374  -0.399  -0.359  -0.158  +0.102  +0.305  +0.358  +0.381  -0.373  -0.371  -0.304  -0.141  +0.140  +0.323  +0.409  +1.000  -0.313  -0.307  -0.256  -0.084  +0.138  +0.319  +0.378  +0.368  -0.244  -0.226  -0.186  -0.055  +0.120  +0.270  +0.313  +0.288  -0.159  -0.157  -0.128  -0.023  +0.106  +0.201  +0.249  +0.218\n",
      "                         :   var40:  +0.093  +0.070  -0.009  -0.095  -0.185  -0.249  -0.272  -0.240  +0.173  +0.114  +0.046  -0.087  -0.214  -0.319  -0.319  -0.304  +0.244  +0.201  +0.115  -0.028  -0.223  -0.321  -0.389  -0.352  +0.309  +0.331  +0.226  +0.035  -0.191  -0.338  -0.387  -0.352  +0.390  +0.385  +0.351  +0.156  -0.080  -0.236  -0.320  -0.313  +1.000  +0.387  +0.354  +0.204  +0.010  -0.157  -0.247  -0.263  +0.356  +0.349  +0.327  +0.208  +0.055  -0.081  -0.170  -0.182  +0.280  +0.325  +0.281  +0.173  +0.069  -0.022  -0.106  -0.116\n",
      "                         :   var41:  +0.040  +0.013  -0.067  -0.145  -0.233  -0.296  -0.313  -0.267  +0.128  +0.055  -0.012  -0.143  -0.273  -0.354  -0.366  -0.340  +0.211  +0.179  +0.092  -0.081  -0.265  -0.356  -0.397  -0.372  +0.302  +0.292  +0.222  +0.021  -0.198  -0.339  -0.372  -0.353  +0.384  +0.375  +0.343  +0.157  -0.058  -0.229  -0.305  -0.307  +0.387  +1.000  +0.384  +0.255  +0.061  -0.127  -0.228  -0.229  +0.368  +0.387  +0.384  +0.267  +0.107  -0.046  -0.113  -0.165  +0.305  +0.338  +0.323  +0.243  +0.131  +0.025  -0.075  -0.090\n",
      "                         :   var42:  -0.028  -0.066  -0.157  -0.243  -0.317  -0.328  -0.316  -0.277  +0.028  -0.024  -0.130  -0.230  -0.336  -0.378  -0.363  -0.329  +0.125  +0.067  -0.025  -0.171  -0.312  -0.367  -0.393  -0.335  +0.193  +0.196  +0.113  -0.003  -0.217  -0.306  -0.323  -0.304  +0.306  +0.312  +0.326  +0.187  +0.008  -0.148  -0.211  -0.256  +0.354  +0.384  +1.000  +0.300  +0.151  -0.037  -0.111  -0.173  +0.344  +0.370  +0.403  +0.334  +0.214  +0.069  -0.000  -0.069  +0.302  +0.351  +0.371  +0.305  +0.227  +0.132  +0.026  -0.013\n",
      "                         :   var43:  -0.120  -0.185  -0.258  -0.285  -0.320  -0.314  -0.295  -0.239  -0.104  -0.165  -0.233  -0.295  -0.336  -0.349  -0.301  -0.261  -0.053  -0.093  -0.147  -0.215  -0.292  -0.306  -0.280  -0.213  +0.031  +0.035  +0.018  -0.072  -0.136  -0.171  -0.184  -0.178  +0.133  +0.185  +0.187  +0.144  +0.093  +0.014  -0.046  -0.084  +0.204  +0.255  +0.300  +1.000  +0.242  +0.111  +0.054  +0.013  +0.221  +0.293  +0.336  +0.343  +0.275  +0.208  +0.130  +0.045  +0.235  +0.272  +0.335  +0.316  +0.279  +0.216  +0.157  +0.083\n",
      "                         :   var44:  -0.226  -0.281  -0.341  -0.341  -0.313  -0.253  -0.191  -0.128  -0.259  -0.315  -0.345  -0.362  -0.316  -0.254  -0.172  -0.105  -0.228  -0.266  -0.317  -0.292  -0.260  -0.156  -0.083  -0.042  -0.174  -0.180  -0.195  -0.154  -0.097  -0.029  +0.031  +0.025  -0.077  -0.059  -0.001  +0.086  +0.153  +0.181  +0.173  +0.138  +0.010  +0.061  +0.151  +0.242  +1.000  +0.317  +0.283  +0.220  +0.054  +0.134  +0.212  +0.302  +0.340  +0.356  +0.310  +0.234  +0.107  +0.148  +0.239  +0.298  +0.350  +0.338  +0.305  +0.209\n",
      "                         :   var45:  -0.277  -0.309  -0.343  -0.307  -0.223  -0.159  -0.081  -0.030  -0.330  -0.391  -0.392  -0.318  -0.219  -0.105  -0.017  +0.056  -0.345  -0.394  -0.379  -0.319  -0.162  +0.005  +0.093  +0.130  -0.317  -0.349  -0.304  -0.170  -0.010  +0.160  +0.218  +0.224  -0.251  -0.251  -0.175  -0.021  +0.186  +0.321  +0.353  +0.319  -0.157  -0.127  -0.037  +0.111  +0.317  +1.000  +0.413  +0.347  -0.093  -0.014  +0.070  +0.202  +0.335  +0.408  +0.403  +0.335  -0.036  +0.027  +0.102  +0.192  +0.312  +0.349  +0.358  +0.290\n",
      "                         :   var46:  -0.276  -0.340  -0.311  -0.268  -0.168  -0.090  +0.012  +0.076  -0.346  -0.385  -0.382  -0.295  -0.154  -0.011  +0.089  +0.139  -0.372  -0.398  -0.390  -0.282  -0.113  +0.075  +0.193  +0.239  -0.365  -0.401  -0.350  -0.192  +0.060  +0.205  +0.302  +0.322  -0.331  -0.326  -0.234  -0.045  +0.187  +0.356  +0.397  +0.378  -0.247  -0.228  -0.111  +0.054  +0.283  +0.413  +1.000  +0.421  -0.161  -0.114  -0.042  +0.131  +0.283  +0.383  +0.422  +0.375  -0.088  -0.058  +0.020  +0.140  +0.272  +0.336  +0.356  +0.290\n",
      "                         :   var47:  -0.231  -0.284  -0.269  -0.209  -0.111  -0.013  +0.045  +0.102  -0.309  -0.336  -0.317  -0.227  -0.116  +0.031  +0.130  +0.178  -0.339  -0.376  -0.337  -0.222  -0.048  +0.124  +0.231  +0.254  -0.358  -0.386  -0.344  -0.178  +0.062  +0.244  +0.317  +0.322  -0.324  -0.316  -0.257  -0.088  +0.155  +0.331  +0.389  +0.368  -0.263  -0.229  -0.173  +0.013  +0.220  +0.347  +0.421  +1.000  -0.185  -0.147  -0.098  +0.066  +0.224  +0.337  +0.373  +0.329  -0.110  -0.091  -0.036  +0.080  +0.190  +0.261  +0.326  +0.257\n",
      "                         :   var48:  +0.011  -0.013  -0.078  -0.157  -0.218  -0.263  -0.248  -0.228  +0.075  +0.033  -0.037  -0.137  -0.264  -0.304  -0.312  -0.258  +0.153  +0.121  +0.028  -0.096  -0.251  -0.317  -0.340  -0.298  +0.233  +0.231  +0.168  -0.014  -0.184  -0.285  -0.307  -0.299  +0.334  +0.321  +0.298  +0.147  -0.061  -0.193  -0.240  -0.244  +0.356  +0.368  +0.344  +0.221  +0.054  -0.093  -0.161  -0.185  +1.000  +0.340  +0.345  +0.252  +0.102  -0.005  -0.086  -0.133  +0.265  +0.317  +0.305  +0.219  +0.133  +0.041  -0.049  -0.066\n",
      "                         :   var49:  -0.016  -0.067  -0.164  -0.221  -0.289  -0.323  -0.308  -0.259  +0.006  -0.045  -0.124  -0.235  -0.329  -0.385  -0.354  -0.320  +0.095  +0.067  -0.021  -0.170  -0.317  -0.355  -0.367  -0.317  +0.214  +0.199  +0.117  -0.002  -0.197  -0.311  -0.306  -0.291  +0.321  +0.317  +0.305  +0.183  +0.011  -0.153  -0.206  -0.226  +0.349  +0.387  +0.370  +0.293  +0.134  -0.014  -0.114  -0.147  +0.340  +1.000  +0.404  +0.349  +0.211  +0.069  -0.011  -0.068  +0.293  +0.369  +0.370  +0.314  +0.234  +0.122  +0.022  -0.015\n",
      "                         :   var50:  -0.072  -0.145  -0.228  -0.309  -0.337  -0.360  -0.331  -0.282  -0.057  -0.122  -0.206  -0.292  -0.372  -0.404  -0.393  -0.320  +0.043  -0.015  -0.111  -0.220  -0.354  -0.369  -0.360  -0.302  +0.143  +0.134  +0.070  -0.061  -0.201  -0.286  -0.289  -0.272  +0.288  +0.283  +0.272  +0.175  +0.034  -0.081  -0.144  -0.186  +0.327  +0.384  +0.403  +0.336  +0.212  +0.070  -0.042  -0.098  +0.345  +0.404  +1.000  +0.379  +0.278  +0.144  +0.072  -0.007  +0.303  +0.374  +0.401  +0.367  +0.289  +0.203  +0.098  +0.035\n",
      "                         :   var51:  -0.181  -0.236  -0.327  -0.377  -0.386  -0.373  -0.306  -0.254  -0.177  -0.248  -0.324  -0.375  -0.414  -0.392  -0.332  -0.253  -0.115  -0.152  -0.229  -0.290  -0.351  -0.311  -0.272  -0.221  -0.013  -0.016  -0.047  -0.109  -0.159  -0.182  -0.183  -0.135  +0.134  +0.151  +0.175  +0.170  +0.106  +0.037  +0.007  -0.055  +0.208  +0.267  +0.334  +0.343  +0.302  +0.202  +0.131  +0.066  +0.252  +0.349  +0.379  +1.000  +0.371  +0.277  +0.228  +0.120  +0.251  +0.354  +0.402  +0.409  +0.371  +0.303  +0.217  +0.139\n",
      "                         :   var52:  -0.243  -0.307  -0.382  -0.388  -0.355  -0.299  -0.230  -0.169  -0.254  -0.336  -0.395  -0.403  -0.391  -0.299  -0.220  -0.161  -0.229  -0.280  -0.320  -0.348  -0.321  -0.224  -0.127  -0.077  -0.168  -0.175  -0.163  -0.175  -0.119  -0.034  -0.020  +0.004  -0.040  -0.023  +0.035  +0.093  +0.164  +0.163  +0.174  +0.120  +0.055  +0.107  +0.214  +0.275  +0.340  +0.335  +0.283  +0.224  +0.102  +0.211  +0.278  +0.371  +1.000  +0.397  +0.345  +0.242  +0.154  +0.225  +0.316  +0.357  +0.422  +0.385  +0.325  +0.270\n",
      "                         :   var53:  -0.265  -0.329  -0.360  -0.360  -0.306  -0.222  -0.147  -0.101  -0.320  -0.381  -0.398  -0.378  -0.315  -0.194  -0.109  -0.042  -0.310  -0.357  -0.370  -0.350  -0.237  -0.096  -0.015  +0.060  -0.265  -0.283  -0.288  -0.205  -0.064  +0.077  +0.123  +0.154  -0.176  -0.175  -0.107  +0.022  +0.177  +0.286  +0.280  +0.270  -0.081  -0.046  +0.069  +0.208  +0.356  +0.408  +0.383  +0.337  -0.005  +0.069  +0.144  +0.277  +0.397  +1.000  +0.413  +0.338  +0.033  +0.116  +0.203  +0.299  +0.393  +0.403  +0.389  +0.326\n",
      "                         :   var54:  -0.263  -0.322  -0.340  -0.310  -0.238  -0.158  -0.086  -0.038  -0.317  -0.399  -0.383  -0.335  -0.233  -0.126  -0.024  +0.032  -0.339  -0.370  -0.369  -0.318  -0.178  -0.028  +0.079  +0.125  -0.344  -0.342  -0.319  -0.220  -0.032  +0.144  +0.201  +0.235  -0.248  -0.233  -0.173  -0.007  +0.188  +0.303  +0.352  +0.313  -0.170  -0.113  -0.000  +0.130  +0.310  +0.403  +0.422  +0.373  -0.086  -0.011  +0.072  +0.228  +0.345  +0.413  +1.000  +0.363  -0.014  +0.039  +0.115  +0.225  +0.346  +0.377  +0.379  +0.324\n",
      "                         :   var55:  -0.220  -0.268  -0.267  -0.221  -0.165  -0.077  -0.012  +0.013  -0.259  -0.308  -0.308  -0.241  -0.163  -0.037  +0.039  +0.076  -0.288  -0.331  -0.311  -0.222  -0.094  +0.029  +0.124  +0.154  -0.274  -0.315  -0.302  -0.162  -0.008  +0.154  +0.212  +0.223  -0.257  -0.239  -0.200  -0.043  +0.140  +0.271  +0.326  +0.288  -0.182  -0.165  -0.069  +0.045  +0.234  +0.335  +0.375  +0.329  -0.133  -0.068  -0.007  +0.120  +0.242  +0.338  +0.363  +1.000  -0.065  -0.019  +0.045  +0.133  +0.244  +0.276  +0.330  +0.239\n",
      "                         :   var56:  -0.015  -0.053  -0.123  -0.179  -0.207  -0.226  -0.217  -0.192  +0.005  -0.032  -0.092  -0.171  -0.254  -0.290  -0.255  -0.192  +0.062  +0.052  -0.040  -0.118  -0.220  -0.268  -0.259  -0.224  +0.119  +0.123  +0.091  -0.024  -0.150  -0.223  -0.252  -0.202  +0.241  +0.229  +0.244  +0.116  -0.015  -0.122  -0.157  -0.159  +0.280  +0.305  +0.302  +0.235  +0.107  -0.036  -0.088  -0.110  +0.265  +0.293  +0.303  +0.251  +0.154  +0.033  -0.014  -0.065  +1.000  +0.287  +0.294  +0.232  +0.173  +0.104  -0.009  -0.010\n",
      "                         :   var57:  -0.087  -0.123  -0.200  -0.255  -0.281  -0.284  -0.270  -0.219  -0.036  -0.087  -0.183  -0.252  -0.323  -0.363  -0.306  -0.254  +0.041  -0.013  -0.085  -0.198  -0.298  -0.310  -0.309  -0.253  +0.126  +0.115  +0.068  -0.091  -0.195  -0.245  -0.262  -0.230  +0.261  +0.257  +0.242  +0.145  -0.001  -0.095  -0.148  -0.157  +0.325  +0.338  +0.351  +0.272  +0.148  +0.027  -0.058  -0.091  +0.317  +0.369  +0.374  +0.354  +0.225  +0.116  +0.039  -0.019  +0.287  +1.000  +0.362  +0.314  +0.262  +0.152  +0.069  +0.034\n",
      "                         :   var58:  -0.123  -0.166  -0.252  -0.317  -0.347  -0.349  -0.315  -0.257  -0.086  -0.159  -0.243  -0.319  -0.379  -0.367  -0.334  -0.277  -0.026  -0.071  -0.138  -0.258  -0.323  -0.340  -0.314  -0.264  +0.076  +0.063  -0.005  -0.082  -0.197  -0.233  -0.232  -0.207  +0.217  +0.209  +0.235  +0.145  +0.068  -0.047  -0.115  -0.128  +0.281  +0.323  +0.371  +0.335  +0.239  +0.102  +0.020  -0.036  +0.305  +0.370  +0.401  +0.402  +0.316  +0.203  +0.115  +0.045  +0.294  +0.362  +1.000  +0.383  +0.324  +0.232  +0.142  +0.084\n",
      "                         :   var59:  -0.168  -0.231  -0.330  -0.359  -0.357  -0.314  -0.292  -0.215  -0.187  -0.249  -0.315  -0.366  -0.378  -0.366  -0.298  -0.221  -0.114  -0.164  -0.246  -0.312  -0.335  -0.297  -0.239  -0.189  -0.035  -0.052  -0.078  -0.141  -0.172  -0.139  -0.140  -0.108  +0.100  +0.095  +0.134  +0.143  +0.109  +0.045  +0.017  -0.023  +0.173  +0.243  +0.305  +0.316  +0.298  +0.192  +0.140  +0.080  +0.219  +0.314  +0.367  +0.409  +0.357  +0.299  +0.225  +0.133  +0.232  +0.314  +0.383  +1.000  +0.365  +0.329  +0.227  +0.158\n",
      "                         :   var60:  -0.228  -0.295  -0.363  -0.378  -0.361  -0.320  -0.254  -0.182  -0.250  -0.319  -0.378  -0.387  -0.393  -0.319  -0.235  -0.166  -0.203  -0.275  -0.317  -0.348  -0.329  -0.236  -0.171  -0.081  -0.138  -0.163  -0.168  -0.173  -0.131  -0.070  -0.033  -0.009  +0.011  +0.001  +0.068  +0.077  +0.167  +0.155  +0.124  +0.106  +0.069  +0.131  +0.227  +0.279  +0.350  +0.312  +0.272  +0.190  +0.133  +0.234  +0.289  +0.371  +0.422  +0.393  +0.346  +0.244  +0.173  +0.262  +0.324  +0.365  +1.000  +0.393  +0.326  +0.243\n",
      "                         :   var61:  -0.231  -0.284  -0.333  -0.340  -0.318  -0.253  -0.190  -0.105  -0.271  -0.342  -0.379  -0.370  -0.323  -0.235  -0.151  -0.075  -0.253  -0.299  -0.344  -0.333  -0.266  -0.146  -0.080  -0.013  -0.208  -0.235  -0.224  -0.172  -0.089  +0.017  +0.054  +0.088  -0.105  -0.096  -0.052  +0.052  +0.161  +0.216  +0.229  +0.201  -0.022  +0.025  +0.132  +0.216  +0.338  +0.349  +0.336  +0.261  +0.041  +0.122  +0.203  +0.303  +0.385  +0.403  +0.377  +0.276  +0.104  +0.152  +0.232  +0.329  +0.393  +1.000  +0.361  +0.272\n",
      "                         :   var62:  -0.225  -0.262  -0.313  -0.298  -0.241  -0.173  -0.101  -0.062  -0.280  -0.334  -0.345  -0.323  -0.239  -0.139  -0.065  -0.014  -0.279  -0.334  -0.343  -0.298  -0.193  -0.057  +0.035  +0.072  -0.242  -0.280  -0.282  -0.186  -0.059  +0.087  +0.138  +0.151  -0.189  -0.192  -0.121  +0.020  +0.154  +0.251  +0.282  +0.249  -0.106  -0.075  +0.026  +0.157  +0.305  +0.358  +0.356  +0.326  -0.049  +0.022  +0.098  +0.217  +0.325  +0.389  +0.379  +0.330  -0.009  +0.069  +0.142  +0.227  +0.326  +0.361  +1.000  +0.289\n",
      "                         :   var63:  -0.170  -0.217  -0.257  -0.210  -0.177  -0.105  -0.061  -0.002  -0.227  -0.254  -0.261  -0.229  -0.167  -0.080  -0.009  +0.018  -0.235  -0.258  -0.277  -0.239  -0.143  -0.022  +0.060  +0.093  -0.209  -0.247  -0.212  -0.139  -0.034  +0.103  +0.131  +0.160  -0.182  -0.171  -0.114  -0.009  +0.125  +0.210  +0.238  +0.218  -0.116  -0.090  -0.013  +0.083  +0.209  +0.290  +0.290  +0.257  -0.066  -0.015  +0.035  +0.139  +0.270  +0.326  +0.324  +0.239  -0.010  +0.034  +0.084  +0.158  +0.243  +0.272  +0.289  +1.000\n",
      "                         : ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "DataSetFactory           : [dataset] :  \n",
      "                         : \n"
     ]
    }
   ],
   "source": [
    "//Boosted Decision Trees\n",
    "factory.BookMethod(loader,TMVA::Types::kBDT, \"BDT\",\n",
    "                   \"!V:NTrees=800:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks (Dense and Convolutional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool useDNN = true; \n",
    "bool useCNN = true; \n",
    "bool useKeras = false; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mDL_DENSE\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|1|64:BatchLayout=1|128|64:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=L2,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=Standard\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|1|64:BatchLayout=1|128|64:Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=L2,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.:Architecture=Standard\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|1|64\" [The Layout of the input]\n",
      "                         :     BatchLayout: \"1|128|64\" [The Layout of the batch]\n",
      "                         :     Layout: \"DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"STANDARD\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=L2,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         : The STANDARD architecture has been deprecated. Please use Architecture=CPU or Architecture=CPU.See the TMVA Users' Guide for instructions if you encounter problems.\n",
      "                         : Will use the deprecated STANDARD architecture !\n"
     ]
    }
   ],
   "source": [
    "if (useDNN) { \n",
    "    \n",
    "     TString inputLayoutString = \"InputLayout=1|1|64\"; \n",
    "     TString batchLayoutString = \"BatchLayout=1|128|64\";\n",
    "     TString layoutString (\"Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR\");\n",
    "                                                                                                                                                                                       \n",
    "      //Training strategy\n",
    "      TString training1(\"LearningRate=1e-3,Momentum=0.9,Repetitions=1,\"\n",
    "                        \"ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,\"\n",
    "                        \"MaxEpochs=20,WeightDecay=1e-4,Regularization=L2,\"\n",
    "                        \"Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.\");\n",
    "  \n",
    "      TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "      trainingStrategyString += training1; // + \"|\" + training2 + \"|\" + training3;\n",
    "\n",
    "      //Options                                                                                                                                                                \n",
    "      TString dnnOptions(\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"\n",
    "                          \"WeightInitialization=XAVIERUNIFORM\");\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (inputLayoutString);\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (batchLayoutString);\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (layoutString);\n",
    "      dnnOptions.Append (\":\"); dnnOptions.Append (trainingStrategyString);\n",
    "\n",
    "      dnnOptions += \":Architecture=Standard\";\n",
    "      factory.BookMethod(loader, TMVA::Types::kDL, \"DL_DENSE\", dnnOptions);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Booking method: \u001b[1mDL_CNN\u001b[0m\n",
      "                         : \n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|8|8:BatchLayout=128|1|64:Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     <none>\n",
      "                         : - Default:\n",
      "                         :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "                         : Parsing option string: \n",
      "                         : ... \"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:WeightInitialization=XAVIERUNIFORM:InputLayout=1|8|8:BatchLayout=128|1|64:Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR:TrainingStrategy=LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0:Architecture=CPU\"\n",
      "                         : The following options are set:\n",
      "                         : - By User:\n",
      "                         :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "                         :     VarTransform: \"None\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "                         :     H: \"False\" [Print method-specific help message]\n",
      "                         :     InputLayout: \"1|8|8\" [The Layout of the input]\n",
      "                         :     BatchLayout: \"128|1|64\" [The Layout of the batch]\n",
      "                         :     Layout: \"CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR\" [Layout of the network.]\n",
      "                         :     ErrorStrategy: \"CROSSENTROPY\" [Loss function: Mean squared error (regression) or cross entropy (binary classification).]\n",
      "                         :     WeightInitialization: \"XAVIERUNIFORM\" [Weight initialization strategy]\n",
      "                         :     Architecture: \"CPU\" [Which architecture to perform the training on.]\n",
      "                         :     TrainingStrategy: \"LearningRate=1e-3,Momentum=0.9,Repetitions=1,ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,MaxEpochs=20,WeightDecay=1e-4,Regularization=None,Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0\" [Defines the training strategies.]\n",
      "                         : - Default:\n",
      "                         :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "                         :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "                         :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "                         :     RandomSeed: \"0\" [Random seed used for weight initialization and batch shuffling]\n",
      "                         : Will use now the CPU architecture !\n"
     ]
    }
   ],
   "source": [
    "if (useCNN) { \n",
    "    \n",
    "    TString inputLayoutString(\"InputLayout=1|8|8\");                                                                                                                                     \n",
    "    TString batchLayoutString(\"BatchLayout=128|1|64\");\n",
    "    TString layoutString     (\"Layout=CONV|10|3|3|1|1|1|1|RELU,CONV|10|3|3|1|1|1|1|RELU,MAXPOOL|2|2|1|1,\"\n",
    "                              \"RESHAPE|FLAT,DENSE|64|TANH,DENSE|1|LINEAR\");\n",
    "\n",
    "    //Training strategy                                                                                                                          \n",
    "    TString training1(\"LearningRate=1e-3,Momentum=0.9,Repetitions=1,\"\n",
    "                     \"ConvergenceSteps=20,BatchSize=128,TestRepetitions=1,\"\n",
    "                     \"MaxEpochs=20,WeightDecay=1e-4,Regularization=None,\"\n",
    "                     \"Optimizer=ADAM,DropConfig=0.0+0.0+0.0+0.0\");\n",
    "    TString trainingStrategyString (\"TrainingStrategy=\");\n",
    "    trainingStrategyString += training1; // + \"|\" + training1 + \"|\" + training2;   }\n",
    "    \n",
    "    //Options                                                                                                                         \n",
    "    TString cnnOptions (\"!H:V:ErrorStrategy=CROSSENTROPY:VarTransform=None:\"\n",
    "                       \"WeightInitialization=XAVIERUNIFORM\");\n",
    "\n",
    "    cnnOptions.Append(\":\"); cnnOptions.Append(inputLayoutString);\n",
    "    cnnOptions.Append(\":\"); cnnOptions.Append(batchLayoutString);\n",
    "    cnnOptions.Append(\":\"); cnnOptions.Append(layoutString);\n",
    "    cnnOptions.Append(\":\"); cnnOptions.Append(trainingStrategyString);\n",
    "    cnnOptions.Append(\":Architecture=CPU\");\n",
    "\n",
    "    factory.BookMethod(loader, TMVA::Types::kDL, \"DL_CNN\", cnnOptions);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mTrain all methods\u001b[0m\n",
      "Factory                  : [dataset] : Create Transformation \"I\" with events from all classes.\n",
      "                         : \n",
      "                         : Transformation, Variable selection : \n",
      "                         : Input : variable 'var0' <---> Output : variable 'var0'\n",
      "                         : Input : variable 'var1' <---> Output : variable 'var1'\n",
      "                         : Input : variable 'var2' <---> Output : variable 'var2'\n",
      "                         : Input : variable 'var3' <---> Output : variable 'var3'\n",
      "                         : Input : variable 'var4' <---> Output : variable 'var4'\n",
      "                         : Input : variable 'var5' <---> Output : variable 'var5'\n",
      "                         : Input : variable 'var6' <---> Output : variable 'var6'\n",
      "                         : Input : variable 'var7' <---> Output : variable 'var7'\n",
      "                         : Input : variable 'var8' <---> Output : variable 'var8'\n",
      "                         : Input : variable 'var9' <---> Output : variable 'var9'\n",
      "                         : Input : variable 'var10' <---> Output : variable 'var10'\n",
      "                         : Input : variable 'var11' <---> Output : variable 'var11'\n",
      "                         : Input : variable 'var12' <---> Output : variable 'var12'\n",
      "                         : Input : variable 'var13' <---> Output : variable 'var13'\n",
      "                         : Input : variable 'var14' <---> Output : variable 'var14'\n",
      "                         : Input : variable 'var15' <---> Output : variable 'var15'\n",
      "                         : Input : variable 'var16' <---> Output : variable 'var16'\n",
      "                         : Input : variable 'var17' <---> Output : variable 'var17'\n",
      "                         : Input : variable 'var18' <---> Output : variable 'var18'\n",
      "                         : Input : variable 'var19' <---> Output : variable 'var19'\n",
      "                         : Input : variable 'var20' <---> Output : variable 'var20'\n",
      "                         : Input : variable 'var21' <---> Output : variable 'var21'\n",
      "                         : Input : variable 'var22' <---> Output : variable 'var22'\n",
      "                         : Input : variable 'var23' <---> Output : variable 'var23'\n",
      "                         : Input : variable 'var24' <---> Output : variable 'var24'\n",
      "                         : Input : variable 'var25' <---> Output : variable 'var25'\n",
      "                         : Input : variable 'var26' <---> Output : variable 'var26'\n",
      "                         : Input : variable 'var27' <---> Output : variable 'var27'\n",
      "                         : Input : variable 'var28' <---> Output : variable 'var28'\n",
      "                         : Input : variable 'var29' <---> Output : variable 'var29'\n",
      "                         : Input : variable 'var30' <---> Output : variable 'var30'\n",
      "                         : Input : variable 'var31' <---> Output : variable 'var31'\n",
      "                         : Input : variable 'var32' <---> Output : variable 'var32'\n",
      "                         : Input : variable 'var33' <---> Output : variable 'var33'\n",
      "                         : Input : variable 'var34' <---> Output : variable 'var34'\n",
      "                         : Input : variable 'var35' <---> Output : variable 'var35'\n",
      "                         : Input : variable 'var36' <---> Output : variable 'var36'\n",
      "                         : Input : variable 'var37' <---> Output : variable 'var37'\n",
      "                         : Input : variable 'var38' <---> Output : variable 'var38'\n",
      "                         : Input : variable 'var39' <---> Output : variable 'var39'\n",
      "                         : Input : variable 'var40' <---> Output : variable 'var40'\n",
      "                         : Input : variable 'var41' <---> Output : variable 'var41'\n",
      "                         : Input : variable 'var42' <---> Output : variable 'var42'\n",
      "                         : Input : variable 'var43' <---> Output : variable 'var43'\n",
      "                         : Input : variable 'var44' <---> Output : variable 'var44'\n",
      "                         : Input : variable 'var45' <---> Output : variable 'var45'\n",
      "                         : Input : variable 'var46' <---> Output : variable 'var46'\n",
      "                         : Input : variable 'var47' <---> Output : variable 'var47'\n",
      "                         : Input : variable 'var48' <---> Output : variable 'var48'\n",
      "                         : Input : variable 'var49' <---> Output : variable 'var49'\n",
      "                         : Input : variable 'var50' <---> Output : variable 'var50'\n",
      "                         : Input : variable 'var51' <---> Output : variable 'var51'\n",
      "                         : Input : variable 'var52' <---> Output : variable 'var52'\n",
      "                         : Input : variable 'var53' <---> Output : variable 'var53'\n",
      "                         : Input : variable 'var54' <---> Output : variable 'var54'\n",
      "                         : Input : variable 'var55' <---> Output : variable 'var55'\n",
      "                         : Input : variable 'var56' <---> Output : variable 'var56'\n",
      "                         : Input : variable 'var57' <---> Output : variable 'var57'\n",
      "                         : Input : variable 'var58' <---> Output : variable 'var58'\n",
      "                         : Input : variable 'var59' <---> Output : variable 'var59'\n",
      "                         : Input : variable 'var60' <---> Output : variable 'var60'\n",
      "                         : Input : variable 'var61' <---> Output : variable 'var61'\n",
      "                         : Input : variable 'var62' <---> Output : variable 'var62'\n",
      "                         : Input : variable 'var63' <---> Output : variable 'var63'\n",
      "TFHandler_Factory        : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4991     3.8926   [    -10.257     22.057 ]\n",
      "                         :     var1:     3.9787     4.5075   [    -10.122     25.016 ]\n",
      "                         :     var2:     5.5671     4.8848   [    -9.4795     26.928 ]\n",
      "                         :     var3:     6.5379     5.0192   [    -9.7689     29.230 ]\n",
      "                         :     var4:     6.3871     4.9931   [    -11.476     28.503 ]\n",
      "                         :     var5:     5.4269     4.8235   [    -9.1293     30.122 ]\n",
      "                         :     var6:     3.9107     4.4655   [    -11.141     25.933 ]\n",
      "                         :     var7:     2.4388     3.8766   [    -9.8326     22.021 ]\n",
      "                         :     var8:     3.7375     4.4169   [    -9.0364     24.243 ]\n",
      "                         :     var9:     6.0120     5.1559   [    -9.2157     28.235 ]\n",
      "                         :    var10:     8.3558     5.4951   [    -7.3822     29.694 ]\n",
      "                         :    var11:     9.8310     5.5008   [    -7.2496     32.294 ]\n",
      "                         :    var12:     9.7061     5.5019   [    -9.2380     31.239 ]\n",
      "                         :    var13:     8.2453     5.5157   [    -8.3455     31.442 ]\n",
      "                         :    var14:     5.9694     5.0886   [    -9.2506     29.027 ]\n",
      "                         :    var15:     3.6435     4.3955   [    -10.587     23.653 ]\n",
      "                         :    var16:     4.8733     4.7778   [    -11.709     25.905 ]\n",
      "                         :    var17:     7.9688     5.5580   [    -7.5348     33.189 ]\n",
      "                         :    var18:     10.951     5.7539   [    -7.3453     34.627 ]\n",
      "                         :    var19:     12.853     5.4921   [    -4.1117     33.786 ]\n",
      "                         :    var20:     12.820     5.4867   [    -5.3808     32.315 ]\n",
      "                         :    var21:     10.889     5.7651   [    -6.3743     36.072 ]\n",
      "                         :    var22:     7.9053     5.5376   [    -7.9329     32.559 ]\n",
      "                         :    var23:     4.7962     4.7362   [    -10.919     23.587 ]\n",
      "                         :    var24:     5.6555     5.0342   [    -9.8326     29.659 ]\n",
      "                         :    var25:     9.1977     5.6658   [    -6.3310     34.142 ]\n",
      "                         :    var26:     12.554     5.6514   [    -5.9403     33.692 ]\n",
      "                         :    var27:     14.795     5.1093   [    -2.1615     37.430 ]\n",
      "                         :    var28:     14.660     5.2013   [    -2.8283     38.805 ]\n",
      "                         :    var29:     12.449     5.6721   [    -4.6051     37.288 ]\n",
      "                         :    var30:     9.0063     5.7010   [    -10.813     33.124 ]\n",
      "                         :    var31:     5.4970     4.9834   [    -10.336     30.010 ]\n",
      "                         :    var32:     5.6520     5.0125   [    -9.2211     27.428 ]\n",
      "                         :    var33:     9.1908     5.7259   [    -8.0987     33.206 ]\n",
      "                         :    var34:     12.684     5.6427   [    -5.9139     36.323 ]\n",
      "                         :    var35:     14.812     5.1435   [    -3.2133     34.607 ]\n",
      "                         :    var36:     14.707     5.1390   [    -2.8380     35.026 ]\n",
      "                         :    var37:     12.530     5.5950   [    -6.1929     33.746 ]\n",
      "                         :    var38:     9.0409     5.7810   [    -8.0031     32.626 ]\n",
      "                         :    var39:     5.5780     4.9512   [    -8.8630     29.784 ]\n",
      "                         :    var40:     4.9770     4.7758   [    -10.145     25.266 ]\n",
      "                         :    var41:     7.8714     5.5548   [    -8.5314     37.635 ]\n",
      "                         :    var42:     11.016     5.7577   [    -8.1448     36.124 ]\n",
      "                         :    var43:     12.821     5.4932   [    -6.6966     43.293 ]\n",
      "                         :    var44:     12.887     5.5346   [    -4.9059     34.631 ]\n",
      "                         :    var45:     10.988     5.8018   [    -6.8219     34.300 ]\n",
      "                         :    var46:     7.9529     5.5516   [    -10.776     32.909 ]\n",
      "                         :    var47:     4.8126     4.7994   [    -9.0429     27.598 ]\n",
      "                         :    var48:     3.8114     4.3913   [    -10.010     25.702 ]\n",
      "                         :    var49:     6.0972     5.0664   [    -8.8429     30.707 ]\n",
      "                         :    var50:     8.4053     5.4578   [    -8.4494     30.113 ]\n",
      "                         :    var51:     9.8185     5.5505   [    -8.6899     33.313 ]\n",
      "                         :    var52:     9.8044     5.5527   [    -6.3026     33.478 ]\n",
      "                         :    var53:     8.2980     5.4937   [    -8.6684     35.304 ]\n",
      "                         :    var54:     5.9857     5.0800   [    -8.5084     28.695 ]\n",
      "                         :    var55:     3.5967     4.2971   [    -8.7073     21.957 ]\n",
      "                         :    var56:     2.5031     3.9146   [    -11.497     20.361 ]\n",
      "                         :    var57:     3.9950     4.4431   [    -8.8077     25.394 ]\n",
      "                         :    var58:     5.6013     4.8508   [    -11.076     26.883 ]\n",
      "                         :    var59:     6.5296     5.0636   [    -9.0219     28.571 ]\n",
      "                         :    var60:     6.6129     5.0677   [    -9.9130     27.121 ]\n",
      "                         :    var61:     5.5612     4.9134   [    -9.6182     26.950 ]\n",
      "                         :    var62:     4.0187     4.5265   [    -11.569     26.781 ]\n",
      "                         :    var63:     2.4755     3.8720   [    -9.2495     22.465 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         : Ranking input variables (method unspecific)...\n",
      "IdTransformation         : Ranking result (top variable is best ranked)\n",
      "                         : ------------------------------\n",
      "                         : Rank : Variable  : Separation\n",
      "                         : ------------------------------\n",
      "                         :    1 : var31     : 1.987e-02\n",
      "                         :    2 : var60     : 1.452e-02\n",
      "                         :    3 : var39     : 1.448e-02\n",
      "                         :    4 : var38     : 1.352e-02\n",
      "                         :    5 : var11     : 1.318e-02\n",
      "                         :    6 : var4      : 1.276e-02\n",
      "                         :    7 : var24     : 1.248e-02\n",
      "                         :    8 : var23     : 1.231e-02\n",
      "                         :    9 : var59     : 1.229e-02\n",
      "                         :   10 : var30     : 1.203e-02\n",
      "                         :   11 : var51     : 1.037e-02\n",
      "                         :   12 : var3      : 9.927e-03\n",
      "                         :   13 : var32     : 9.479e-03\n",
      "                         :   14 : var47     : 8.533e-03\n",
      "                         :   15 : var40     : 8.417e-03\n",
      "                         :   16 : var58     : 8.149e-03\n",
      "                         :   17 : var25     : 8.039e-03\n",
      "                         :   18 : var2      : 8.016e-03\n",
      "                         :   19 : var12     : 7.996e-03\n",
      "                         :   20 : var46     : 7.747e-03\n",
      "                         :   21 : var33     : 7.741e-03\n",
      "                         :   22 : var16     : 7.662e-03\n",
      "                         :   23 : var5      : 7.407e-03\n",
      "                         :   24 : var41     : 6.987e-03\n",
      "                         :   25 : var55     : 6.755e-03\n",
      "                         :   26 : var43     : 6.707e-03\n",
      "                         :   27 : var26     : 6.565e-03\n",
      "                         :   28 : var13     : 6.508e-03\n",
      "                         :   29 : var52     : 6.318e-03\n",
      "                         :   30 : var15     : 6.001e-03\n",
      "                         :   31 : var61     : 5.907e-03\n",
      "                         :   32 : var22     : 5.683e-03\n",
      "                         :   33 : var29     : 5.676e-03\n",
      "                         :   34 : var54     : 5.618e-03\n",
      "                         :   35 : var20     : 5.605e-03\n",
      "                         :   36 : var37     : 5.505e-03\n",
      "                         :   37 : var19     : 5.274e-03\n",
      "                         :   38 : var17     : 4.922e-03\n",
      "                         :   39 : var49     : 4.797e-03\n",
      "                         :   40 : var8      : 4.496e-03\n",
      "                         :   41 : var7      : 4.491e-03\n",
      "                         :   42 : var34     : 4.484e-03\n",
      "                         :   43 : var48     : 4.478e-03\n",
      "                         :   44 : var9      : 4.435e-03\n",
      "                         :   45 : var50     : 4.398e-03\n",
      "                         :   46 : var18     : 4.369e-03\n",
      "                         :   47 : var53     : 4.348e-03\n",
      "                         :   48 : var27     : 4.313e-03\n",
      "                         :   49 : var57     : 4.163e-03\n",
      "                         :   50 : var10     : 4.135e-03\n",
      "                         :   51 : var45     : 4.055e-03\n",
      "                         :   52 : var35     : 3.998e-03\n",
      "                         :   53 : var1      : 3.670e-03\n",
      "                         :   54 : var44     : 3.477e-03\n",
      "                         :   55 : var14     : 3.376e-03\n",
      "                         :   56 : var56     : 3.157e-03\n",
      "                         :   57 : var6      : 3.078e-03\n",
      "                         :   58 : var62     : 2.999e-03\n",
      "                         :   59 : var21     : 2.938e-03\n",
      "                         :   60 : var36     : 2.937e-03\n",
      "                         :   61 : var42     : 2.906e-03\n",
      "                         :   62 : var28     : 2.700e-03\n",
      "                         :   63 : var63     : 2.451e-03\n",
      "                         :   64 : var0      : 2.151e-03\n",
      "                         : ------------------------------\n",
      "Factory                  : Train method: BDT for Classification\n",
      "                         : \n",
      "BDT                      : #events: (reweighted) sig: 5000 bkg: 5000\n",
      "                         : #events: (unweighted) sig: 5000 bkg: 5000\n",
      "                         : Training 800 Decision Trees ... patience please\n",
      "                         : Elapsed time for training with 10000 events: 68.6 sec         \n",
      "BDT                      : [dataset] : Evaluation of BDT on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.836 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: DL_DENSE for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on the STANDARD architecture\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 5  Input = ( 1, 1, 64 )  Batch size = 128  Loss function = C\n",
      "\tLayer 0\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 1\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 2\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 3\t DENSE Layer: \t  ( Input = 64 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 4\t DENSE Layer: \t  ( Input = 64 , Width = 1 ) \tOutput = ( 1 , 128 , 1 ) \t Activation Function = Identity\n",
      "                         : Training phase 1 of 1:    Learning rate = 0.001 regularization 2 minimum error = 0.726024\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Test Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |     0.618855    0.639687     1.44691    0.651556     12552.9           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.582685    0.621519     1.37239    0.656573     13947.6           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.562612    0.613685      1.3777     0.65972     13905.8           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.550221    0.609386     1.36865    0.651774     13927.1           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |       0.5425    0.608845     1.36261    0.647308     13957.7           0\n",
      "                         :          6 |     0.537699    0.609303     1.55031    0.640129     10969.2           1\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.532727    0.608454     1.34756    0.641308     14136.5           0\n",
      "                         :          8 |     0.528184    0.609607     1.34682    0.639456     14114.3           1\n",
      "                         :          9 |     0.523149    0.610755     1.34147    0.636044     14153.1           2\n",
      "                         :         10 |     0.522242    0.612716     1.34173    0.637378     14174.7           3\n",
      "                         :         11 |     0.518368    0.611324     1.34059    0.635307     14155.9           4\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |      0.51335    0.608115     1.34108    0.638672       14214           0\n",
      "                         :         13 |     0.511103    0.609003     1.34451    0.639108     14153.6           1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         :         14 |     0.510986    0.612368     1.33941    0.634266     14158.8           2\n",
      "                         :         15 |     0.505149    0.611125     1.34217     0.63972       14213           3\n",
      "                         :         16 |     0.502665    0.612384     1.34502    0.635261     14066.7           4\n",
      "                         :         17 |     0.500445    0.612558     1.33804    0.634882     14198.8           5\n",
      "                         :         18 |     0.497931    0.614111     1.33854    0.635625     14203.8           6\n",
      "                         :         19 |      0.49522    0.615309     1.33899    0.634642     14174.9           7\n",
      "                         :         20 |     0.493066     0.61573     1.34453    0.638331     14137.7           8\n",
      "                         : \n",
      "                         : Elapsed time for training with 10000 events: 27.8 sec         \n",
      "                         : Evaluate deep neural network on the STANDARD architecture  using batches with size = 128\n",
      "                         : \n",
      "DL_DENSE                 : [dataset] : Evaluation of DL_DENSE on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.32 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_DENSE.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_DENSE.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "Factory                  : Train method: DL_CNN for Classification\n",
      "                         : \n",
      "                         : Start of deep neural network training on CPU.\n",
      "                         : \n",
      "                         : *****   Deep Learning Network *****\n",
      "DEEP NEURAL NETWORK:   Depth = 6  Input = ( 1, 8, 8 )  Batch size = 128  Loss function = C\n",
      "\tLayer 0\t CONV LAYER: \t( W = 8 ,  H = 8 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 128 , 10 , 64 ) \t Activation Function = Relu\n",
      "\tLayer 1\t CONV LAYER: \t( W = 8 ,  H = 8 ,  D = 10 ) \t Filter ( W = 3 ,  H = 3 ) \tOutput = ( 128 , 10 , 64 ) \t Activation Function = Relu\n",
      "\tLayer 2\t POOL Layer: \t( W = 7 ,  H = 7 ,  D = 10 ) \t Filter ( W = 2 ,  H = 2 ) \tOutput = ( 128 , 10 , 49 ) \n",
      "\tLayer 3\t RESHAPE Layer \t Input = ( 10 , 7 , 7 ) \tOutput = ( 1 , 128 , 490 ) \n",
      "\tLayer 4\t DENSE Layer: \t  ( Input = 490 , Width = 64 ) \tOutput = ( 1 , 128 , 64 ) \t Activation Function = Tanh\n",
      "\tLayer 5\t DENSE Layer: \t  ( Input = 64 , Width = 1 ) \tOutput = ( 1 , 128 , 1 ) \t Activation Function = Identity\n",
      "                         : Training phase 1 of 1:    Learning rate = 0.001 regularization 0 minimum error = 0.814564\n",
      "                         : --------------------------------------------------------------\n",
      "                         :      Epoch |   Train Err.   Test Err.  t(s)/epoch   t(s)/Loss   nEvents/s Conv. Steps\n",
      "                         : --------------------------------------------------------------\n",
      "                         :          1 Minimum Test error found - save the configuration \n",
      "                         :          1 |      0.68119    0.681631     55.8038      25.002     324.136           0\n",
      "                         :          2 Minimum Test error found - save the configuration \n",
      "                         :          2 |     0.664666    0.664962     52.9936     24.1952     346.686           0\n",
      "                         :          3 Minimum Test error found - save the configuration \n",
      "                         :          3 |     0.649182    0.651265     56.3981     24.7996     315.964           0\n",
      "                         :          4 Minimum Test error found - save the configuration \n",
      "                         :          4 |     0.634324    0.638538     54.4011     23.3946     321.996           0\n",
      "                         :          5 Minimum Test error found - save the configuration \n",
      "                         :          5 |     0.620896    0.628611     54.2006     24.6991     338.424           0\n",
      "                         :          6 Minimum Test error found - save the configuration \n",
      "                         :          6 |     0.614108    0.623261     54.6977     24.1006     326.305           0\n",
      "                         :          7 Minimum Test error found - save the configuration \n",
      "                         :          7 |     0.601781    0.614988     54.7024     23.9991     325.177           0\n",
      "                         :          8 Minimum Test error found - save the configuration \n",
      "                         :          8 |     0.594726     0.60883     54.1017     23.8006     329.493           0\n",
      "                         :          9 Minimum Test error found - save the configuration \n",
      "                         :          9 |     0.593514    0.608061     55.5004     25.4006     331.697           0\n",
      "                         :         10 Minimum Test error found - save the configuration \n",
      "                         :         10 |     0.582938    0.601512      56.395     25.5005     323.165           0\n",
      "                         :         11 Minimum Test error found - save the configuration \n",
      "                         :         11 |     0.579681    0.599484     54.0045     22.8012     319.967           0\n",
      "                         :         12 Minimum Test error found - save the configuration \n",
      "                         :         12 |      0.57337      0.5947     54.5957     23.8956      325.21           0\n",
      "                         :         13 Minimum Test error found - save the configuration \n",
      "                         :         13 |     0.569448    0.592706      55.103     24.4979      326.22           0\n",
      "                         :         14 Minimum Test error found - save the configuration \n",
      "                         :         14 |     0.566213    0.591702     55.0026     24.0068     322.108           0\n",
      "                         :         15 Minimum Test error found - save the configuration \n",
      "                         :         15 |      0.56477     0.58782     54.5001     23.9039     326.315           0\n",
      "                         :         16 |      0.56192    0.588262     54.7937     24.1954     326.293           1\n",
      "                         :         17 Minimum Test error found - save the configuration \n",
      "                         :         17 |     0.556425    0.585009      55.303     23.1026     310.058           0\n",
      "                         :         18 Minimum Test error found - save the configuration \n",
      "                         :         18 |     0.550299    0.583754     56.3989     24.6976      314.94           0\n",
      "                         :         19 Minimum Test error found - save the configuration \n",
      "                         :         19 |      0.54701    0.581614     53.1966     24.3967     346.667           0\n",
      "                         :         20 |     0.548954    0.584529      55.702     24.7991     323.076           1\n",
      "                         : \n",
      "                         : Elapsed time for training with 10000 events: 1.12e+03 sec         \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 128\n",
      "                         : \n",
      "DL_CNN                   : [dataset] : Evaluation of DL_CNN on training sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 11.6 sec       \n",
      "                         : Creating xml weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN.weights.xml\u001b[0m\n",
      "                         : Creating standalone class: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN.class.C\u001b[0m\n",
      "Factory                  : Training finished\n",
      "                         : \n",
      "                         : Ranking input variables (method specific)...\n",
      "BDT                      : Ranking result (top variable is best ranked)\n",
      "                         : ---------------------------------------\n",
      "                         : Rank : Variable  : Variable Importance\n",
      "                         : ---------------------------------------\n",
      "                         :    1 : var4      : 2.133e-02\n",
      "                         :    2 : var31     : 2.026e-02\n",
      "                         :    3 : var32     : 2.000e-02\n",
      "                         :    4 : var51     : 1.973e-02\n",
      "                         :    5 : var39     : 1.958e-02\n",
      "                         :    6 : var11     : 1.937e-02\n",
      "                         :    7 : var60     : 1.902e-02\n",
      "                         :    8 : var24     : 1.844e-02\n",
      "                         :    9 : var12     : 1.828e-02\n",
      "                         :   10 : var29     : 1.808e-02\n",
      "                         :   11 : var17     : 1.800e-02\n",
      "                         :   12 : var3      : 1.798e-02\n",
      "                         :   13 : var13     : 1.766e-02\n",
      "                         :   14 : var59     : 1.760e-02\n",
      "                         :   15 : var58     : 1.755e-02\n",
      "                         :   16 : var2      : 1.739e-02\n",
      "                         :   17 : var23     : 1.720e-02\n",
      "                         :   18 : var40     : 1.710e-02\n",
      "                         :   19 : var53     : 1.702e-02\n",
      "                         :   20 : var30     : 1.697e-02\n",
      "                         :   21 : var49     : 1.677e-02\n",
      "                         :   22 : var15     : 1.654e-02\n",
      "                         :   23 : var25     : 1.654e-02\n",
      "                         :   24 : var41     : 1.636e-02\n",
      "                         :   25 : var37     : 1.635e-02\n",
      "                         :   26 : var16     : 1.602e-02\n",
      "                         :   27 : var20     : 1.593e-02\n",
      "                         :   28 : var26     : 1.590e-02\n",
      "                         :   29 : var22     : 1.575e-02\n",
      "                         :   30 : var47     : 1.561e-02\n",
      "                         :   31 : var54     : 1.550e-02\n",
      "                         :   32 : var46     : 1.550e-02\n",
      "                         :   33 : var38     : 1.530e-02\n",
      "                         :   34 : var43     : 1.515e-02\n",
      "                         :   35 : var8      : 1.509e-02\n",
      "                         :   36 : var33     : 1.504e-02\n",
      "                         :   37 : var19     : 1.503e-02\n",
      "                         :   38 : var5      : 1.499e-02\n",
      "                         :   39 : var45     : 1.488e-02\n",
      "                         :   40 : var21     : 1.472e-02\n",
      "                         :   41 : var10     : 1.466e-02\n",
      "                         :   42 : var56     : 1.449e-02\n",
      "                         :   43 : var61     : 1.443e-02\n",
      "                         :   44 : var48     : 1.439e-02\n",
      "                         :   45 : var62     : 1.397e-02\n",
      "                         :   46 : var50     : 1.394e-02\n",
      "                         :   47 : var27     : 1.385e-02\n",
      "                         :   48 : var52     : 1.385e-02\n",
      "                         :   49 : var35     : 1.374e-02\n",
      "                         :   50 : var42     : 1.368e-02\n",
      "                         :   51 : var9      : 1.363e-02\n",
      "                         :   52 : var7      : 1.348e-02\n",
      "                         :   53 : var6      : 1.339e-02\n",
      "                         :   54 : var57     : 1.325e-02\n",
      "                         :   55 : var55     : 1.315e-02\n",
      "                         :   56 : var36     : 1.313e-02\n",
      "                         :   57 : var28     : 1.302e-02\n",
      "                         :   58 : var63     : 1.276e-02\n",
      "                         :   59 : var18     : 1.248e-02\n",
      "                         :   60 : var14     : 1.237e-02\n",
      "                         :   61 : var44     : 1.205e-02\n",
      "                         :   62 : var1      : 1.201e-02\n",
      "                         :   63 : var0      : 1.178e-02\n",
      "                         :   64 : var34     : 1.095e-02\n",
      "                         : ---------------------------------------\n",
      "                         : No variable ranking supplied by classifier: DL_DENSE\n",
      "                         : No variable ranking supplied by classifier: DL_CNN\n",
      "Factory                  : === Destroy and recreate all methods via weight files for testing ===\n",
      "                         : \n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_BDT.weights.xml\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_DENSE.weights.xml\u001b[0m\n",
      "                         : Reading weight file: \u001b[0;36mdataset/weights/TMVA_CNN_Classification_DL_CNN.weights.xml\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.TrainAllMethods();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Evaluate Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mTest all methods\u001b[0m\n",
      "Factory                  : Test method: BDT for Classification performance\n",
      "                         : \n",
      "BDT                      : [dataset] : Evaluation of BDT on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.513 sec       \n",
      "Factory                  : Test method: DL_DENSE for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on the STANDARD architecture  using batches with size = 1000\n",
      "                         : \n",
      "DL_DENSE                 : [dataset] : Evaluation of DL_DENSE on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 0.332 sec       \n",
      "Factory                  : Test method: DL_CNN for Classification performance\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "DL_CNN                   : [dataset] : Evaluation of DL_CNN on testing sample (10000 events)\n",
      "                         : Elapsed time for evaluation of 10000 events: 2.68 sec       \n"
     ]
    }
   ],
   "source": [
    "factory.TestAllMethods();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : \u001b[1mEvaluate all methods\u001b[0m\n",
      "Factory                  : Evaluate classifier: BDT\n",
      "                         : \n",
      "BDT                      : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "TFHandler_BDT            : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "Factory                  : Evaluate classifier: DL_DENSE\n",
      "                         : \n",
      "DL_DENSE                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on the STANDARD architecture  using batches with size = 1000\n",
      "                         : \n",
      "TFHandler_DL_DENSE       : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factory                  : Evaluate classifier: DL_CNN\n",
      "                         : \n",
      "DL_CNN                   : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "                         : Evaluate deep neural network on CPU using batches with size = 1000\n",
      "                         : \n",
      "TFHandler_DL_CNN         : Variable        Mean        RMS   [        Min        Max ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         :     var0:     2.4375     3.8557   [    -9.9918     19.977 ]\n",
      "                         :     var1:     3.9666     4.4410   [    -9.7103     25.867 ]\n",
      "                         :     var2:     5.4753     4.8697   [    -8.5006     29.836 ]\n",
      "                         :     var3:     6.4672     5.0291   [    -7.9837     27.817 ]\n",
      "                         :     var4:     6.5185     5.0298   [    -10.282     27.355 ]\n",
      "                         :     var5:     5.4855     4.8555   [    -7.9858     27.156 ]\n",
      "                         :     var6:     3.9606     4.4945   [    -9.8938     28.512 ]\n",
      "                         :     var7:     2.4387     3.9116   [    -9.9916     21.240 ]\n",
      "                         :     var8:     3.6854     4.3626   [    -9.7199     27.343 ]\n",
      "                         :     var9:     6.0793     5.1200   [    -8.6642     29.457 ]\n",
      "                         :    var10:     8.3184     5.5430   [    -8.5650     31.127 ]\n",
      "                         :    var11:     9.6351     5.5072   [    -7.6700     32.336 ]\n",
      "                         :    var12:     9.6312     5.5191   [    -8.7419     29.683 ]\n",
      "                         :    var13:     8.3128     5.5239   [    -7.2193     36.270 ]\n",
      "                         :    var14:     6.0416     5.1057   [    -9.4517     30.755 ]\n",
      "                         :    var15:     3.6383     4.4067   [    -12.546     22.148 ]\n",
      "                         :    var16:     4.8401     4.7558   [    -11.279     26.243 ]\n",
      "                         :    var17:     7.9437     5.5305   [    -9.1076     32.552 ]\n",
      "                         :    var18:     10.885     5.7504   [    -11.120     35.900 ]\n",
      "                         :    var19:     12.854     5.5008   [    -5.4607     34.438 ]\n",
      "                         :    var20:     12.877     5.5496   [    -4.9795     34.453 ]\n",
      "                         :    var21:     10.946     5.7644   [    -6.3943     38.104 ]\n",
      "                         :    var22:     7.8424     5.4984   [    -7.1890     35.488 ]\n",
      "                         :    var23:     4.8228     4.7835   [    -9.2727     25.210 ]\n",
      "                         :    var24:     5.6459     4.9148   [    -8.6297     25.267 ]\n",
      "                         :    var25:     9.0667     5.6465   [    -7.8932     30.653 ]\n",
      "                         :    var26:     12.553     5.7546   [    -7.4565     38.462 ]\n",
      "                         :    var27:     14.670     5.1246   [    -1.2005     34.769 ]\n",
      "                         :    var28:     14.686     5.1706   [    -1.8391     35.169 ]\n",
      "                         :    var29:     12.466     5.7116   [    -6.3975     36.999 ]\n",
      "                         :    var30:     9.0021     5.7010   [    -8.3901     32.022 ]\n",
      "                         :    var31:     5.6036     5.0045   [    -9.6496     26.932 ]\n",
      "                         :    var32:     5.5822     4.9826   [    -11.560     26.953 ]\n",
      "                         :    var33:     9.1635     5.6928   [    -7.8908     31.500 ]\n",
      "                         :    var34:     12.670     5.6927   [    -8.4293     39.896 ]\n",
      "                         :    var35:     14.712     5.1167   [    -2.6615     34.365 ]\n",
      "                         :    var36:     14.777     5.1516   [    -2.1642     33.879 ]\n",
      "                         :    var37:     12.528     5.6390   [    -7.0200     33.836 ]\n",
      "                         :    var38:     9.0389     5.6715   [    -9.4451     32.219 ]\n",
      "                         :    var39:     5.5557     4.9419   [    -9.8782     27.489 ]\n",
      "                         :    var40:     4.9449     4.8088   [    -10.422     28.788 ]\n",
      "                         :    var41:     7.9424     5.5420   [    -8.4661     31.055 ]\n",
      "                         :    var42:     11.034     5.7483   [    -6.7219     34.394 ]\n",
      "                         :    var43:     12.846     5.4598   [    -6.0367     35.064 ]\n",
      "                         :    var44:     12.885     5.5471   [    -5.3399     36.017 ]\n",
      "                         :    var45:     11.017     5.7452   [    -8.1370     34.427 ]\n",
      "                         :    var46:     7.9332     5.5564   [    -9.6217     31.459 ]\n",
      "                         :    var47:     4.9462     4.8152   [    -9.0932     24.864 ]\n",
      "                         :    var48:     3.7166     4.3715   [    -10.844     25.228 ]\n",
      "                         :    var49:     6.1139     5.1265   [    -8.5402     29.845 ]\n",
      "                         :    var50:     8.3390     5.4683   [    -8.2915     32.722 ]\n",
      "                         :    var51:     9.8672     5.5727   [    -8.3201     33.142 ]\n",
      "                         :    var52:     9.8495     5.5322   [    -8.3466     33.869 ]\n",
      "                         :    var53:     8.4185     5.4828   [    -9.7019     33.503 ]\n",
      "                         :    var54:     6.0872     5.1308   [    -9.1295     32.443 ]\n",
      "                         :    var55:     3.7002     4.3962   [    -10.585     27.256 ]\n",
      "                         :    var56:     2.4642     3.9476   [    -9.7419     22.620 ]\n",
      "                         :    var57:     4.0422     4.5454   [    -8.2481     26.215 ]\n",
      "                         :    var58:     5.5688     4.8957   [    -7.9015     30.791 ]\n",
      "                         :    var59:     6.5518     5.0494   [    -8.9191     29.350 ]\n",
      "                         :    var60:     6.5388     5.0481   [    -8.4943     28.476 ]\n",
      "                         :    var61:     5.6626     4.9085   [    -9.0860     29.674 ]\n",
      "                         :    var62:     4.0584     4.5142   [    -9.4862     23.124 ]\n",
      "                         :    var63:     2.5276     3.9035   [    -10.527     21.548 ]\n",
      "                         : -----------------------------------------------------------\n",
      "                         : \u001b[32m\n",
      "                         : <PlotVariables> Will not produce scatter plots ==> \n",
      "                         : |  The number of 64 input variables and 0 target values would require 2016 two-dimensional\n",
      "                         : |  histograms, which would occupy the computer's memory. Note that this\n",
      "                         : |  suppression does not have any consequences for your analysis, other\n",
      "                         : |  than not disposing of these scatter plots. You can modify the maximum\n",
      "                         : |  number of input variables allowed to generate scatter plots in your\n",
      "                         : |  script via the command line:\n",
      "                         : |  \"(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;\"\u001b[0m\n",
      "                         : \n",
      "                         : Some more output\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       BDT            : 0.787\n",
      "                         : dataset       DL_DENSE       : 0.771\n",
      "                         : dataset       DL_CNN         : 0.766\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              BDT            : 0.115 (0.195)       0.443 (0.536)      0.728 (0.781)\n",
      "                         : dataset              DL_DENSE       : 0.108 (0.163)       0.417 (0.504)      0.702 (0.762)\n",
      "                         : dataset              DL_CNN         : 0.069 (0.098)       0.392 (0.441)      0.690 (0.726)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:dataset          : Created tree 'TestTree' with 10000 events\n",
      "                         : \n",
      "Dataset:dataset          : Created tree 'TrainTree' with 10000 events\n",
      "                         : \n",
      "Factory                  : \u001b[1mThank you for using TMVA!\u001b[0m\n",
      "                         : \u001b[1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "factory.EvaluateAllMethods();    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "//%jsroot on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAHYCAIAAAApvgy/AAAABmJLR0QAAAAAAAD5Q7t/AAAgAElE\nQVR4nO2daZKturFGxQvPC/DADHheNjAx836oSqWtbpO0QqwVJ+6l2CDER6MklUpVy7IoAAAAgBD/\nd3cFAAAAIF8wFAAAACAKhgIAAABEwVAAAACAKBgKAAAAEAVD4QH0fd80TWXR972/WVVVTdNcXTmL\nvu+rqpqm6ajS9FmbNdM06TX69DccrmmaeyXKHC2pQ9M0fd8fdVl9nKtcNmc/pLGXw+Ec+7BD7iyQ\nN4lrN46jv+VN1VyWZanr2q/VNrqu829Rs6brOnM4vbyS2yXKHC1pjLquzzvoIbdN/px6B47jeNJl\nGsfRuUCvumqARyFrzMdH8Clt29beuK5ru319NPpLRZ+1vUa/BPU3U9M0dV2Lvs/quk63haA822sc\nR31fzfN8zdcqbGPDE7GSaZratrWvvj7W4QeCPPnH3RWAFPM8K6UWz68wTZP21vZ9b57ektyA+sTt\nV54+O3vNhkarJIkuw/TXDMMwDAO2Qs5cdodzG7wKDIV8ST/zXdcNw7DmvTBNk+7d//qp8XVLfbiV\npa2pmPps+4/CyLK+8JUqrdlsw9EPwb4ZDj903/fDMMSOqzVJH/cQWUQ3s3+sYy/NmtLWVzhWvr/j\nhgdn5Ykf8kiuF/muJwXEXNTFAXJ0j+P6a+RvbEow+GUqpeq6Dm6ZLkp5PuqV3ZZ+UXavqt97sngO\nFX1cvWWwDrE7PKinv0ss+CN9+sHKm6JiERXBs3AO7Uuq9zK6Ba9OsMA0ibCP4N0Y7Orya+tXzz6E\nf9uY7YMr7QPZVdJ/dl1n1ttH8au68u51ztr86dw2fmRA8Gb4emlMUf51TD84SyRG4ete9ukHtwnq\nFpRrpchr1IOswFDIGvsVs3Jj86d5QejYBefJtPfSP+nNxnE0W/qF623s0tJvfB+nVn7QoukRN9ss\ny2KOaCq5hJpY+/WUCIcMKuycl30WzovPlsjezNbELso58eBVi4lmzjqx11dJ15MwFPyf/OMG7xzn\navp3jnPbpK2ExM1smkmzpWOlJergV8MQvIWMGvbt6peWrnAQu3y7zNhVtu8N31CIie/cUaZiwfsn\n+PT5cq0XOagetkLOYChkjWOhm4YzSPCNFvsmczYLvjicl5SzjfNRu6wzFPx3R7Cq/vvUNwucNX59\nFq95c4oN7uKvDL7fnSY88cFttvE3+PqJGdzAWekrEzzWGuz2wBBrXYJWhb8ycR/auzhGT9CpEzRT\nlGeKxT6pE3VYhIZC+kJ/9Ygsccw2QQXSK/3nNLFXrPLBHf17LGbe2YXERP56LSA3uDa5Y3+/2vgW\ng/2wBR/+xXpQg3sZnPeCNvy/vkO/GgpBg2MJvSli7+KEoRB8JzpHXHPusc3SJce+xe2i/G0SX/CJ\nozsnHixkHMc1XiiH4J0Wu+W0GeGUsMZ6c07K3DYxKyHWkKz02cREdtaLDAVfbf9Cr3n6fIIKxGR0\nbkLnz6AF6ZcWrJKzzVdDQSRy7H5YIFe4No9Bt9YJH2a6TTKFBF98zmaxV4xdjv/W+2ooJIr9WquV\nhkLs0P42ie8Y50SCm6VNkCCxD770Xv6JO3sZt1Pa4FiDeY+Pn6z0DxtJ00ZM8KC288zZIHbbOFcw\ndkFjIgdNvZWGQnqzWFFrPp2DGyRk9BX4ekaLFQkRs90dvhoKsfNyDI419jRkCHkUHoNJkLdYbUM6\npd2xscQmMWJVVW3b6hGM0hKUUsMw+On/9tdNWT27IvzK6FM7fKSZvhxGt5V1NkMTY3v1fa//NMI2\nTbOn8o1H3/fLsqhQKgU7baiT2MM5hTT2kArnELFzCRa77R6Qkj4jf3Dvmr2+In1wdDXatg3uteH5\n3UDwlBnj8DgYHpkviaFK+u0cey+fURP7WHVd6/ZjWwWueZWvJ1afM15ndV3rttYkwPg6Ht1UQxtq\nwb30GLy+7/Xbf57ntm3ruj7W1uk+R+TaAybNLTFNk93ki1qjZVn0nXZltoZr2sud6Epue3DSe5FZ\nBNaAoZAvuhkexzFtlZsh7D6JnzbUZH/D0zTNPM+mtTsQ52N9PYe3pmn6vm/bVh8x9t3pY1poLWBw\nL9uLoJtwY5EcV/0PtEHQdZ19CEdMbRitKU37wJum0bvYp2PO2uGQC3eG2apPwX/6NlfY2Jeix3nN\nXk3TxNJjHAKGSBnQ9ZAv+hW2/ovTX+k/pXuaDae0za+A4I5nWA/qN4VlQqJgC7QtPY4KnZrjHzYH\n1Se7spXSG5uveWcvvyXo+163u8e+pu3STP3TV01XzN9G91YEq2esqK/328qziz1HJhF4Yt89Am6u\nsE9MRtvJJKqG2Sv2FCQeHOmxFH0NBXBzjATE8UPDbPz4YeeC6j+DI6P8zZzC18QJbghmjNXKD3Hy\nj/g1mDEYUO2UHKxwOoI9WJklEmEerHOwSkEdEiT2Cq70Dx0cpBA8SnAzc4KjNY4xFr6+eXikc6z0\nOTo38/oxNcGVwZBJ/yYP3gxLJE7269OXLsdfn35wgoMg/NKcooIX3Vn5NZgxeP/7FVgZMQq5wbXJ\nGrt5qL20Ns6jFXzxKS+xyZoXX9BQsIPh7aKcN0668XNqFRw9EazVV0PBrmcXyRwVO5C/i30WQYli\nQxh8wcfIYD/Rm/HroICgpObQsabL4auHw1fbHNSuoWOgBGVxLLxgm+o0VH4htiCJAP7YpbFPx7bL\nHRmV0FCwlUw/fV/L+SqjvbGvQKwa9jbmxM0D7j84xu5MPOxrRMZQeChcm9xxXjEG+6HV+A+bs2/9\nm6o54Yewd/RbBbsoe+UYSdYWxLEz7BK+nkvaUPhasl/stsoskWbJb2iDauifvn7fr98r2MAHP+I3\nGwr+/eZL11npk1XIZrVLcw4aK9z/ZrVP0D5QwlAInpqvZPDy6ZVmG+fUEuuDpcV2/1p+UAHnZIMK\nfN1rWfEU2Bt08RTOX0XGUHgo1eK1AZAhOqzd/CnqzjedkTpAbHM8vKmD3YW/MnR/ZWmHsKHkAyuz\npigdtXD4o2euwuGSfj2ofUSjQHBohr9ehC5k28285tLYD9r+oJljS3PK9M8iIYjo3IPbrLx85z3X\ncCd3WypwCvpLKPbNJP2WhQNZ81kJDsGbORhV8GbSPhWAzeBRKBb92Tpaoyun33QIXPQb0del+xxV\nCGm4mdegh8Vu8xcCJGB4ZLHo7y2dl02PRtMv1ljQA5yNPVQSK0FE7Gb+Gn35HkxGBBz+cDw3ezTg\nTJzIbT8eDa7EXIW7K/JIuJkT2EM27q4LFAhdDwAAABCFrgcAAACIgqEAAAAAUTAUAAAAIAqGAgAA\nAETBUAAAAIAoGAoAAAAQBUMBAAAAomAoAAAAQBQMBQAAAIiCoQAAAABRMBQAAAAgCoYCAAAARMFQ\nAAAAgCgYCgAAABAFQwEAAACiYCgAAABAFAwFAAAAiIKhAAAAAFH+sb+IaZqmaVJKNU3TNM3+AvdQ\nVdW9FQAAgJezLMvdVTiSavP59H0/DEPwp67r+r7fXqkdVNX2MwIAANhJec3Qlq6HaZqqqpqmaRzH\nxWMcR73BXbYCAAAAHMVGQ0FbA8GOhqZppmkqwJ6iF0MKiolALhHIJQK5pKBYgtI8JOX5fAAA4EGU\n1wztGvVQVRVdDAAAAAWzy1BYlqXrumEYqqrSPQ4H1SoL8ERJQTERyCUCuUQglxQUS7A3j0Lf9zqA\nUSnVtm1JFkNhvqMLQDERyCUCuUQglxQUS3BYV8o0TX3fz/Ns1ozjeH1ahfI6hwAA4EGU1wzt9Sho\n+6CqqrZt53nuuk4Pkqzrum3bQ6p4F3iipKCYCOQSgVwikEsKiiXYZfgYZYMZlqqqut6pUJ4pBwAA\nD6K8ZmhXCue0HVCYUgAAAC9kV9dDcGBkMcGMeKKkoJgI5BKBXCKQSwqKJdjoUdAmwjzPzkRQ0zTZ\n8YwX0Pf9SYkc8IhIQTERyCUCuUQglxQUS7DRULB9Bo7/oOu6y+ISpmkahiGHWSsBAACKZJehcGMv\ngz8a83DKC0g5GxQTgVwikEsEcklBsQS7YhTujUVomqbruvPK56aRgmIikEsEcolALikolmCLDVVV\nVV3XevbI4Df9lYo7gzAPtAortTa2ZQlt6KzjLgQAeAPlOSe2dD2YKIS+7zMc4LAteFVfV3OBq6pS\nqy90Fdnyw4BI1sr/za9P/gu62rdXg4UiF8wNlkl9Ml/gYbxXscJ4/FlVp3kUVh19tdfBIeiE+Has\nzxIefuEAAIqkPHNhbwrnqqqq32/lpmmqqsrQx3Aei1pi/9I7Vov7b8WxPv6pqrL/2RxyagAAAGpn\nZkb9Ha+njlRK6aiFtm3LMKZ2WoUxWyHmhFhjK6i4K+Jj719b4WInRHl29KkglwjkEoFcUlAswS6P\nwjzPThZn7U4ow6lw0k0j9T04uH6IZCUdJ0QVZ985/R6Ox0wCcolALhHIJQXFEuzyKMAhrLQVgq6I\nSn0EXf4VFWr7Y4epfmN53O15cgAAXs8uQ0HPJW07FfTClXkSz2vMcvNEOfZE1G7429raPukziJ5k\nyHxIaJKbYpmDXCKQSwRySUGxBHul8VMpXD+1tM1rL7Zo/EXa8bD6iFaBr9QcAMCnvGbogPOZpslk\ndL59zoXyrtAGtg3adHtANtkQIg8EAEB5lNcMFXc+x12h8i62ktsQ4fgJoQ2hty5PzJ0UeYOdB3KJ\nQC4ptB0Jtox6qKpKew5ODaG/ncKutEaa8uEjV8Pvv2jyiOhB/xI/lHq3bKDIG+w8kEsEcklBsQRb\nghlNFILJoACPJmgrJHwP9k8f+/pPmmcH+EdiwAUAQM5sMRR0YiX1O9fzsRXKh/LcRyJWWg/+mqjp\nEHEeBEwHz3oo8kK8/AaTglwikEsKiiXYIk1VVXpeKD020t+AUQ9vQDC7Zqx3Y0W/Q2wLrjIA5El5\nzdCW8+n7fhiGxAY3alTeFXoQX02H76mlNpkOXHEAyIfymqFd5+PM3JgDRK7eiK9YwnTYbzQ83dnA\nDSYCuUQglxTajgTFnU9xV6gMVvZTfLEeIqbDmqK5KwDgGsprhrYEM/Z9r3MrxSIZC45whG3YFsCa\n8RRhi8E8e06oY7iozz9/dynsAQYAOJuNox6UUk3T5DlLZGxovrSFKM8qPJuViq0ZT7HNYvg8il3a\nRyWtYu68vtxgIpBLBHJJQbEEpUnDxX466U6KVTNtbg1uUHebDgBQAOU1Q1syM9robgi9nMNcD/B0\n0vkiA2kiA0V8SRa5RP6p32SjR50LAEAB7DIUmqYZhsE2FOZ5LuY9W8yJXMaxiq3MMx1OMv1Xyqok\n079H/DAXzr4BuMFEIJcI5JKCYgmOHx5575jJ8nw+EGPjzNrhslLjKbijAGA95TVDW4IZbehrgLsQ\nTVGxKtV0bGaKqlIYDQDwVg6IUbD/NAMidhabA3iipOSg2Pq5MQO9FclOiuAcmHuqmoNcDwK5RCCX\nFBRLsMtDMk1T27ZKqbqu9WjJeZ67rrsxj0J5Ph84HHGq6fgb5OnZIQHgcMprhg44n77vTUIFexDE\nLZR3heACBKmm5UbDTznclgDvoLxmqLjzIV/3fRSgmMzZsMloMBIVINeVIJcI5JJC25HggBgF3Vnb\n9/3t7oRjKexKX0ABin0NbvgIa/gW0LCENLlm7GV5FHB3XQlySUGxBLtGPej5pruu010PxmhAcSgA\nx1bwnQ16zaI8W8E2Aqpq+dvexTEXeHAAIEN2eRSGYRjH0XYk6DddnnNASOGbT0rZisWcDdEsT4ES\nom6Gn6JwNsRBFhHIJQXFEpBHIQqfd1Leo5i2FfyJrAK5GX5++3wHGTeDFa/w+XsuM1flAzqIQC4p\nKJaAPAoAG/EdDN8zSTv8brv84h/FytrAFw8A3MABeRTqujZr5nmu6/rGrgciV2/kzYqJJ70Mtvqf\n6iUsgxfq/Oa7awPIJYW2I8HeSaHGcVRKzfM8z7NSahzHMgIU1CvfxTt5s2LpdJBrfQzGGfGzyRLz\nNLzQwfDmu2sDyCUFxRKUZviUZ8rBo1mbyinR6q9wM3DPA+RDec3Q3hiFaZrsPtQbkzcbqggbyjmj\negWDYj6JrAwfPobERNjeEMpEboaCL0HBp3YGyCUFxRLsMnx0HgU90YOe60H/WUaMAsBJBN0M2+IY\nVPwFx4MAcAvlNUO7zqeqKmcKKB3eeKNG5V0hKJVVvRKruyQUFgNAHpTXDO01FIJe0HEc7xohSeTq\njaCYCCPXrjiGyIjK0IbPvjTcXSKQSwptR4KD8yhoysijUNiVvgAUE2HkWjtcIj1Q4jNHkx/K8PQI\nBu4uEcglBcUS7MrMOI5j27bTNGlzwY9RKMNiALgMYyv4aR//ftVvNL/Vr6q/X3Vp2mNhbVlZYy+P\nrDcAlMverof0Btf3QeA+uhEUE7FSrlWRj1vTNz3oenF3iUAuKbQdCYo7n+KuEIDmu8UQM9zLshgA\nMqe8ZmjvpFAGZnkAOJXvM1GZd5M3B5W9gd8f4fxZ2DsOAHayMZix7/uqqkwsQlVVbdu2bWuvfDqP\njvy6BRQTsU2u2ExUnxt9mYNKRQIeTcUyjHzMrT6Zg1xSUCzBFkNB51nquk77D/R/x3FclqWu67Zt\nD63hbfBdJQXFROyUywlTCEwnoSyLIT6pxFNmlODuEoFcUlAswZauFCfPUlVV9kiHe7tnyuscAkgj\nm7hyU7ZHnimA9ZTXDG3vetAL2j5wsimU0fuQz7fUU0AxEUfJlZhOQlmehu/JGOwyPx0MObgWbq/A\ns0AuKSiW4IBJoVShMYyFmYQXgGIiDpcrPdW1CqZv+vjZfVH6KZuOrbAI7i4RyCUFxRLs9SjoDEtm\nfcF2A8Cz+JrwUSnPXHiCawEALmaLodB13TAMTdPoV4bdDdG2rW03PBpeiFJQTMSVcgUthj8Hw4qe\niM/fb5jVmrtLBHJJQbEEW/IomITNSikz9sFMOV1GgILCEyUHxUTcIlcsGcNPbmh79WdC6GD2BWfN\nqWfE3SUCuaSgWILSgjPLCzcFOI/YiInFX82s1gDrKK8Z2tL1sNJncJdroYqwoZwzqlcwKCYiB7mi\nQQyLqpx18TkqYx0Tx1Y1B7keBHJJQbEEGxMuVVUVnGBaM01T0zR3ZV5aImwo54zqFQyKichKrnAQ\nwxK3GJzdz8/XlJVc+YNcUlAswZYYhWma9NTSOtWSPcZhmqZ5npVSXdcVE6wA8B7CQQzfIhh+9v39\n05/YmrcwwHPZ25XS9702COZ51kaD5pDKbYCpQm8ExUQ8Qi7HYljUEsjtGDmLY4MYHiFXPiCXFNqO\nBMWdT3FXCOB2/JjHNdGOP/syqzW8jPKaocOmmQaAUlnUEuyMULbFUHn5GPQGn/manIXC3qcARbI3\nhXPBEAQrBcVEPEuuVeMjvJERbiGRgMc1YY/Pkut2kEsKiiUozUNSns8HIE/c8IXVnRE/u9MlAYVS\nXjNE1wMAbMEZHxEdGfGztfveTHdJKIwGgGyg6yEKnigpKCaiDLlWpV5QX3olgjaB0x9RhlyXgVxS\nUCzBLkNhmqZDciDmCR80UlBMRDFyJWIX1mR4/CkknufRbHBstcsGuaSgWIJdXQ8696KZFwoAXoux\nFfyxlIFeCWV1TER6Jez+CF7iADeyN0ZhHMdSrQReT1JQTESpctneBdtoCIyo/PkhOq7SiV0oUq6T\nKPXuOg8US7BLmjOU1VNIpNM76nSQTdP4801wsQFyIzZHpbKNhnX5mni6IX/Ka4Z2xSgc607QEQ96\nIom2bWOTTlVVNQyDvf1RFQCAM4jlYFB28GMk1DE4NeVZFQWAELsMn6Zp9BRQDtvK1DaHbvj7vh+G\nwS/HWW/voiFf942gmIiXyxV0MyQcDL598Gb1vvLyu2sDtB0JdsUoJGaa3sA8z+M4mpKHYdD9Cwce\nQkRhV/oCUEzEy+UKBj9acQzu7JROhKMicCEJskhBsQS5GD66u8HJwdJ1XTAKwYyzaNvW2aY8Uw7g\nDaQcDCu8C4oXPWRDgc3Qso9xHOu6NqV1Xbe5HKcySqm6rv0tu64zh/M32KmDvaB+Xz3+TyygGAtn\nLSzK+af/7z+n3kMc2Oa1CzyMNypm/iyGXcGMfd+bVAqaYRgO7Czwi9JdEuM4LssyjuM8z/4224Qw\n+9qFOGtYSC+gGAsHLKglmO1xUR/mgrKedPWJ0yXxzgUexnsVK4xdMQrDMDie/6ZptOlwEvqI2jho\nmmYcx1MPBwC34Ewkof6yNgWSLpi3M3kXAM5g71wPTgyBPwxhJcEd703lxCgsKSgmArm+EvQuJKaN\ncCyDN4+lfO2JbwbFEuw1FA5MY1DXtXEPGJ+B+VMfSPdumF2OHXbhwOeIFBQTgVwrccyF6LxTemOv\nM+KdDQB3lxQUS7Cr66HrOj3uwPgDhmGo63qbJ8BMMaX/NEMlp2ky2Rp0Tkb7yTebAUDBLGpxR0ZI\nZotQtAQAW9k7ikNHF5o/67re6WPQu381NWKbkTTjRlBMBHKJsOWyLYbYEEqzl7PmJZpzd0mh7UhQ\n3PkUd4UAwMFPurBUUVtBMWEEXEt5zdCWrgc7YWLQf1DqfJIAkAPBMRF+Mse/7b2eiMLe4wCnsuWB\nqapKdzHEooRufAhxH90IiolALhExudZ3Q6g39URwd0mh7UhQ3PkUd4UAII3TEyHqhvjZhZcGHEd5\nzdDezIzBlUz9DACXEc61ENv4l49dXjmEEmAlG4dHalNAj3ewIxL0CMkyYhTKswrPBsVEIJeItFxO\n1EK16P+k5A0OoVSleBe4u6SgWIKN0iQM8P0jJPfAxQZ4M243RNJW+NmFzgg4lPKaoY0eBWOMFyYH\nADwa17WgqnTIggpNFWGWeb8BqJ0xCs5TVFhoAt2WUlBMBHKJEMnlpHxOhCx87FVQ7MJza34XKJZg\nl6HgJF3u+76qqmLMBT4mpKCYCOQSIZXLnR5C/U4l9a09cMyFh7Yf3F1SUCzBLkOhbdu6ro2+0zTp\n2R+OqBgAwC4CoyF+llaZC387PtNWADiKXUEGVVWN4+iMcQiuvIzEIy09UyIwpKCYCOQSsUeuQKKF\nvz9SZT439zN3lxQSLiXYNXtknhx1hQq70heAYiKQS8QeuZyZJ6vFshXiiZ+VCgyhfMpVe0o98wHF\nEuzqeqjrum1bE5Rg5oAoI48CABTDohY3asFuF5KdCzQh8HL2GshN08zzbK+5sd9B4T66FRQTgVwi\njpLLn3lS2Z0R63I/53/huLuk0HYkOOZ8jFPhdl9CeVcIAA4nPFH1z1L4BfLceAW4mPKaoV1dD5pp\nmrShcLuVAACwBt0T4aZb+FkKj4nwh00yGgJewgF5FNq2HYZhmiadRyE4U9QT4S0gBcVEIJeIk+SK\nBi7EzYULarWfbCuWLSiWYO/wSD2zgzYO+r7v+34Yhhu9LuX5fADgbDaMn6QnAmKU1wzt7Xpw8jBq\ni6GY5IwA8Aai3RAqOiDiKa4FgP0cbCiUBE++FBQTgVwiLpArZSusi1o4sXJCsqrMI0CxBAfkUbDX\nlJRHoTDf0QWgmAjkEnGNXK6tYB90hWshn8aGu0sKiiUgjwIAwAc7QxZ4Bb2c8pqhA87HHh55uy+B\npBk3gmIikEvExXLtD29Ut1oM3F1SaDsSFHc+xV0hALgL21xYHDOA0RAQobxmaEuMQlVV2nNQJbnd\nuwAAsIdoeKN6ZIQjwDa2GD5m8qf0kIe2ba+PV8B9dCMoJgK5RNwoV6obQmXqWuDukkLbkeDE89H5\nl04qPEZ5VwgAbse1FdTiuhNCr52sohbgMsprhg7Io9A0jc7cbFI0aorJ5QwAL8fNyKQq1zJY0Q2h\nmCECnskuQ6Hve51Hoa5rvWYYhttDE2IxExvKOaN6BYNiIpBLRA5yObZCINHCOnPhAnKQ61mgWIJd\nhsIwDF3XmZCFpmnGcXTSKlzPEmFDOWdUr2BQTARyichELttWUMZc+Fj13VzY/PWynkzkehAolmBv\n14PTv7AmyBEA4Lk43RDKz+GoomkcHfiKhUdwylwPt/c+HALPsBQUE4FcInKTy49aWJPy2XdwnnRe\nucmVPyiWYJeh0HVd27Y6jFGjJ54+qnL3gidKCoqJQC4Recrl90SsiVpQ3gwRh7dSecqVMyiWYO8o\njr7vh2Ewf9Z1fW+/Q3njUgAgfz5yOK4bPKnuzrUAJ1FeM1Tc+ZA04z5QTARyichfLtdWUJ+9D/HK\nn2Eu5C9XbtB2JNjV9VBVVcFxi4Vd6QtAMRHIJSJ/udxEC2rVLNXKO7VDuiHylys3UCzBLsMnwzEO\n5ZlyAPAgNvsVlGUi8BJ7NOU1Q7vOZ5qmtm3runaGOdyYkxH30Y2gmAjkEvEgub7YCmpVN8TOk32Q\nXJlA25Fgr0chmF6JWdgB4M04c0Mo+VRSvMeeS3nNUHHnU9wVAoAn4tsKmj+LAVuhUMprhvYmXCoY\n8m9IQTERyCXicXLpjExOogWl0zj+LH05oz35FR4n1+2gWILSDJ/yTDkAKIOP2IWkX4H8Co+mvGYI\njwIAwBV8jJ9cfl0LoeyN16R5BlgJhkIUHk4pKCYCuUSUIVdgNqmfpfDEEH9bCrshypDrSlAswRYP\nydfECTdOClWezwcAyoNuiIIprxnacj5fLS+GRwIApHGGRSzV1Wme4STKa4a2dD2YmVLHcVRKdV3n\n/HlwHYVUETaUc0b1CgbFRCCXiPLkCnRDrJttUq1Qozy5zgbFEuwyfKqqGsfR6Wi415gqz5QDgLIJ\nuBbUqgknedflSXnN0N5gxmA4QlazPwAA5Ew4wnGFa4GPYLiGgw0FbSLcGMx4IDyEUlBMBHKJKFsu\nJzXTfluhbLnOAMUS/GPPzuM4tm1bVZWOS5imaZ7n22MUjqIw39EFoJgI5BLxBrkWtZhuCG0rLFUV\n7INYlsU0bEFH9xvkOhYUS7C3K2Wapr7v9dRQdV33fX+vO6G8ziEAeBXhCaW+jZzkvZcP5TVDxZ0P\nU4XeB4qJQC4Rb5NrZYRjzFZ4m1z7oe1IsPd8+r73QxdvDGYs7woBwGsJ5JeUUDUAACAASURBVGVK\n2goK10IGlNcM7YpR0DdoXdcHVQYAAP5YE7VgxyuoElspuJ1dhoJSys+jUAw8b1JQTARyiXitXLat\noJSqlrCtoCzXgl54p1ybee0Ntoa9CZdyUzbDKgEA7GfN9BDGVuA1eCPlNUO78ih0XVeqOwEAICsC\niRaUm2vBtE9kBYAD2dv1MM9zVVVOmEIZmRnLswrPBsVEIJcI5FJeyMKSNAZQTARyJdhrKBweydj3\nvVKqaZqEr0Inb2h+ObYCBm4aKSgmArlEIJcmYCt8hix8zcUEQRAqQUa30TRNbdtqy0NneNRGg0Pf\n98Mw1HWtszw50ZQ8GABQPMZW+BqsoGgCL6e8ZmjX+cS6GLZ95eu9dJnaGgjWzZ6ysmmaeZ5PSjNS\n3sU+GxQTgVwikMshnWKB5ApSaDsS7B31EFy/rUxn0urgHNYJA8LsVdgVAgDwWZO6Eb/CLZTXDO2K\nUfC12BwxEJx2cpomf01d19M0me0ZdgEAL0QPghDFKyjMBdjGcjTbyhzH0dlRKVXXtV+4pq5rHc3Q\ndV1wg2062Avq96Hyf2IBxVi4eCH2nLKglFLLzz/9/+DDaJNDnXNbOFAW82cx7MqjEOOo4ZExb8Gy\nLNqp0HXdMAz+rxsw+9qFOGtYSC+gGAvnLcSeUxbMgjJpni3R/G1yqGqGCzHFNl+LktjV9eAbBGZw\n455iEzijMZum8Q0FAIBX8XXMpPptw3QHBN0QIGKXodC2rb+y67oNRZkhD7aR4RscTdNcls2pvICU\ns0ExEcglArnSrLEVIAE3WIJdXQ9LiGDygzXUdW0sDzP60fxpohfneTa2gsm7cAbcNFJQTARyiUCu\nr9g5nn/wBqbZPTikebbhBkuwNzOj+u2A0M6APZ0O0zRVVWXuXR3eqNfr3EpKqaZpuq6zPRllpIsG\nANiP8Sv8JXhO+hX4jIY17L1L9Ce+vWbnxNPBcZLrNyNpxo2gmAjkEoFcK0knYvrbjBQLn9B2JNh1\nPtpKsC0DP1XixZR3hQAARLi5mPwuCb0Zc1KfQ3nN0N7MjL7/ILjyMsq7QgAAUrAVbqS8ZuiUPApl\nQKSPFBQTgVwikEtG9WEcOHZDYHMrPuydvPz00+w1FJwxDmfnUbiSwkzCC0AxEcglArlE/KQP+rQV\nfHPBUfXNjSU3WIJdox7GcWzbtqoqMze02ppHAQAADsfOr6CUqlTldEPYiZhUiW5z2M8B90Tf92YM\nwuYkCkdB5OqNoJgI5BKBXCJcuaqq+hTPj1p4+TgI2o4Eu86n7/vbLQOH8q4QAMABSGwF3qJ7KK8Z\nOn7Uw72Ud4UAAA6g+kvw7PARyoCtsJvymqFdwYxOksTCeHNczzZQTARyiUAuEQG5zAyHVXS05Jvh\nBktwwOyRvr5lGFNlnMWVoJgI5BKBXCLCci3Lj1/h95P3J9+zFeG4LIuZYfJVmr/qZKXsMhRMGCMA\nADyJzzkgtMXgeBreZitAjNLug4T7SHqmPCRSUEwEcolALhEpueyX5PIxeNIYCi8cAcGohwR753qI\n/dT3/S1BjuVdIQCAgzF2gJly+tdceLOtcBTlNUO7ghn1FFD27JFmuW3b3EZOAgCAUtZ8kp4L1iRw\ntJs6Av1ezl6PgpNkaZqmtm2XZTELB9RRAu6jG0ExEcglArlEfJfrswPiZ13cr1C8+LQdCfbmUfB3\nN8kVbsmyUN4VAgA4BWyFcyivGdo7KRSjHgAAHkmyMfNnkKID4rXsGh6pEy51XWfcBjr/ku6SUA+f\nRrI8q/BsUEwEcolALhFr5bIyK6jfCScdE8FkVigbbrAEe/MoKKWGYRiGQa+p69r4GMZx3FW1u+Gm\nkYJiIpBLBHKJ2CKXZyuYRExvyMJU6nkdQmlXveD7GADgFByHQSi5AqMl11NeM7QrRsEPUJimqRgn\nVTEnchkoJgK5RCCXCJlcXqvmTwaxLEthjZ8DN1iCXYaCkyyhaZq2beu63lupPCj7qTgDFBOBXCKQ\nS4RYrmVxkiv8jXp4R1QjN1iCXTEK4zia2SN1mEJus04DAADAHvZ2pejESkqprutySMVI0owbQTER\nyCUCuURsl+szu/N70irQdiTYm0ehaRo9uqE8R0JhV/oCUEwEcolALhEHyFVi/0ICbrAEWwyfrx1U\nNypenikHAHApkekli3cqHEV5zdCWGIWnJ0hYSXkX+2xQTARyiUAuEbvkMlmYlE6u8LuoPoIcC4Mb\nLMEB0kzTpPsdzMKNcLEBAA7A8itUn+/URS04FRKU1wztzaNQVZUZ+ND3fVVVt4c0VhHurRUAwJOw\nmrpFLTFHAq/WN7B39kg7Z7NSqu/7YRjKiFEozyo8GxQTgVwikEvEYXLFB0EU5lSg7Uiw11DwEyfc\nMru0ffTCrhAAwG1EDAVVoq1wFOU1Q3uHRwIAQLFE0jWqT6OBDoiy2WUo6GmmTVCCmejh9pDGQ+DW\nl4JiIpBLBHKJOFKuuK1Q0gAIbrAEez0kOijB/OmELFxPeT4fAICb+cysoJwJIP56J3j3KlViM3TY\n+eQwNlKVeIUAAO7nM1hB2bbChxXB67fAZuiwGAVjJTRNc69T4SjwRElBMRHIJQK5RBwvl90B4fRB\nLH99EM+9TM+t+QXsMnzMjFAOZQyPBACAP9IdEOrHtcAbuLxmaJdHoW3buq51Rueu68ZxrOu667qD\n6gYAANmwLH5go5+IiQR35XFMHgWNHv5wrzFF0owbQTERyCUCuUScKJfnV1BFxCvQdiQ4JkbBiUso\nI0ahsCt9ASgmArlEIJeIE+Xy/Aqx4z7Lr8ANlmCvoaC9CE3TzPN8QHUAAOBp/M1AfdxIOsiHXYbC\nOI7zPPd9r4c8mK6pHMZJ7udZ5nAOoJgI5BKBXCJOlytpDTzRVuAGS3Ck9TdN0zRN984eWV7nEABA\njsQzK7x8GojymqEjEy6pDHwJ5V0hAIAciY+WxFAo7Kw3dj30fV9VlY5h1FM8tG3btm1J3puSzuUa\nUEwEcolALhFXyOW1hcFIhaeMlnxEJe9ii6Gg53eo61oppe2Duq6XZdEJFW53KhxFYSbhBaCYCOQS\ngVwiLpIrPvzhujocxLNqezFbPCRVVZnJn7TRYArRuRrvzaMQ+4n7AADgYJIdEOr3nfyq1y9dDz+Y\niMUM/QdLBGk5eKKkoJgI5BKBXCKuk8t+tX7OAWEneM6/AyLz6t3LYZNClUdhJuEFoJgI5BKBXCIu\nlcuzFWLk3BhzgyXAUAAAgH182gr2BBDbHLqQFf/YtpuTLCHDDoj9lNfPdDYoJgK5RCCXiBvkWhbf\nnVCpH6NhWX4GTGZ7HbOtWA5skeZrwuYyJoUCAAAZv7ZCZb2G7ahG9QInf3nNUHHnU9wVAgB4DFa6\nRmf4g3qNrVBeM0SMQpSc427yBMVEIJcI5BJxm1xWZoXg8IffH7O7mhlWKR8wFKIUZhJeAIqJQC4R\nyCUiT7lynoQ6T8UyAUMBAACOw2pxfacC7fETwVCIkpvBmz8oJgK5RCCXiCzkqqp0aucsKvlLVpXJ\nDQyFKFi+UlBMBHKJQC4R+ci1/La/fqRCVuSjWIZgKAAAwKEsiz+3pKID4rFkZyj0fd/3vZ5xKs00\nTU7ep2PBEyUFxUQglwjkEpGFXL/WgJ2o0SGLeiqlcqpJhmRkKEzTVFXVNE16CsqvRkDbtmvsic1g\n80pBMRHIJQK5ROQlV3KoZCbkpVhmZJQXQueBDs5e7aOtPzPbtb0+nzMCAHg11me6ydXo5F8q741d\nXjOUkUdhnmfjRdALMYeB/rWu61PrgydKCoqJQC4RyCUiF7nsoZKRqMZMqppJNfIkF0NB2wTO5FJB\nQ2GaprSz4SgKMwkvAMVEIJcI5BKRkVzJqMZ8yEix/MjFUAgSNBTath3HMbFXtQmzLwsssMACC0cu\nqB+MU0EpVVkd31VV5VLVgxYKY+M009fgz17dNE1d1+lZrfcYhmZfPSmqPZM6C18XUEy0YMikPpkv\nVFa/LwtfF3J8GKtKKbVUnxNL/k4/ba+5ZeFYxQoja0PBR09vrQ0Fs9z3fdp02Eapl/w8UEwEcolA\nLhE5y2VshUpViZGTF5OzYreTi6FghjzYTb7f/HddZ5aNoXCGlQAAAEeyLCrkmfedCpAbGY3iaJpm\nnmddH3tZRdwG9nBKQ3XcuJQDi3oJKCYCuUQgl4h85TLd+YtS6meopDEUbqwzbUeCXDwK6jfhkrlj\nTMTiNE3aeXAxhV3pC0AxEcglArlE5CvXp1/B6X24sYnNV7EMyM7wCY6TXE95phwAQGlUlZN/ye59\nePo7vLxmqLjzwX10HygmArlEIJeIB8j1ays4iRo111eetiNB1nkU7qWwK30BKCYCuUQgl4gnyZVB\ngMLtR88cDAUAALic34a5WnKxFSAGhkIURuxIQTERyCUCuUQ8S67KMw+ur/+zFLsYDIUo2LZSUEwE\ncolALhGPkOsj25LXTl/ccj9CsbvAUAAAgHv4i2T87YCwG2y+8jMBQyEK96gUFBOBXCKQS8Sj5brl\n4/7Rip1NaaM4yhuXAgBQNmbW6aWyghwfG+FYXjOERwEAALLAHgHxt5Jv/bvBUIjC3SkFxUQglwjk\nEvEsudw5JL1ghQt4lmIXU5qHpDyfDwDAGyimA6K8ZiijSaGOImYYFnblAAAALqDAroclgrQcPFFS\nUEwEcolALhFPlMsfKqmsD7yzz+iJil1GgYbCUeCBkIJiIpBLBHKJQC4pKJYAQwEAALLgXqcCxMBQ\niMJNKQXFRCCXCOQSUYBc/gQQ5x7u+YqdR2nBmeWFmwIAvIq/4Q8qkM45/zd8ec0QHgUAAMiIP/sg\n9JXPp//1YChE4XaUgmIikEsEcokoQy679+Hsb/QyFDsJDIUohfmOLgDFRCCXCOQSUY5cVvt9alRj\nOYqdAIYCAADkxUdSZ7717wZDIQqeKCkoJgK5RCCXiFLlOu+7v1TFDgFDIQqeKCkoJgK5RCCXiNLk\nOn9KydIUOxQMBQAAyJRENgV8AJeBoRCFu1AKiolALhHIJaJguU769C9Ysf2UlheivEwXAADv5GPi\nafU397T6bdfzfNuX1wzhUQAAAIAoGApR8ERJQTERyCUCuUSUIdfHIMkQB55mGYqdBIZClMJ8RxeA\nYiKQSwRyiShMrp+QxjPb8sIUO5Z/3F2B44kZhtwHAADPpqp0pMKyLPgALqNAj8ISQVoOd6EUFBOB\nXCKQS0Qxcpneh7NnnS5GsTMo0FA4CjwQUlBMBHKJQC4RRcoVtBWOauCLVOwoMBQAACBf7JDGs/0K\nEARDIQqeKCkoJgK5RCCXiMLkCs4RdexkkoUpdiwYClHwRElBMRHIJQK5RJQnV3Co5IGnWZ5iB4Kh\nAAAAj6FarpgjCmwwFKJw50lBMRHIJQK5RCCXFBRLUFpK6vKSbAMAgOZv9gc7wjGzeR/Ka4bwKAAA\nwDP4mtQZzgBDIQqeKCkoJgK5RCCXiOLlMq6FvzX7Trl4xfaAoRClMN/RBaCYCOQSgVwikEsKiiXA\nUAAAgMfg9z7Qxp8NhkIUPFFSUEwEcolALhFvkOvY3oc3KLaZAmePPIq0lcpdFSQrWTL/zsi8ermB\nXCKQSwqKJcBQ2A43Vs5kZbIAwIEsatHuhEpVuifCzDpd3tDEHKDrIcr6lmaapmmaguudha/lbNgg\ndvQzuOxAbwBTRgRyiUAuKSiWAEMhynqztG3btm2dlX3ft23bNI1e1gs2VVX1fe9sn7hZm6Zp29Zp\nqqdpCh7dwewVrMl6vh4I1sN3jwjkElG8XMvvm/IvBdO+Uy5esT0UaChUEc4+rt3qq8+P777v53n2\nf7V3WfmxnjhKAhp4AADYRoGGwhJBWo7ItqjrehgGe808z3Vd62X9Ee+YDuZXs/04jsozBZyjOAbH\nMAxOOdM09X3v+Cr8Yp1t7JW+0yK4MewEV6cI5BJRvlzL4jsV9lC+Yjso0FA4CpFt4TfDTvtd17W9\nzTzPflveNE3XdY7BYeMYHHrB7krQ/Rc6aqGqKr2B+a9emOdZPxJ6G1OU3l73ZZgynQLXSAErwdUp\nArlEvE0u21bY9qZ6m2IyYt/fD+WyM7IPpJQax7Gu667r9Bq9XNd1Xdd6jfYW+MumBL2v/mkcR/+I\nukxdrF6jl+3S7H3tLc0GXdf5NdeFO1XV6+0CtemzRpwceFBVAWALSg94sP7l0a7dXoHDwaMQRWqW\nNk1jnAGOw0D9fvebXgDb32A7BvxOCgc73GEYBt8toT6dB1+rbbY0u5s6GD+HUz4cAh4aEcgl4j1y\nLfaJ7nAKvEexDZBHIcoi9ET1fa8NBb/fQVPXtWmV9Ve7Rq+04w0dC8DHWBJN0zhWhb1jrBrJ84hu\ns2e4BPhIb7CXg1wiXiHXsig9wfRvWgXDhmwKr1BsK3gUjkQHIkzTFGxTtTPA+UxXSg3DYPosll/P\nf8Kp0HWdPkqwOZ8sNvsAtCvCOQuSKABAnjD99Lnc0d9xIgeeUbooFerpN34Cvd7u+DdbKqXslX68\ngt7M2XH5jVGwy7EP6tTEObqyYhTsktVvbIQdf+AUaAdePOiGyb+q+dcwK5BLxFvkMvEJy+JEKshL\nuqjteCJ4FKIsck+U/gRP+PZ1LGEwsMDZbE14ge+3GMdRZ22qqmqeZ9sHYAZBBNE/6R3bttX11AUO\nw3BNIoq3seEGezPIJQK5pK8sFEtQWlrsyxJ955xR3B82qVd+DTII7rhy39zI+QIBwGEYg2D5jVT4\nW3HPG6C8l09x53PcFUoXVd6tUBj5X6D8a5gVyCXiRXL5hoLaYitc1nY8EboeohR2pSE3uMFEIJeI\nF8llzrSq9oQ0vkgxORgKAABQFLT6x5JdHgUzejDRKW6GIKY320l57iPICm4wEcgl4l1y/SZUMGyY\n/eFdignJyKPgTDcQywFQVZXOa5TebD/cNHAq3GAikEvES+XaMTjrpYqtIyMbys5erLMc+nVz1vub\nFTnqwR7WGBySEPs1OB7yceMXtsH3AcCLsEyEyjz3lVJ3WADlvXwyOp+qqsZxNM2Y86fGnz6xbduT\nDAXRqIcDD7pmpU7OqH4VsH8yiaL9n5wNyib/ZzX/GmYFcol4o1y/78lthgKjHhLk0vUQG/rvb2av\nPLXBy+pK2zme/amozU/jOJpZpJumMevtEt5gJTyCrG6w/EEuEW+U6/eU/8Y+LEqtzrz0RsVWk4uh\nECTdpOl+B5ND0FBtwuy7fuEujC/B/0kbB7Ff38m2i8sCCyw8cSFBJtV4ItmNerCJ9aYbp7rfN6H2\nGYZm32VZqqoyn+POT5vLPwR/WikH3bnwkliErySu4L0L+dcwq4XKcuey8HXh6+ur1AWllKqqZXHn\nk/y6+7GKFUbWHoUgfd/ryQiWZTm1LczqkpsJF6qqGobBnqU6CB6F/MnqBssf5BKBXDZrPvRRLEEu\nhoITpWivtJmmSTeT542K3Mb/HUTshq7revylruu2bTEFAAA+oLE/h1wMBaWUbv/0sjYRjKHQNI0Z\nNqnXTBYn1Ser3qbGYpqmuq4TptI8z/Q75E9WN1j+IJeIt8tlTn+15fB2xZJkFKOgEy6Zq2W869M0\nmTmX9YIz6u8kl5Go2P/973+HHHT/zRqbARJyA1enCOQSgVyL+ghTqL4NWUSxBBkZCkqp5Xfwnt3O\nmUh+xbX8daXM8+wP99A/DcNQ1zWGAgC8keU3nXNV/bgTFiVP6Awf5GUoqJw+hb9aoFcyDIOdO8Ek\nXNLYfgjnJ8iWrG6w/EEuEcglBcUSlCbNZRe7ujAzI2yAxx7gvZjEBvodUCl1oUO6vJdPRsGMAAAA\nx1NUq30DGApR+LiHU+EGE4FcIt4uV+iDPq3J2xVLkl2MQj6IfEfcZCClMOfk2SCXCOTSLJU1R1R6\nSxSLg0cBAAAAomAoRMFJAKfCDSYCuUQgl9v78M1fgGIJMBSi4ImCU+EGE4FcIpArSMIaQLEExCg8\nA5OpOjj/hSj5hJ30Oliav4uZiePr0WP1TBQLAHAe68MUIMpSFgeeUboo59f/HUTwWHVdO1dNT55p\namL/uea8HMZxTPxqNgjeMPbuTj2dSl587+V/b+dfw6xALhHI9YPuc1h+/8VluazteCJ0PURZcvJE\n1XVtrlnXdcMw7Em/aFsG9lxcGt/scDJqB8vs+36eZ7PLOI7DMNiOBPugms31LwMUEIFcIpArRqz3\nAcUSFGgoVBHurtdh9H2vbYVDShNNItV13TzPwX4EPael+bNpmrqumQsbAG5nMa9/jIFNFGgoxJwn\n0nJyti20O+GoZlg3/+s3djwQhnmebT/HNE3MOpEg5xssQ5BLBHL9sPrNj2IJCjQUjuI9nijHnTAM\nQ8IZo9v+YFRjXdd636ZpfBOhbVu7TCIZ33ODHQJyiUAuh+WbGYBiCRj1cAzV/x1nct19v9Z1nfYE\njOPYtq0/CEJ7OLQjQU93OY6j2abrOowDALgaM/E0bAVDIcrhM4BVuXaQOV0YTdOkW/SmaXQHhK2P\nsRuaptEFVlVlb/O12LdR3hRzp4JcIpBLCooloOshiuimWf73v6///rdim5XuhJj/fxvDMHRdJ9rF\nr4D2MdjbSMt8G7yVRCCXCORKEAxHQLEEGAoPQ3+4a6++vVJaiP7vNE36mdkQdTiOox0C6Q+zHIbB\nzwABAHAn2ANyMBSiZBUEO8+zjgHUjbHd92//qvna6uuQw7Zt27bVGRr8X78WqAdAmj91MKO9lzM8\n0glmrKrq5YMns7rB8ge5RCDXByu8BSiWoLRemcv6mZwDHXXQAm5WUWKG86DHEQD+qCqlfnM5V+d2\nNJT38inufDAUlFLxzojb2+/LKO9ZBYDtfBoK6syghPJePox6iCK62Fl5AnTwQfCn9xgK+VPe2+RU\nkEsEcrnYgySXH1vBBsUSlCbNXR4FyA0uEAB8UFV/00ie2ftQ3suHYEYAAHgBy2LnZ8zKDZw5GApR\nuI3gVLjBRCCXCOSSgmIJMBSiFOY7gtzgBhOBXCKQSwqKJcBQeAbTL8Gf9pS5pjSz8tgKAABcirEG\nsApExCZlfigHnlG6KOfX/x1E8Fh+fsOu6+ya2H+uwSlQ51wypdl/LsuiMzGbX50T1wkiRUe/hjxr\nZZN/DbMCuUQgVwy1qJ9/nxJd1nY8ETwKUZacPFF249113TAMG5Iua5qmmefZlKYzMdulzfOcdhJs\nPjTYZHWD5Q9yiUCuGLF4RhRLUKChUEW4u16H0fe9thW27T7Psz1PhJ4K0rYM9MyQsd33HBoAAB5H\ngYZCzHkiLSdn20J/028IDghOO9n3vV1UempKPcUDiZv2k/MNliHIJQK5pKBYggINhaMo0hOlp276\nupnuj4gZItM0Ob0VsIEib7DzQC4RyCUFxRJgKBzD/1X/d8i/ys8sege6P4IOCAAoDQwCORgKUY73\nRGVgA+hIRmflNE1+V4J2GMTcBunuCVgDrk4RyCUCuaSgWAIMhSgiT9T/lv99//e/79ss64b3bm6n\ng8ENfd/71oNSahzHYRhiHRDLstABsQdcnSKQSwRyfcURCMUSYCg8DP31PwyDPXJBFNVY13XbtnYO\npXmedbIEBx23GLQhNF3XJX4FAMgcHAlrwFCIktUNNM+zHuSp4wbGcbTdCeZXTforX8cztm1rCuy6\nLrYLCRXOI6sbLH+QSwRySUGxBKXNhnnXNNNHHfTKm1UbAaXGGZQ30ysAHMJfzHil1AmdDuW9fIo7\nHwwFpVTcE1CqWeBT3rMKAIdgDIWlUhWGwgr+cXcF8kV0sbNyW8Wmj1JvMhTyp7y3yakglwjkSrD8\nWAgfoFiC0qS5y6MAucEFAoAY2lDAo7ASghkBAOCNVEW15idC10OUr1ZhVt0N8DjK++w4FeQSgVzr\n0VqhWILSpOFiAwDAV37CFE4Y+FBeM0TXAwAAvJSi2vPTwFCIQs+CFBQTgVwikEsEcq3EhCmgWAIM\nhSiF+Y4uAMVEIJcI5BKBXFJQLEGBwYwxw5D7AAAANMFsChCkQEPhwCSJ2BYiUEwEcolALhHIJUJ/\nXqJYDLoeonDTSEExEcglArlEINd6tFIolgBDAQAA3gtpl76CoRCFIFgpKCYCuUQglwjkkoJiCTAU\nouCJkoJiIpBLBHKJQK41LFYaBRRLgKEAAAAAUTAUouCJkoJiIpBLBHKJQC4pKJYAQwEAAN4N3Q5J\nMBQAAAAgCoYCAAC8lAVnwgowFAAAACAKhgIAAABEeepcD33fK6Wapmma5uaqrOPA1Ot5FnUs2Z5j\nnople47IdVdRx5LnOWYrV3k8z6MwTVNVVdM0TdPUtq22GAAAAOAMnmeRaRfCNE1Kqb7vh2GwTyFb\nczXPinGON5aWZ1HHllZ8UceWlmdRx5aWYVE/k01XzDwc5XnnU1XVOI6mx8H/M8+LnWfFOMcbS8uz\nqGNLK76oY0vLs6hjS8uwKAyFrzys60E7Epy4BL0SAABgI0W17Afz1GBGG8dQODAT57FJPfOsGOd4\nY2l5FnVsacUXdWxpeRZ1bGnZFfVrIpDFOUYJhoLtYCjM4QMAABdB6xHhYV0PAAAAcCUPMxTsIQ/O\nSgAAADichxkKSqm6rtu21cvaRMBQAAAAOIlHjuKwQ07ssZEH8rjMj1eyRpy+76dpan65qmo5sv5e\n0mnEXp5DbI1cWiXuLiV8GF9+a32l73skCrM8k3Ecx3E8qWSlVF3XdV0rpbquO+MoD2WlOPrWQkPp\nvaQ3vqJmWbJSrq7r9Gb6NjvpPZA/2x7G18r1Fa0n+gR5qqFwHvqJ0sv6lXRrdfJijTjO+jdrKLqX\nzAv9gorlyUq57Le5bvwuqV12bHgY7V3AMI4jdmealz5jCZx7hVvHZo04zstI2+lXVC4/1t9L5iv5\nze/xNXK92e502CDXy2+wGOM4dl2nteJtH+R5wYynQubHBCvF0R3tiQ1ewvp7aZomZ8qSF7L+7qrr\nWsco6K73a6qXGyvl0j3uWqhpmuZ5fnlIRxAdvUF0QgIMhe+89mW0OilKdQAABj1JREFUhrQ4etYu\nbaqDisjVtq32u4CDL9c8z/M8t23L/LE+wbur67phGNq2bdu2rmvkgg1gKHwHGzxBTBw9G/gwDOM4\n8m4y+HI1TVPXNfdYkJgsy7JoQ0G3gtdWKl98ubSlrt3p4zjiUYBtYCjA8fR937atDsPmxZRGfyLr\n4W1mGSdWDBO9r+HuSqP9eSbfjLYV7q4UPA8MhQ/I/JhgpTi6xx1Hwkq5dBSVPQ7+nbkBVsr1QmWC\n8KaCS7kzkjJL7AFXbx58FSQhTl3X2sNphmvb3FHZ+1kjl7P9m4PS18jlDHZXLx5QukYuf9QDL7QE\nilEPEUqYPfJYdOe6Sf5IlJlNTBwdUK2X9YJJs61ZXhnSv0YuMKyRq2maruvsu+u13TRr5NLjHZxU\nthfXEwrgkSmcLyA4+gg0iCMCuUSslAtVNcgFF4ChAAAAAFEIZgQAAIAoGAoAAAAQBUMBAAAAomAo\nAACk6Pu+srAThFRVdfiwCz31gGgXe/hD0zS6Vn3fp6MXb8zYMU2TfWhdZ80ZCcecoR/2cdNSi65v\nwbGiGAoAAFHMfCV6QLlOGm1al5PSb4taSl0ZPe5Rj40cx7H5JbHjjYaCPUmHbsLtPBnO4OpDMAk9\nRcnNzF4xU8Oh2CxzdyVwAADIH6WUsRI0Z6ctkubdsrMqPWJWd7uSwQqrMxMfbUtrtkbYR4i/DTwK\nAAApnO/7aZpM2iLbNW16KLRD22RZ1r5041o329srV7q4bRe9OaieFkuXoL/F/a4H+1hmpTO3sinc\n/sjWRZmf7O3tHhmTp8HeV2/gn0Xf986Mss65a49IouamYn5/kKOqvVIfWs+oov80XQ+O/uZE9HpH\n2Kqq/vvf/9ob69MJ5tUuhLstFQCAfDFNmuNX0Kjfb1+9mZ3FXH+2apPCSXBu9tXrx3F08jEHP3l1\nNfztbY9CbNmpjz4X+0B2ZZSVGNsc1DlHs2xXxvmkTpyI7TDQhwgmNV8sj87XiiVOxK6YXau6rnXh\n6jMXuDlo8BDOeSml/vOf/zgFFgaGAgBAiq7r7Ckr7ZbANCT+eruJMk2g3WLZ28dasuAGzqFjXQ9m\nvTPjgz4d+0DOBrpw50S+nq9Zb8/E4bf9Qf+87WCwLQanYva+ay6EmWjmq6EQ01B9Bk/olf/5z3/M\n8r/+9a+gtoXBXA8AAClst7Z29Q/DsHg5bW3HuG1YqEg8vJ6IQfN17g/t0HZi5ZyxA4l97fr4AXfp\nwmOHsNcvlmNAn5cubWWwpOkE0fLqSep1OepT/68Vq+t6GAa9zfrQQn1cs5dz+Rz++c9/KqX++9//\n/vOf//z3v/+tbQVN0zS6J6gwiFEAAIji9OKbAIX9XdFVVbVtqxsnp89+DXp28p11SBRultcYCgYd\nBKCU0kNFvh5I20n27suy6MZe/c4wZx/x61lP07Qsi26wnaCKNNrE0Qf9uldd1//+97/1cpGWgQOG\nAgBAFP2haa9Z0zyv9BAsy7Im4YE5aG+xsiZ6M7s+vh9iW+FO9J8975RxD3ytm4kTDKK7CZyKramV\nNjj0WNavu2i0SivP/V//+tc8z74lVGYkI4YCAECCuq71d79ZE/R768308obW4msTaDfAyhrssAYn\nGt9vmJ12fY3/3Dlf2xDR/oCY9z5oo9grdWm6Ddb+CVPzNRWzMzSI0HutdITo3gen30GT7rZ4KncG\nSAAAZI//6jc/qc/YPYOJmHNi95xAOYNpnJb4YAEzJtPsotd/DWZcPqMFVSisz9kgFpAYO18/MDMR\n/B/cPlZasOaJijnb2yMm7AJNOKd9LD9Dhn++5s9gOg11ZgaIG2GaaQCA79iu9a/brBxS75S5Jjhx\nTTU27ystPLi97k1ItCwmitMvKnb0Qyq2uTR7R7OXf4m/nvhzwVAAANiL3WzoBsPOGvQqqqqq6zpt\nJFVV9XR9qqr6z3/+o/sgNNpALDKLM4YCAMBenKA8PbrvvurcgxHha7OiQzgfGvqnwx59Y6iqim1P\niz0xAICL2dMvUAYrUzs8HZ1E4e5aXAeGAgAAAERheCQAAABEwVAAAACAKBgKAAAAEAVDAQAAAKJg\nKAAAAEAUDAUAAACIgqEAAAAAUTAUAAAAIAqGAgAAAETBUAAAAIAoGAoAAAAQ5f8BtcugsNgiLTsA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = factory.GetROCCurve(loader);\n",
    "c1->Draw();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFile->Close();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROOT C++",
   "language": "c++",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".C",
   "mimetype": " text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
